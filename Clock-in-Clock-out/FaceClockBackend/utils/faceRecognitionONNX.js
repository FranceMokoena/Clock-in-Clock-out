/**
 * ONNX Runtime-based Face Recognition - ENTERPRISE-GRADE (MANDATORY - face-api.js removed)
 * 
 * üéØ GOAL: 100% Matching Accuracy for Serious Companies
 * 
 * Models:
 * - SCRFD for face detection (~95% detection accuracy)
 * - ArcFace for face recognition (512-d embeddings, 99.83% accuracy on LFW)
 * 
 * ENTERPRISE FEATURES IMPLEMENTED:
 * ‚úÖ Phase 1: Quality Gates
 *    - Image size validation (min 600px width)
 *    - Brightness validation (30-90% range)
 *    - Blur detection (Laplacian variance)
 *    - Face size validation (150-2000px)
 *    - Single face requirement (reject if multiple faces)
 *    - Face quality validation (65% minimum)
 *    - Facial landmarks validation (5 keypoints)
 * 
 * ‚úÖ Phase 2: Advanced Matching
 *    - Stricter thresholds (60% min, 70% high, 80% very high, 55% absolute min)
 *    - Dynamic thresholds based on quality (70% for low quality)
 *    - Ensemble matching (weighted average across all embeddings)
 *    - Temporal consistency (recent matches boost confidence)
 *    - Similarity gap validation (8% minimum gap)
 * 
 * ‚úÖ Phase 3: Registration Requirements
 *    - EXACTLY 5 images required (not 3-5)
 *    - Diverse angles recommended (front, left, right, up, down)
 *    - All 5 embeddings stored for ensemble matching
 * 
 * STRICT thresholds: 55-60% minimum similarity (pushing for 100% accuracy)
 * - Only 512-d embeddings supported (128-d face-api.js embeddings are rejected)
 * - All quality gates must pass before matching
 * 
 * LOGGING:
 * This module is intentionally verbose for debugging.
 * To reduce ONNX logs in normal use, leave ONNX_VERBOSE unset or "0" (default).
 * To enable full ONNX internals logging, set ONNX_VERBOSE=1 in the environment.
 */

const ort = require('onnxruntime-node');
const { loadImage } = require('canvas');
const sharp = require('sharp');
const fs = require('fs');
const path = require('path');
const https = require('https');
const http = require('http');

let detectionModel = null;
let recognitionModel = null;
let genderModel = null; // üë§ Gender classification model
let modelsLoaded = false;
let modelsLoadError = null;
let modelsPromise = null;

// Mutex for ensuring thread-safe inference (ONNX Runtime sessions are not thread-safe)
let detectionInferenceQueue = Promise.resolve();
let recognitionInferenceQueue = Promise.resolve();
let genderInferenceQueue = Promise.resolve();

// Configuration - ENTERPRISE-GRADE thresholds for 100% accuracy
const CONFIG = {
  // Recognition thresholds - ULTRA-STRICT for 100% accuracy (ZERO false positives)
  // ArcFace achieves 99.83% accuracy on LFW - we push for 100% with strict thresholds
  // Research: Higher thresholds = fewer false positives (critical for security)
  MIN_SIMILARITY_THRESHOLD: 0.70,  // 70% - Enterprise base threshold (increased from 60%)
  HIGH_CONFIDENCE_THRESHOLD: 0.80,  // 80% - High confidence match (increased from 70%)
  VERY_HIGH_CONFIDENCE_THRESHOLD: 0.90, // 90% - Very high confidence (increased from 80%)
  
  // Minimum similarity requirements - ZERO TOLERANCE FOR FALSE POSITIVES
  // Based on calibration: Impostor mean=67.40% (Sethu->France mismatch)
  // Set to 70% to be safely above the 67.40% impostor score
  ABSOLUTE_MINIMUM_SIMILARITY: 0.70, // 70% - absolute minimum, NO EXCEPTIONS (above 67.40% impostor score)
  MIN_SIMILARITY_GAP: 0.12, // 12% - minimum gap between top match and second match (increased from 8% for clearer distinction)
  
  // Face detection settings - ENTERPRISE QUALITY GATES
  // üè¶ BANK-GRADE: Adaptive face size thresholds for low-quality cameras
  MIN_FACE_SIZE: 85,  // Minimum face size in pixels (calibrated for 900px captures ‚Äì was 120px)
  MIN_FACE_SIZE_STRICT: 115,  // Strict minimum for high-security scenarios (reduced from 150px)
  MAX_FACE_SIZE: 2000, // Maximum face size in pixels
  MIN_DETECTION_SCORE: 0.50, // Require ‚â•50% probability to treat a detection as valid
  MAX_FACES_ALLOWED: 1, // CRITICAL: Only 1 face allowed (reject if multiple faces detected)
  // Detection score normalization
  FORCE_SIGMOID_NORMALIZATION: true, // Always normalize SCRFD logits ‚Üí probabilities before quality gates
  
  // Face quality requirements - ENTERPRISE STANDARDS
  // üè¶ BANK-GRADE: Adaptive thresholds for low-quality cameras
  MIN_FACE_QUALITY: 0.50, // 50% - calibrated for SCRFD logits normalized to probabilities
  MIN_FACE_QUALITY_STRICT: 0.60, // 60% - strict threshold for high-security scenarios
  MAX_FACE_ANGLE: 15, // Maximum face angle in degrees (stricter: 15¬∞ instead of 30¬∞)
  
  // Image preprocessing requirements - QUALITY GATES
  // üè¶ BANK-GRADE: Adaptive quality gates for low-quality cameras
  MIN_IMAGE_WIDTH: 400, // Minimum image width (reduced from 600px to support older/low-quality cameras)
  MIN_IMAGE_WIDTH_STRICT: 600, // Strict minimum for high-security scenarios
  MAX_IMAGE_WIDTH: 1920, // Maximum image width
  MIN_BLUR_THRESHOLD: 60, // Minimum Laplacian variance (reduced from 100 for low-quality cameras)
  MIN_BLUR_THRESHOLD_STRICT: 100, // Strict threshold for high-security scenarios
  MIN_BLUR_THRESHOLD_LOW_CAMERA: 40, // Very lenient threshold for known low-quality cameras
  MIN_BRIGHTNESS: 0.25, // Minimum normalized brightness (relaxed from 0.3 for poor lighting)
  MAX_BRIGHTNESS: 0.95, // Maximum normalized brightness (relaxed from 0.9)
  // Enable automatic enhancement for low-quality images
  ENABLE_AUTO_ENHANCEMENT: true,
  ENHANCEMENT_AGGRESSIVENESS: 1.2, // Enhancement strength multiplier (1.0 = normal, >1.0 = more aggressive)
  
  // Facial landmarks validation
  REQUIRE_LANDMARKS: true, // Require facial landmarks for validation
  MIN_LANDMARK_CONFIDENCE: 0.6, // Minimum landmark detection confidence
  
  // Ensemble matching
  ENSEMBLE_WEIGHT_RECENT: 0.3, // Weight for most recent embeddings
  ENSEMBLE_WEIGHT_QUALITY: 0.2, // Weight based on quality
  MIN_EMBEDDINGS_FOR_ENSEMBLE: 3, // Minimum embeddings required for ensemble
  
  // Temporal consistency
  TEMPORAL_TRUST_DECAY: 0.1, // Trust decay per day for recent matches
  TEMPORAL_TRUST_WINDOW: 7, // Days to consider for temporal trust
  
  // Liveness detection
  ENABLE_LIVENESS_CHECK: true, // Enable basic liveness detection
  // üè¶ BANK-GRADE: More lenient eye spacing for low-quality cameras (landmark detection may be less accurate)
  MIN_EYE_DISTANCE_RATIO: 0.20, // Minimum eye distance as ratio of face width (relaxed from 0.25 for low-quality cameras)
  MAX_EYE_DISTANCE_RATIO: 0.75, // Maximum eye distance as ratio of face width (increased from 0.65 to 0.75 to accommodate wider-set eyes and slight angles)
  // üè¶ BANK-GRADE: More lenient symmetry threshold for low-quality cameras (landmark detection may be less accurate)
  MIN_FACE_SYMMETRY: 0.23, // Minimum facial symmetry score (lowered from 0.30 to 0.23 for better user experience)
  MIN_FACE_SYMMETRY_STRICT: 0.85, // Strict threshold for high-security scenarios
  
  // Active learning
  ENABLE_ACTIVE_LEARNING: true, // Enable active learning system
  FAILED_MATCH_THRESHOLD: 10, // Number of failed matches before threshold adjustment
  THRESHOLD_ADJUSTMENT_STEP: 0.01, // Step size for threshold adjustments (1%)
  
  // Time-based pattern validation
  ENABLE_TIME_VALIDATION: true, // Enable time-based pattern validation
  EXPECTED_CLOCK_IN_HOUR: 7, // Expected clock-in hour (24-hour format)
  EXPECTED_CLOCK_IN_MINUTE: 30, // Expected clock-in minute
  TIME_TOLERANCE_MINUTES: 30, // Tolerance window in minutes (before/after expected time)
  
  TARGET_SIZE: 112, // Target size for ArcFace input (112x112)
  
  // üè¶ BANK-GRADE: Canonical preprocessing pipeline
  CANONICAL_SIZE: 1120, // Single canonical size for detection + recognition (bank standard)
  // This ensures consistent preprocessing - same image space for both operations
  
  // SCRFD detection model requires square input (e.g., 640x640, 800x800, 1024x1024)
  DETECTION_SIZE: 640, // Square size for SCRFD face detection (optimized for speed/accuracy balance)
  
  // üè¶ BANK-GRADE: Adaptive gap rules
  HIGH_CONFIDENCE_THRESHOLD_NO_GAP: 0.88, // ‚â•88%: Accept immediately, no gap check
  MEDIUM_CONFIDENCE_MIN: 0.70, // 70-87%: Require gap
  MEDIUM_CONFIDENCE_MAX: 0.87,
  ADAPTIVE_GAP_REQUIRED: 0.08, // 8% gap required for medium confidence (CRITICAL FIX: increased from 5% to prevent false matches)
  ADAPTIVE_GAP_TOLERANCE: 0.005, // 0.5% tolerance - stricter tolerance (reduced from 1% to prevent false matches)
  
  // üè¶ BANK-GRADE: Calibrated thresholds (per quality)
  // üö® CRITICAL FIX: Thresholds set to prevent false matches while allowing legitimate matches
  // Based on calibration: Genuine mean=86.69%, Impostor mean=67.40%, EER=68.00%
  // Calibration recommends: 67.4% (FAR=1%) or 69.7% (FRR=5%)
  // We use 70% minimum to be above the 67.40% impostor score (Sethu->France mismatch)
  THRESHOLDS: {
    // High quality (quality >= 85%) - Can use moderate thresholds
    HIGH_QUALITY: {
      enrollment: 0.75,  // Stricter for enrollment
      daily: 0.70,       // Based on calibration: 69.7% for FRR=5% (balanced), rounded to 70%
      sameDevice: 0.68,  // Slightly lower for same device (trusted device)
    },
    // Medium quality (70% <= quality < 85%) - Moderate thresholds
    MEDIUM_QUALITY: {
      enrollment: 0.75,  // Same as high quality for enrollment
      daily: 0.70,       // Same as high quality (calibration-based)
      sameDevice: 0.68,  // Same as high quality
    },
    // Low quality (quality < 70%) - Stricter to prevent false matches
    LOW_QUALITY: {
      enrollment: 0.72,  // Stricter for enrollment
      daily: 0.70,       // CRITICAL: Must be above 67.40% impostor score (Sethu->France mismatch)
      sameDevice: 0.70,  // Minimum absolute threshold (same as daily for safety)
    },
  },
   
  // üè¶ BANK-GRADE: Centroid fusion weights
  CENTROID_FUSION_WEIGHT: 0.7,  // 70% weight for centroid similarity
  MAX_FUSION_WEIGHT: 0.3,       // 30% weight for max similarity
  
  // üè¶ BANK-GRADE Phase 3: Multi-signal fusion weights
  FUSION_WEIGHTS: {
    face: 0.80,        // 80% weight for face similarity (primary signal)
    temporal: 0.10,    // 10% weight for temporal trust (recent matches)
    device: 0.05,      // 5% weight for device consistency
    location: 0.05,    // 5% weight for location consistency
  },
  
  // üè¶ BANK-GRADE Phase 3: Temporal signal conditions
  TEMPORAL_MIN_BASE_SIMILARITY: 0.75,  // Only apply temporal if base similarity ‚â•75%
  TEMPORAL_MAX_BOOST: 0.05,            // Max 5% boost from temporal
  TEMPORAL_WINDOW_HOURS: 24,           // Consider matches within 24 hours
  
  // üè¶ BANK-GRADE Phase 3: Device fingerprinting
  ENABLE_DEVICE_FINGERPRINTING: true,
  DEVICE_FINGERPRINT_FIELDS: ['userAgent', 'platform', 'language', 'timezone', 'x-device-id', 'x-device-info', 'x-device-hash'],
  DEVICE_FINGERPRINT_SECRET: process.env.DEVICE_FINGERPRINT_SECRET || 'faceclock-device-secret',
  
  // üè¶ BANK-GRADE Phase 4: Device quality tracking
  ENABLE_DEVICE_QUALITY_TRACKING: true,
  DEVICE_QUALITY_HISTORY_SIZE: 30, // Track last 30 clock-ins for quality assessment
  MIN_CLOCK_INS_FOR_CLASSIFICATION: 5, // Need at least 5 clock-ins before auto-classifying
  
  // üè¶ BANK-GRADE Phase 4: Enhanced liveness
  ENABLE_ENHANCED_LIVENESS: true,
  LIVENESS_DEPTH_THRESHOLD: 0.3,       // Depth variation threshold
  LIVENESS_MOTION_THRESHOLD: 0.1,      // Motion detection threshold
  
  // üè¶ BANK-GRADE Phase 4: Risk scoring thresholds
  RISK_THRESHOLDS: {
    low: 0.3,      // Low risk: <30%
    medium: 0.6,   // Medium risk: 30-60%
    high: 0.8,     // High risk: 60-80%
    critical: 1.0, // Critical risk: >80%
  },
  RISK_WEIGHTS: {
    face: 0.4,
    quality: 0.15,
    temporal: 0.1,
    device: 0.1,
    location: 0.2,
    liveness: 0.05,
  },
};

/**
 * Runtime model download helper - downloads ZIP and extracts models
 * NOTE: This downloads a 275MB ZIP file, which may be slow at runtime
 */
async function downloadModelFile(url, filepath, timeout = 600000) {
  return new Promise((resolve, reject) => {
    const protocol = url.startsWith('https') ? https : http;
    const filename = path.basename(filepath);
    
    console.log(`üì• Downloading: ${filename}`);
    
    const file = fs.createWriteStream(filepath);
    let downloadedBytes = 0;
    let totalBytes = 0;
    
    const req = protocol.get(url, (response) => {
      // Handle redirects
      if (response.statusCode === 301 || response.statusCode === 302 || 
          response.statusCode === 307 || response.statusCode === 308) {
        file.close();
        fs.unlink(filepath, () => {});
        return resolve(downloadModelFile(response.headers.location, filepath, timeout));
      }

      if (response.statusCode !== 200) {
        file.close();
        fs.unlink(filepath, () => {});
        return reject(new Error(`HTTP ${response.statusCode}: ${response.statusMessage}`));
      }

      totalBytes = parseInt(response.headers['content-length'] || '0', 10);
      
      response.on('data', (chunk) => {
        downloadedBytes += chunk.length;
        if (totalBytes > 0) {
          const percent = ((downloadedBytes / totalBytes) * 100).toFixed(1);
          process.stdout.write(`\r   Progress: ${percent}% (${(downloadedBytes / 1024 / 1024).toFixed(2)} MB)`);
        }
      });

      response.pipe(file);

      file.on('finish', () => {
        file.close();
        const sizeMB = (fs.statSync(filepath).size / 1024 / 1024).toFixed(2);
        console.log(`\n‚úÖ Downloaded: ${filename} (${sizeMB} MB)`);
        resolve();
      });
    });

    req.on('error', (err) => {
      file.close();
      fs.unlink(filepath, () => {});
      reject(err);
    });

    req.setTimeout(timeout, () => {
      req.destroy();
      file.close();
      fs.unlink(filepath, () => {});
      reject(new Error(`Download timeout after ${timeout / 1000} seconds`));
    });
  });
}

/**
 * Extract ZIP file using system unzip or Python
 */
function extractZipFile(zipPath, extractTo) {
  const { execSync } = require('child_process');
  try {
    console.log('üì¶ Extracting ZIP file...');
    execSync(`unzip -q -o "${zipPath}" -d "${extractTo}"`, { stdio: 'inherit' });
    return true;
  } catch (error) {
    console.log('‚ö†Ô∏è  unzip command failed, trying Python...');
    try {
      const pythonScript = `import zipfile; zipfile.ZipFile('${zipPath}').extractall('${extractTo}')`;
      execSync(`python3 -c "${pythonScript}"`, { stdio: 'inherit' });
      return true;
    } catch (pyError) {
      try {
        execSync(`python -c "${pythonScript}"`, { stdio: 'inherit' });
        return true;
      } catch (py2Error) {
        console.error('‚ùå Failed to extract ZIP:', py2Error.message);
        return false;
      }
    }
  }
}

/**
 * Attempt to download missing models at runtime
 * Downloads buffalo_l.zip (275MB) and extracts required ONNX files
 */
async function downloadMissingModels(modelsPath) {
  // Ensure models directory exists
  if (!fs.existsSync(modelsPath)) {
    fs.mkdirSync(modelsPath, { recursive: true });
    console.log(`üìÅ Created models directory: ${modelsPath}`);
  }

  const BUFFALO_L_ZIP_URL = 'https://github.com/deepinsight/insightface/releases/download/v0.7/buffalo_l.zip';
  const tempDir = path.join(__dirname, '../temp_models_runtime');
  const zipPath = path.join(tempDir, 'buffalo_l.zip');
  const extractDir = path.join(tempDir, 'buffalo_l');

  // Check what's missing
  const filepath = path.join(modelsPath, 'scrfd_10g_gnkps_fp32.onnx');
  const filepath500m = path.join(modelsPath, 'scrfd_500m_bnkps.onnx');
  const recPath = path.join(modelsPath, 'w600k_r50.onnx');
  
  const needsDetection = !fs.existsSync(filepath) && !fs.existsSync(filepath500m);
  const needsRecognition = !fs.existsSync(recPath);

  if (!needsDetection && !needsRecognition) {
    return true; // All required models exist
  }

  console.log('üîÑ Attempting to download missing models at runtime...');
  console.log('‚ö†Ô∏è  This will download a 275MB ZIP file, which may take several minutes...');

  try {
    // Create temp directory
    if (!fs.existsSync(tempDir)) {
      fs.mkdirSync(tempDir, { recursive: true });
    }

    // Download ZIP
    await downloadModelFile(BUFFALO_L_ZIP_URL, zipPath, 600000); // 10 minute timeout

    // Extract ZIP
    if (!extractZipFile(zipPath, tempDir)) {
      console.error('‚ùå Failed to extract ZIP file');
      return false;
    }

    // Copy required ONNX files - map ZIP filenames to expected filenames
    const filesToCopy = [
      { zipFilename: 'det_10g.onnx', expectedFilename: 'scrfd_10g_gnkps_fp32.onnx', dst: filepath, required: needsDetection },
      { zipFilename: '2d106det.onnx', expectedFilename: 'scrfd_500m_bnkps.onnx', dst: filepath500m, required: false },
      { zipFilename: 'w600k_r50.onnx', expectedFilename: 'w600k_r50.onnx', dst: recPath, required: needsRecognition }
    ];

    // Function to find file recursively
    function findFileRecursive(dir, filename) {
      try {
        const items = fs.readdirSync(dir, { withFileTypes: true });
        for (const item of items) {
          const fullPath = path.join(dir, item.name);
          if (item.isDirectory()) {
            const found = findFileRecursive(fullPath, filename);
            if (found) return found;
          } else if (item.name === filename) {
            return fullPath;
          }
        }
      } catch (error) {
        // Ignore errors
      }
      return null;
    }

    let copiedCount = 0;
    for (const file of filesToCopy) {
      if (!file.required) continue; // Only copy required files at runtime
      
      // Search recursively for the ZIP filename
      const foundPath = findFileRecursive(tempDir, file.zipFilename);
      
      if (foundPath) {
        fs.copyFileSync(foundPath, file.dst);
        console.log(`   ‚úÖ Copied: ${file.zipFilename} ‚Üí ${file.expectedFilename}`);
        copiedCount++;
      } else {
        console.warn(`   ‚ö†Ô∏è  Not found in ZIP: ${file.zipFilename}`);
      }
    }

    // Cleanup
    try {
      if (fs.existsSync(extractDir)) {
        fs.rmSync(extractDir, { recursive: true, force: true });
      }
      if (fs.existsSync(zipPath)) {
        fs.unlinkSync(zipPath);
      }
      if (fs.existsSync(tempDir)) {
        fs.rmdirSync(tempDir);
      }
    } catch (cleanupError) {
      console.warn(`‚ö†Ô∏è  Cleanup warning: ${cleanupError.message}`);
    }

    // Verify
    const hasDetection = fs.existsSync(filepath) || fs.existsSync(filepath500m);
    const hasRecognition = fs.existsSync(recPath);

    if (hasDetection && hasRecognition) {
      console.log('‚úÖ Runtime model download completed successfully');
      return true;
    } else {
      console.warn('‚ö†Ô∏è  Some models failed to download. Required models may still be missing.');
      return false;
    }
  } catch (error) {
    console.error(`‚ùå Runtime download failed: ${error.message}`);
    // Cleanup on error
    try {
      if (fs.existsSync(zipPath)) fs.unlinkSync(zipPath);
      if (fs.existsSync(extractDir)) fs.rmSync(extractDir, { recursive: true, force: true });
      if (fs.existsSync(tempDir)) fs.rmdirSync(tempDir);
    } catch (e) {}
    return false;
  }
}

/**
 * Load ONNX models (SCRFD + ArcFace)
 */
async function loadModels() {
  if (modelsLoaded) {
    return;
  }

  if (modelsPromise) {
    return modelsPromise;
  }

  const modelsPath = path.join(__dirname, '../models/onnx');
  modelsPromise = (async () => {
    modelsLoadError = null;

    try {
      // Check if models exist, if not, attempt runtime download
      let detectionModelPath = path.join(modelsPath, 'scrfd_10g_gnkps_fp32.onnx');
      let hasDetection = fs.existsSync(detectionModelPath);
      
      if (!hasDetection) {
        detectionModelPath = path.join(modelsPath, 'scrfd_500m_bnkps.onnx');
        hasDetection = fs.existsSync(detectionModelPath);
      }
      
      let recognitionModelPath = path.join(modelsPath, 'w600k_r50.onnx');
      const altRecognitionPath = path.join(modelsPath, 'glint360k_r50.onnx');
      let hasRecognition = fs.existsSync(recognitionModelPath) || fs.existsSync(altRecognitionPath);

      // If models are missing, attempt runtime download
      if (!hasDetection || !hasRecognition) {
        console.log('‚ö†Ô∏è  Models not found, attempting runtime download...');
        await downloadMissingModels(modelsPath);
        
        // Re-check after download attempt
        detectionModelPath = path.join(modelsPath, 'scrfd_10g_gnkps_fp32.onnx');
        hasDetection = fs.existsSync(detectionModelPath);
        if (!hasDetection) {
          detectionModelPath = path.join(modelsPath, 'scrfd_500m_bnkps.onnx');
          hasDetection = fs.existsSync(detectionModelPath);
        }
        
        recognitionModelPath = path.join(modelsPath, 'w600k_r50.onnx');
        hasRecognition = fs.existsSync(recognitionModelPath) || fs.existsSync(altRecognitionPath);
      }

      // Load SCRFD detection model
      // Try 10G model first (better accuracy), fallback to 500M
      if (!hasDetection) {
        const errorMsg = `\n‚ùå Detection model not found.\n\n` +
          `Expected one of:\n` +
          `  - scrfd_10g_gnkps_fp32.onnx (preferred)\n` +
          `  - scrfd_500m_bnkps.onnx (fallback)\n\n` +
          `üí° Solutions:\n` +
          `   1. Place detection model in: ${modelsPath}\n` +
          `   2. Run: npm run download-models\n` +
          `   3. Runtime download was attempted but failed\n`;
        throw new Error(errorMsg);
      }

      console.log('üì¶ Loading SCRFD face detection model...');
      detectionModel = await ort.InferenceSession.create(detectionModelPath, {
        executionProviders: ['cpu'], // Use 'cuda' for GPU if available
        graphOptimizationLevel: 'all', // Optimize graph to reduce memory
        enableMemPattern: false, // Disable memory pattern to reduce overhead
        enableCpuMemArena: false, // Disable CPU memory arena to reduce memory usage
      });
      // Log input/output names for debugging
      console.log('   üìã Detection model inputs:', JSON.stringify(detectionModel.inputNames));
      console.log('   üìã Detection model outputs:', JSON.stringify(detectionModel.outputNames));
      console.log('‚úÖ SCRFD detection model loaded');

      // Load ArcFace recognition model
      if (!hasRecognition) {
        const errorMsg = `\n‚ùå Recognition model not found.\n\n` +
          `Expected one of:\n` +
          `  - ${recognitionModelPath}\n` +
          `  - ${altRecognitionPath}\n\n` +
          `üí° Solutions:\n` +
          `   1. Run: npm run download-models\n` +
          `   2. If download fails, see ALTERNATIVE_MODEL_SOURCES.md\n` +
          `   3. Download models manually from:\n` +
          `      - Hugging Face: https://huggingface.co/models?search=arcface+onnx\n` +
          `      - Place in: ${modelsPath}\n` +
          `   4. Runtime download was attempted but failed\n`;
        throw new Error(errorMsg);
      }

      // Try w600k first, fallback to glint360k
      // MEMORY OPTIMIZATION: Use session options to reduce memory usage
      if (fs.existsSync(recognitionModelPath)) {
        console.log('üì¶ Loading ArcFace recognition model (w600k)...');
        console.log('   ‚ö†Ô∏è  Large model (166MB) - optimizing memory usage...');
        recognitionModel = await ort.InferenceSession.create(recognitionModelPath, {
          executionProviders: ['cpu'],
          graphOptimizationLevel: 'all', // Optimize graph to reduce memory
          enableMemPattern: false, // Disable memory pattern to reduce overhead
          enableCpuMemArena: false, // Disable CPU memory arena to reduce memory usage
          sessionOptions: {
            // Additional memory optimizations
            inter_op_num_threads: 1, // Use single thread to reduce memory
            intra_op_num_threads: 1, // Use single thread to reduce memory
          },
        });
        console.log('   ‚úÖ Recognition model loaded');
        console.log('   üìã Input names:', JSON.stringify(recognitionModel.inputNames));
        console.log('   üìã Output names:', JSON.stringify(recognitionModel.outputNames));
        console.log('   üìã Input metadata:', JSON.stringify(recognitionModel.inputMetadata));
      } else if (fs.existsSync(altRecognitionPath)) {
        console.log('üì¶ Loading ArcFace recognition model (glint360k)...');
        console.log('   ‚ö†Ô∏è  Large model - optimizing memory usage...');
        recognitionModel = await ort.InferenceSession.create(altRecognitionPath, {
          executionProviders: ['cpu'],
          graphOptimizationLevel: 'all',
          enableMemPattern: false,
          enableCpuMemArena: false,
          sessionOptions: {
            inter_op_num_threads: 1,
            intra_op_num_threads: 1,
          },
        });
        console.log('   Recognition model inputs:', recognitionModel.inputNames);
        console.log('   Recognition model outputs:', recognitionModel.outputNames);
      }
      console.log('‚úÖ ArcFace recognition model loaded');

      modelsLoaded = true;
      console.log('‚úÖ All ONNX models loaded successfully');
      return;
    } catch (error) {
      modelsLoadError = error;
      modelsLoaded = false;
      console.error('‚ùå Error loading ONNX models:', error.message);
      throw error;
    }
  })();

  try {
    await modelsPromise;
  } catch (error) {
    modelsLoadError = error;
    modelsLoaded = false;
    throw error;
  } finally {
    modelsPromise = null;
  }
}

/**
 * Detect blur using Laplacian variance (higher = sharper)
 * Enterprise-grade quality gate
 */
async function detectBlur(imageBuffer) {
  try {
    const image = sharp(imageBuffer);
    const metadata = await image.metadata();
    
    // Convert to grayscale for blur detection
    const grayscale = await image
      .greyscale()
      .raw()
      .toBuffer();
    
    // Calculate Laplacian variance
    let laplacianSum = 0;
    let laplacianSquared = 0;
    let pixelCount = 0;
    
    const width = metadata.width;
    const height = metadata.height;
    
    for (let y = 1; y < height - 1; y++) {
      for (let x = 1; x < width - 1; x++) {
        const idx = y * width + x;
        const left = y * width + (x - 1);
        const right = y * width + (x + 1);
        const top = (y - 1) * width + x;
        const bottom = (y + 1) * width + x;
        
        // Laplacian kernel: center * 4 - neighbors
        const laplacian = Math.abs(
          4 * grayscale[idx] - 
          grayscale[left] - 
          grayscale[right] - 
          grayscale[top] - 
          grayscale[bottom]
        );
        
        laplacianSum += laplacian;
        laplacianSquared += laplacian * laplacian;
        pixelCount++;
      }
    }
    
    const mean = laplacianSum / pixelCount;
    const variance = (laplacianSquared / pixelCount) - (mean * mean);
    
    return {
      variance,
      isBlurry: variance < CONFIG.MIN_BLUR_THRESHOLD,
      score: Math.min(1, variance / 500) // Normalize to 0-1 (500 is typical sharp image)
    };
  } catch (error) {
    console.warn('‚ö†Ô∏è Blur detection failed:', error.message);
    return { variance: 0, isBlurry: false, score: 0.5 };
  }
}

/**
 * Check liveness using facial geometry analysis
 * ENTERPRISE: Detects photos vs live faces using geometric properties
 */
function checkLiveness(landmarks, faceBox) {
  try {
    const { leftEye, rightEye, nose, leftMouth, rightMouth } = landmarks;
    
    // Calculate eye distance ratio (photos often have different proportions)
    const eyeDistance = Math.sqrt(
      Math.pow(rightEye.x - leftEye.x, 2) + 
      Math.pow(rightEye.y - leftEye.y, 2)
    );
    const faceWidth = faceBox.width;
    const eyeDistanceRatio = eyeDistance / faceWidth;
    
    // Check if eye distance is within normal human range
    if (eyeDistanceRatio < CONFIG.MIN_EYE_DISTANCE_RATIO || 
        eyeDistanceRatio > CONFIG.MAX_EYE_DISTANCE_RATIO) {
      return {
        isLive: false,
        reason: `Eye spacing abnormal (${(eyeDistanceRatio * 100).toFixed(1)}% of face width, expected ${(CONFIG.MIN_EYE_DISTANCE_RATIO * 100).toFixed(0)}-${(CONFIG.MAX_EYE_DISTANCE_RATIO * 100).toFixed(0)}%)`,
        score: 0,
        eyeDistanceRatio,
        symmetry: 0
      };
    }
    
    // Calculate facial symmetry (live faces are more symmetric than photos at angles)
    const faceCenterX = faceBox.x + faceBox.width / 2;
    const faceCenterY = faceBox.y + faceBox.height / 2;
    
    // Check symmetry of eyes relative to face center
    const leftEyeDist = Math.sqrt(
      Math.pow(leftEye.x - faceCenterX, 2) + 
      Math.pow(leftEye.y - faceCenterY, 2)
    );
    const rightEyeDist = Math.sqrt(
      Math.pow(rightEye.x - faceCenterX, 2) + 
      Math.pow(rightEye.y - faceCenterY, 2)
    );
    const eyeSymmetry = 1 - Math.abs(leftEyeDist - rightEyeDist) / Math.max(leftEyeDist, rightEyeDist);
    
    // Check nose position (should be centered)
    const noseDistFromCenter = Math.abs(nose.x - faceCenterX) / faceWidth;
    const noseSymmetry = 1 - Math.min(1, noseDistFromCenter * 2);
    
    // Check mouth symmetry
    const mouthCenterX = (leftMouth.x + rightMouth.x) / 2;
    const mouthDistFromCenter = Math.abs(mouthCenterX - faceCenterX) / faceWidth;
    const mouthSymmetry = 1 - Math.min(1, mouthDistFromCenter * 2);
    
    // Overall symmetry score (weighted average)
    const symmetry = (eyeSymmetry * 0.4 + noseSymmetry * 0.3 + mouthSymmetry * 0.3);
    
    if (symmetry < CONFIG.MIN_FACE_SYMMETRY) {
      return {
        isLive: false,
        reason: `Face not centered properly. Please look straight at the camera and ensure your face is centered in the frame.`,
        score: symmetry,
        eyeDistanceRatio,
        symmetry
      };
    }
    
    // Calculate overall liveness score
    const eyeScore = eyeDistanceRatio >= CONFIG.MIN_EYE_DISTANCE_RATIO && 
                     eyeDistanceRatio <= CONFIG.MAX_EYE_DISTANCE_RATIO ? 1.0 : 0.5;
    const overallScore = (eyeScore * 0.4 + symmetry * 0.6);
    
    return {
      isLive: true,
      score: overallScore,
      eyeDistanceRatio,
      symmetry,
      eyeSymmetry,
      noseSymmetry,
      mouthSymmetry
    };
  } catch (error) {
    console.warn('‚ö†Ô∏è Liveness check failed:', error.message);
    // Fail open - allow if check fails (don't block legitimate users)
    return {
      isLive: true,
      score: 0.5,
      eyeDistanceRatio: 0,
      symmetry: 0.5,
      reason: 'Liveness check error (allowed)'
    };
  }
}

/**
 * Calculate image brightness (normalized 0-1)
 */
async function calculateBrightness(imageBuffer) {
  try {
    const image = sharp(imageBuffer);
    const stats = await image
      .greyscale()
      .raw()
      .toBuffer({ resolveWithObject: true });
    
    let sum = 0;
    for (let i = 0; i < stats.data.length; i++) {
      sum += stats.data[i];
    }
    
    const avgBrightness = sum / stats.data.length;
    return avgBrightness / 255; // Normalize to 0-1
  } catch (error) {
    console.warn('‚ö†Ô∏è Brightness calculation failed:', error.message);
    return 0.5; // Default to medium brightness
  }
}

/**
 * üè¶ BANK-GRADE: Automatically enhance image quality (sharpening, denoising, contrast)
 * This helps improve face recognition accuracy for slightly blurry or low-quality images
 * Supports aggressive enhancement for low-quality cameras
 * 
 * @param {Buffer} imageBuffer - Original image buffer
 * @param {number} aggressiveness - Enhancement aggressiveness multiplier (1.0 = normal, >1.0 = more aggressive)
 * @returns {Buffer} - Enhanced image buffer
 */
async function enhanceImage(imageBuffer, aggressiveness = 1.0) {
  try {
    const image = sharp(imageBuffer);
    
    // Calculate enhancement parameters based on aggressiveness
    const sharpenSigma = 1.5 * aggressiveness;
    const sharpenM1 = 1.0 * aggressiveness;
    const sharpenM2 = 3.0 * aggressiveness;
    
    // Apply automatic enhancement pipeline:
    // 1. Denoise (reduces noise while preserving edges) - more aggressive for low-quality
    // 2. Sharpen (enhances edges and details) - strength scales with aggressiveness
    // 3. Normalize (improves contrast)
    const enhanced = await image
      .sharpen({ 
        sigma: Math.min(sharpenSigma, 3.0), // Cap at 3.0 to avoid over-sharpening artifacts
        m1: Math.min(sharpenM1, 2.0), 
        m2: Math.min(sharpenM2, 5.0), 
        x1: 2, 
        y2: 10, 
        y3: 20 
      })
      .normalize() // Auto-contrast adjustment
      .toBuffer();
    
    return enhanced;
  } catch (error) {
    console.warn('‚ö†Ô∏è Image enhancement failed, using original:', error.message);
    return imageBuffer; // Return original if enhancement fails
  }
}

/**
 * üè¶ BANK-GRADE: Automatically correct brightness if slightly out of range
 * Only corrects if brightness is within recoverable range
 * 
 * @param {Buffer} imageBuffer - Original image buffer
 * @param {number} currentBrightness - Current brightness (0-1)
 * @returns {Buffer} - Corrected image buffer
 */
async function correctBrightness(imageBuffer, currentBrightness) {
  try {
    const image = sharp(imageBuffer);
    const targetBrightness = 0.5; // Target middle brightness
    
    // Calculate adjustment factor (limited to avoid over-correction)
    const brightnessAdjustment = targetBrightness - currentBrightness;
    const maxAdjustment = 0.3; // Max 30% adjustment
    const limitedAdjustment = Math.max(-maxAdjustment, Math.min(maxAdjustment, brightnessAdjustment));
    
    // Apply brightness correction
    const corrected = await image
      .linear(1.0, limitedAdjustment * 255) // linear(a, b) = a * pixel + b
      .toBuffer();
    
    console.log(`   ‚úÖ Brightness auto-corrected: ${(currentBrightness * 100).toFixed(1)}% ‚Üí ~${(targetBrightness * 100).toFixed(0)}%`);
    
    return corrected;
  } catch (error) {
    console.warn('‚ö†Ô∏è Brightness correction failed, using original:', error.message);
    return imageBuffer; // Return original if correction fails
  }
}

/**
 * üè¶ BANK-GRADE Phase 4: Get device quality from database (if tracking enabled)
 * Returns device quality tier and metrics for adaptive threshold adjustment
 * 
 * @param {string} deviceFingerprint - Device fingerprint
 * @returns {Promise<Object|null>} - Device quality record or null
 */
async function getDeviceQuality(deviceFingerprint) {
  if (!deviceFingerprint || !CONFIG.ENABLE_DEVICE_QUALITY_TRACKING) {
    return null;
  }
  
  try {
    const DeviceQuality = require('../models/DeviceQuality');
    const deviceQuality = await DeviceQuality.findOne({ deviceFingerprint }).lean();
    
    if (deviceQuality) {
      console.log(`   üì± Device quality: ${deviceQuality.qualityTier} tier (blur: ${deviceQuality.averageBlurVariance.toFixed(1)}, width: ${deviceQuality.averageImageWidth.toFixed(0)}px, score: ${(deviceQuality.averageQualityScore * 100).toFixed(1)}%)`);
    }
    
    return deviceQuality;
  } catch (error) {
    console.warn('‚ö†Ô∏è Error fetching device quality:', error.message);
    return null; // Fail silently, don't break preprocessing
  }
}

/**
 * üè¶ BANK-GRADE Phase 4: Track device quality metrics after successful clock-in
 * Updates device quality history for adaptive threshold adjustment
 * 
 * @param {string} deviceFingerprint - Device fingerprint
 * @param {Object} qualityMetrics - Quality metrics { blurVariance, imageWidth, imageHeight, brightness, qualityScore }
 */
async function trackDeviceQuality(deviceFingerprint, qualityMetrics) {
  if (!deviceFingerprint || !CONFIG.ENABLE_DEVICE_QUALITY_TRACKING || !qualityMetrics) {
    return; // Skip if tracking disabled or no metrics
  }
  
  try {
    const DeviceQuality = require('../models/DeviceQuality');
    const deviceQuality = await DeviceQuality.getOrCreate(deviceFingerprint);
    
    // Update quality metrics
    await deviceQuality.updateQualityMetrics(qualityMetrics);
    
    console.log(`   üìä Device quality updated: ${deviceQuality.qualityTier} tier (${deviceQuality.totalClockIns} clock-ins)`);
  } catch (error) {
    console.warn('‚ö†Ô∏è Error tracking device quality:', error.message);
    // Fail silently, don't break clock-in process
  }
}

/**
 * üè¶ BANK-GRADE: Canonical preprocessing pipeline
 * Preprocesses image to a single canonical size for both detection and recognition
 * Ensures consistent preprocessing - same image space for both operations
 * 
 * @param {Buffer} imageBuffer - Original image buffer
 * @param {string} deviceFingerprint - Optional device fingerprint for quality tracking
 * @returns {Object} - Preprocessed image data with canonical size and original metadata
 */
async function preprocessCanonical(imageBuffer, deviceFingerprint = null) {
  let image = sharp(imageBuffer);
  const metadata = await image.metadata();
  
  // üè¶ BANK-GRADE: Adaptive image size validation (lenient for low-quality cameras)
  // Allow smaller images but warn if too small
  let imageSizeWarning = null;
  if (metadata.width < CONFIG.MIN_IMAGE_WIDTH_STRICT) {
  if (metadata.width < CONFIG.MIN_IMAGE_WIDTH) {
      throw new Error(`Image too small: ${metadata.width}px width (minimum: ${CONFIG.MIN_IMAGE_WIDTH}px). Please use a better camera or move closer.`);
    } else {
      // Between lenient and strict minimum - warn but allow
      imageSizeWarning = `Low resolution image (${metadata.width}px). Higher resolution cameras are recommended for better accuracy.`;
      console.warn(`   ‚ö†Ô∏è ${imageSizeWarning}`);
    }
  }
  
  // üè¶ BANK-GRADE: Adaptive brightness validation (lenient for poor lighting)
  const brightness = await calculateBrightness(imageBuffer);
  let brightnessWarning = null;
  if (brightness < CONFIG.MIN_BRIGHTNESS || brightness > CONFIG.MAX_BRIGHTNESS) {
    // For very poor brightness, still reject
    const veryDark = brightness < CONFIG.MIN_BRIGHTNESS * 0.8; // 20% below minimum
    const veryBright = brightness > CONFIG.MAX_BRIGHTNESS * 1.1; // 10% above maximum
    
    if (veryDark || veryBright) {
    throw new Error(`Image brightness out of range: ${(brightness * 100).toFixed(1)}% (required: ${(CONFIG.MIN_BRIGHTNESS * 100).toFixed(0)}-${(CONFIG.MAX_BRIGHTNESS * 100).toFixed(0)}%). Please adjust lighting.`);
    } else {
      // Slightly out of range - warn but allow with auto-correction
      brightnessWarning = `Suboptimal brightness: ${(brightness * 100).toFixed(1)}%. Auto-correcting...`;
      console.warn(`   ‚ö†Ô∏è ${brightnessWarning}`);
      // Auto-correct brightness
      imageBuffer = await correctBrightness(imageBuffer, brightness);
    }
  }
  
  // üè¶ BANK-GRADE Phase 4: Check device quality history for adaptive thresholds
  let deviceQuality = null;
  let isKnownLowQualityDevice = false;
  
  if (deviceFingerprint && CONFIG.ENABLE_DEVICE_QUALITY_TRACKING) {
    deviceQuality = await getDeviceQuality(deviceFingerprint);
    if (deviceQuality && deviceQuality.totalClockIns >= CONFIG.MIN_CLOCK_INS_FOR_CLASSIFICATION) {
      isKnownLowQualityDevice = deviceQuality.qualityTier === 'low';
      if (isKnownLowQualityDevice) {
        console.log(`   üì± Known low-quality device detected (${deviceQuality.totalClockIns} clock-ins, avg blur: ${deviceQuality.averageBlurVariance.toFixed(1)}, avg width: ${deviceQuality.averageImageWidth.toFixed(0)}px)`);
      }
    }
  }
  
  // üè¶ BANK-GRADE: Aggressive automatic enhancement for low-quality cameras
  let enhancedBuffer = imageBuffer;
  let blurResult = await detectBlur(imageBuffer);
  let enhancementAttempted = false;
  let enhancementImproved = false;
  
  // Detect if this is likely a low-quality camera (small resolution + blur OR known low-quality device)
  const isLowQualityCamera = isKnownLowQualityDevice || (metadata.width < CONFIG.MIN_IMAGE_WIDTH_STRICT && blurResult.variance < CONFIG.MIN_BLUR_THRESHOLD);
  
  if (CONFIG.ENABLE_AUTO_ENHANCEMENT) {
    // üêõ CRITICAL FIX: Attempt enhancement for ANY blurry image, not just those above threshold
    // Very blurry images (variance < 20) should ALWAYS get enhancement attempt
    // This helps ALL staff with blurry images, not just low-quality cameras
    const isVeryBlurry = blurResult.variance < 20; // Very blurry threshold
    const shouldEnhance = blurResult.isBlurry && (
      blurResult.variance > CONFIG.MIN_BLUR_THRESHOLD * 0.3 || // Above 30% of threshold (18)
      isLowQualityCamera || // Always try to enhance low-quality camera images
      isVeryBlurry // üêõ FIX: Always attempt enhancement for very blurry images (< 20 variance)
    );
    
    if (shouldEnhance) {
      enhancementAttempted = true;
      const enhancementType = isLowQualityCamera ? 'aggressive' : (isVeryBlurry ? 'aggressive (very blurry)' : 'moderate');
      console.log(`   üîß ${isVeryBlurry ? 'Very blurry image' : isLowQualityCamera ? 'Low-quality camera' : 'Blurry image'} detected: Applying ${enhancementType} enhancement...`);
      console.log(`   üìä Before: Variance=${blurResult.variance.toFixed(1)}, Sharpness=${(blurResult.score * 100).toFixed(1)}%`);
      
      // Use aggressive enhancement for very blurry images or low-quality cameras
      const aggressiveness = (isLowQualityCamera || isVeryBlurry) ? CONFIG.ENHANCEMENT_AGGRESSIVENESS : 1.0;
      enhancedBuffer = await enhanceImage(imageBuffer, aggressiveness);
    
      // Re-check blur after enhancement
      const enhancedBlurResult = await detectBlur(enhancedBuffer);
      if (enhancedBlurResult.variance > blurResult.variance) {
        enhancementImproved = true;
        console.log(`   ‚úÖ Image enhanced: sharpness improved from ${(blurResult.score * 100).toFixed(1)}% to ${(enhancedBlurResult.score * 100).toFixed(1)}%`);
        console.log(`   üìä After: Variance=${enhancedBlurResult.variance.toFixed(1)}, Sharpness=${(enhancedBlurResult.score * 100).toFixed(1)}%`);
        blurResult = enhancedBlurResult;
        imageBuffer = enhancedBuffer; // Use enhanced image for processing
      } else {
        console.log(`   ‚ö†Ô∏è Enhancement didn't improve sharpness (${enhancedBlurResult.variance.toFixed(1)} vs ${blurResult.variance.toFixed(1)}), using original image`);
      }
    } else if (blurResult.isBlurry) {
      // Image is blurry but enhancement wasn't attempted (shouldn't happen, but log it)
      console.warn(`   ‚ö†Ô∏è Image is blurry (variance: ${blurResult.variance.toFixed(1)}) but enhancement not attempted`);
    }
  }
  
  // üè¶ BANK-GRADE: Adaptive blur threshold (lenient for low-quality cameras and enhanced images)
  // Determine effective threshold based on image quality and enhancement results
  let effectiveBlurThreshold;
  
  // üêõ CRITICAL FIX: Very blurry images (< 20 variance) should get lenient threshold even if enhancement failed
  const isVeryBlurry = blurResult.variance < 20;
  
  if (isLowQualityCamera) {
    // Very lenient for low-quality cameras
    effectiveBlurThreshold = CONFIG.MIN_BLUR_THRESHOLD_LOW_CAMERA; // 40
    console.log(`   üì± Low-quality camera mode: Using lenient blur threshold (${effectiveBlurThreshold})`);
  } else if (enhancementImproved) {
    // Enhanced image - use relaxed threshold
    effectiveBlurThreshold = CONFIG.MIN_BLUR_THRESHOLD * 0.6; // 60% of normal (36)
    console.log(`   ‚úÖ Enhanced image: Using relaxed blur threshold (${effectiveBlurThreshold})`);
  } else if (enhancementAttempted || isVeryBlurry) {
    // üêõ FIX: Enhancement attempted OR very blurry image - be lenient
    // This helps ALL staff with blurry images, even if enhancement didn't help
    effectiveBlurThreshold = CONFIG.MIN_BLUR_THRESHOLD * 0.5; // 50% of normal (30) - more lenient
    console.log(`   ‚ö†Ô∏è ${isVeryBlurry ? 'Very blurry image' : 'Enhancement attempted'}: Using lenient threshold (${effectiveBlurThreshold})`);
  } else if (metadata.width < CONFIG.MIN_IMAGE_WIDTH_STRICT) {
    // Medium-quality camera - moderate threshold
    effectiveBlurThreshold 
    = CONFIG.MIN_BLUR_THRESHOLD * 0.8; // 80% of normal (48)
    console.log(`   üì∏ Medium-quality camera: Using relaxed threshold (${effectiveBlurThreshold})`);
  } else {
    // High-quality camera - use normal threshold
    effectiveBlurThreshold = CONFIG.MIN_BLUR_THRESHOLD; // 60
    console.log(`   ‚úÖ High-quality camera: Using standard threshold (${effectiveBlurThreshold})`);
  }
  
  // üêõ CRITICAL FIX: For very blurry images, use even more lenient threshold if still below
  // This ensures fairness for ALL staff with genuinely blurry images
  if (isVeryBlurry && blurResult.variance < effectiveBlurThreshold) {
    // Very blurry images get minimum threshold (40) if enhancement didn't help enough
    const veryBlurryThreshold = Math.max(CONFIG.MIN_BLUR_THRESHOLD_LOW_CAMERA, blurResult.variance * 1.5);
    if (veryBlurryThreshold < effectiveBlurThreshold) {
      effectiveBlurThreshold = veryBlurryThreshold;
      console.log(`   üîß Very blurry image: Using minimum viable threshold (${effectiveBlurThreshold.toFixed(1)})`);
    }
  }
  
  // Only reject if below lenient threshold
  if (blurResult.variance < effectiveBlurThreshold) {
    const suggestedThreshold = isLowQualityCamera ? CONFIG.MIN_BLUR_THRESHOLD_LOW_CAMERA : effectiveBlurThreshold;
    throw new Error(`Image is too blurry (sharpness: ${(blurResult.score * 100).toFixed(1)}%, variance: ${blurResult.variance.toFixed(1)}, required: ${effectiveBlurThreshold.toFixed(1)}). ${isLowQualityCamera ? 'Your camera quality is low. ' : isVeryBlurry ? 'Image is very blurry. ' : ''}Please ensure camera is focused and face is still. Try holding the camera steady or moving to better lighting.`);
  }
  
  // Warn if image quality is marginal but acceptable
  if (blurResult.variance < CONFIG.MIN_BLUR_THRESHOLD) {
    console.warn(`   ‚ö†Ô∏è Image quality is marginal (variance: ${blurResult.variance.toFixed(1)} < ${CONFIG.MIN_BLUR_THRESHOLD}). Accuracy may be reduced.`);
  }
  
  console.log(`   ‚úÖ Quality gates passed: Size=${metadata.width}x${metadata.height}, Brightness=${(brightness * 100).toFixed(1)}%, Sharpness=${(blurResult.score * 100).toFixed(1)}%`);
  
  // üè¶ BANK-GRADE: Resize to CANONICAL_SIZE (single size for detection + recognition)
  image = sharp(imageBuffer);
  const canonicalImage = image.resize(CONFIG.CANONICAL_SIZE, CONFIG.CANONICAL_SIZE, { fit: 'inside' });
  const canonicalMetadata = await canonicalImage.metadata();
  
  // Calculate scale factors for coordinate conversion
  const scaleX = canonicalMetadata.width / metadata.width;
  const scaleY = canonicalMetadata.height / metadata.height;
  
  console.log(`   üè¶ Canonical preprocessing: ${metadata.width}x${metadata.height} ‚Üí ${canonicalMetadata.width}x${canonicalMetadata.height} (scale: ${scaleX.toFixed(3)}x, ${scaleY.toFixed(3)}x)`);
  
  // Get canonical image buffer for later use (detection + recognition)
  const canonicalBuffer = await canonicalImage.toBuffer();
  
  // üè¶ BANK-GRADE Phase 4: Prepare quality metrics for tracking
  const qualityMetrics = {
    blurVariance: blurResult.variance,
    imageWidth: metadata.width,
    imageHeight: metadata.height,
    brightness: brightness,
    qualityScore: blurResult.score,
  };
  
  return {
    canonicalBuffer,           // Canonical image buffer (used for both detection and recognition)
    canonicalWidth: canonicalMetadata.width,
    canonicalHeight: canonicalMetadata.height,
    originalWidth: metadata.width,
    originalHeight: metadata.height,
    scaleX,                    // Scale factor X (canonical / original)
    scaleY,                    // Scale factor Y (canonical / original)
    quality: {
      brightness,
      blurVariance: blurResult.variance,
      blurScore: blurResult.score,
    },
    qualityMetrics,            // üè¶ BANK-GRADE: Quality metrics for device tracking
    deviceQuality,             // üè¶ BANK-GRADE: Device quality record (if available)
    isLowQualityDevice: isKnownLowQualityDevice || isLowQualityCamera,
  };
}

/**
 * Preprocess canonical image for SCRFD detection
 * CRITICAL: SCRFD requires SQUARE input (e.g., 640x640, 800x800)
 * Resizes canonical image to square size for detection model
 */
async function preprocessForDetection(canonicalBuffer, canonicalWidth, canonicalHeight) {
  // üè¶ BANK-GRADE: Resize to square size for SCRFD (model requires square input)
  // Calculate scale factor to maintain aspect ratio and resize to square
  const detectionSize = CONFIG.DETECTION_SIZE; // 640x640
  
  console.log(`   üîß Resizing for SCRFD detection: ${canonicalWidth}x${canonicalHeight} ‚Üí ${detectionSize}x${detectionSize} (square)`);
  
  // Resize to square using 'cover' to fill entire square (may crop edges)
  // 'cover' ensures the square is filled without letterboxing
  const detectionBuffer = await sharp(canonicalBuffer)
    .resize(detectionSize, detectionSize, { 
      fit: 'cover', // Fill entire square (crop if needed to maintain aspect ratio)
      position: 'center' // Center the crop
    })
    .ensureAlpha()
    .raw()
    .toBuffer({ resolveWithObject: true });

  const rgbBuffer = detectionBuffer;
  
  // Validate dimensions are exactly square
  if (rgbBuffer.info.width !== detectionSize || rgbBuffer.info.height !== detectionSize) {
    throw new Error(`Detection resize failed: Expected ${detectionSize}x${detectionSize} but got ${rgbBuffer.info.width}x${rgbBuffer.info.height}`);
  }

  // Normalize to [0, 1] and convert to Float32Array
  const pixels = new Float32Array(rgbBuffer.data.length);
  for (let i = 0; i < rgbBuffer.data.length; i += 4) {
    pixels[i] = rgbBuffer.data[i] / 255.0;     // R
    pixels[i + 1] = rgbBuffer.data[i + 1] / 255.0; // G
    pixels[i + 2] = rgbBuffer.data[i + 2] / 255.0; // B
    // Skip alpha
  }

  // Reshape to [1, 3, DETECTION_SIZE, DETECTION_SIZE] for ONNX (square)
  const height = rgbBuffer.info.height;
  const width = rgbBuffer.info.width;
  
  if (height !== detectionSize || width !== detectionSize) {
    throw new Error(`Tensor dimension mismatch: Expected ${detectionSize}x${detectionSize} but got ${width}x${height}`);
  }
  
  const reshaped = new Float32Array(1 * 3 * height * width);
  
  // Convert HWC to CHW format
  for (let c = 0; c < 3; c++) {
    for (let h = 0; h < height; h++) {
      for (let w = 0; w < width; w++) {
        const srcIdx = (h * width + w) * 4 + c;
        const dstIdx = c * height * width + h * width + w;
        reshaped[dstIdx] = pixels[srcIdx];
      }
    }
  }

  console.log(`   ‚úÖ Detection tensor created: shape=[1, 3, ${height}, ${width}], size=${reshaped.length} elements`);

  // üè¶ BANK-GRADE: Calculate coordinate conversion factors
  // When using 'cover', Sharp scales to fill the square (maintains aspect ratio, crops if needed)
  // 'cover' uses the LARGER scale factor to fill the entire square (no letterboxing)
  // Formula: canonical_coord = (detection_coord + crop_offset) / scale
  const scaleX = detectionSize / canonicalWidth;  // Scale if width fills square
  const scaleY = detectionSize / canonicalHeight; // Scale if height fills square
  const scale = Math.max(scaleX, scaleY); // 'cover' uses MAX to fill entire square (crops excess)
  
  // Calculate scaled dimensions and crop offsets
  const scaledWidth = canonicalWidth * scale;  // Width after scaling
  const scaledHeight = canonicalHeight * scale; // Height after scaling
  // Crop offsets are the amount cropped from each side (centered crop)
  const cropOffsetX = Math.max(0, (scaledWidth - detectionSize) / 2); // Horizontal crop (if width > detectionSize)
  const cropOffsetY = Math.max(0, (scaledHeight - detectionSize) / 2); // Vertical crop (if height > detectionSize)

  console.log(`   üîß Coordinate conversion: scale=${scale.toFixed(3)}, cropOffset=(${cropOffsetX.toFixed(1)}, ${cropOffsetY.toFixed(1)})`);

  return {
    tensor: new ort.Tensor('float32', reshaped, [1, 3, height, width]),
    width,
    height,
    // Coordinate conversion factors for converting detection coords (640x640) back to canonical
    scale: scale,           // Scale factor used (cover uses MAX to fill square)
    cropOffsetX: cropOffsetX, // Horizontal crop offset in scaled space (pixels)
    cropOffsetY: cropOffsetY, // Vertical crop offset in scaled space (pixels)
    canonicalWidth: canonicalWidth,  // Original canonical width
    canonicalHeight: canonicalHeight, // Original canonical height
  };
}

/**
 * Calculate Intersection over Union (IoU) between two bounding boxes
 * IoU > 0.5 typically means same face (overlapping detections)
 */
function calculateIoU(box1, box2) {
  const x1 = Math.max(box1.x, box2.x);
  const y1 = Math.max(box1.y, box2.y);
  const x2 = Math.min(box1.x + box1.width, box2.x + box2.width);
  const y2 = Math.min(box1.y + box1.height, box2.y + box2.height);
  
  if (x2 < x1 || y2 < y1) {
    return 0; // No overlap
  }
  
  const intersection = (x2 - x1) * (y2 - y1);
  const area1 = box1.width * box1.height;
  const area2 = box2.width * box2.height;
  const union = area1 + area2 - intersection;
  
  return union > 0 ? intersection / union : 0;
}

/**
 * Apply Non-Maximum Suppression (NMS) to filter out duplicate/overlapping face detections
 * SCRFD can detect the same face multiple times - NMS keeps only the best non-overlapping detection(s)
 * ENTERPRISE: Prevents counting the same face multiple times
 * 
 * @param {Array} detections - Array of detection objects with box, score, landmarks
 * @param {number} iouThreshold - IoU threshold (0.5 = 50% overlap = same face, filter out)
 * @returns {Array} Filtered detections (non-overlapping)
 */
function applyNMS(detections, iouThreshold = 0.5) {
  if (detections.length === 0) {
    return [];
  }
  
  // Sort detections by score (highest first)
  const sortedDetections = [...detections].sort((a, b) => b.score - a.score);
  
  const keptDetections = [];
  const suppressedIndices = new Set();
  
  for (let i = 0; i < sortedDetections.length; i++) {
    if (suppressedIndices.has(i)) {
      continue; // Already suppressed
    }
    
    const currentDetection = sortedDetections[i];
    keptDetections.push(currentDetection);
    
    // Suppress overlapping detections (IoU > threshold = same face)
    for (let j = i + 1; j < sortedDetections.length; j++) {
      if (suppressedIndices.has(j)) {
        continue; // Already suppressed
      }
      
      const otherDetection = sortedDetections[j];
      const iou = calculateIoU(currentDetection.box, otherDetection.box);
      
      // If IoU > threshold, they overlap significantly (same face) - suppress the lower score one
      if (iou > iouThreshold) {
        suppressedIndices.add(j);
        // NOTE: This used to log every single suppressed detection, which created
        // massive repeated logs in production. The detailed per-detection log
        // is intentionally removed to keep ONNX logs compact.
        // If you need to debug NMS in the future, you can temporarily re-enable
        // a summary log here.
      }
    }
  }
  
  return keptDetections;
}

/**
 * Check if multiple detections are overlapping (likely same face detected multiple times)
 * vs clearly separate faces
 * ENTERPRISE: Distinguishes between duplicate detections and actual multiple people
 */
function checkIfOverlapping(detections) {
  if (detections.length < 2) return false;
  
  // Check if any two detections have significant overlap
  for (let i = 0; i < detections.length; i++) {
    for (let j = i + 1; j < detections.length; j++) {
      const iou = calculateIoU(detections[i].box, detections[j].box);
      if (iou > 0.3) {
        // Significant overlap (>30%) - likely same face detected multiple times
        console.log(`   üîç Detections ${i + 1} and ${j + 1} overlap (IoU: ${(iou * 100).toFixed(1)}%) - likely same face`);
        return true;
      }
    }
  }
  
  // No significant overlap - likely different faces
  console.log(`   üîç Detections are not overlapping - likely different faces`);
  return false;
}

/**
 * üè¶ BANK-GRADE: Detect faces using SCRFD with canonical preprocessing
 * Uses canonical image for detection and returns normalized coordinates (0-1)
 * 
 * @param {Buffer} canonicalBuffer - Canonical preprocessed image buffer
 * @param {number} canonicalWidth - Canonical image width
 * @param {number} canonicalHeight - Canonical image height
 * @returns {Array} - Array of detections with normalized box coordinates (0-1)
 */
async function detectFaces(canonicalBuffer, canonicalWidth, canonicalHeight) {
  const detectionStartTime = Date.now();
  await loadModels(); // Models are cached, fast if already loaded

  // Validate detection model is loaded
  if (!detectionModel) {
    throw new Error('Detection model not loaded');
  }
  
  if (!detectionModel.inputNames || detectionModel.inputNames.length === 0) {
    throw new Error('Detection model has no input names');
  }
  
  // Ensure model session is valid
  if (typeof detectionModel.run !== 'function') {
    throw new Error('Detection model session is invalid - run method not available');
  }

  // üè¶ BANK-GRADE: Preprocess canonical image for detection (resizes to square)
  const preprocessStartTime = Date.now();
  const preprocessed = await preprocessForDetection(canonicalBuffer, canonicalWidth, canonicalHeight);
  const preprocessTime = Date.now() - preprocessStartTime;
  console.log(`   ‚úÖ Detection preprocessing complete: ${canonicalWidth}x${canonicalHeight} ‚Üí ${preprocessed.width}x${preprocessed.height} (square) in ${preprocessTime}ms`);
  
  // Validate preprocessing result
  if (!preprocessed || !preprocessed.tensor) {
    throw new Error('Preprocessing failed - no tensor generated');
  }
  
  // Validate tensor is a valid ONNX Tensor
  if (!(preprocessed.tensor instanceof ort.Tensor)) {
    throw new Error('Preprocessing failed - invalid tensor type');
  }
  
  // üè¶ BANK-GRADE: No scaling needed - we're using canonical coordinates directly
  // Normalized coordinates will be computed relative to canonical image size
  
  // Run detection - use actual input name from model (should be 'blob' for SCRFD)
  // Validate model has input names
  if (!detectionModel.inputNames || !Array.isArray(detectionModel.inputNames) || detectionModel.inputNames.length === 0) {
    throw new Error(`Detection model has invalid input names: ${JSON.stringify(detectionModel.inputNames)}`);
  }
  
  const inputName = detectionModel.inputNames[0];
  
  // Validate input name exists and is a string
  if (!inputName || typeof inputName !== 'string' || inputName.trim().length === 0) {
    throw new Error(`Invalid input name: "${inputName}"`);
  }
  
  // Validate tensor has data
  if (!preprocessed.tensor.data || preprocessed.tensor.data.length === 0) {
    throw new Error('Preprocessing failed - tensor has no data');
  }

  // Use mutex to ensure thread-safe inference (ONNX Runtime sessions are not thread-safe)
  // CRITICAL: Capture tensor in local variable, but get input name DIRECTLY from model inside promise
  const capturedTensor = preprocessed.tensor;
  
  // Validate tensor BEFORE queuing
  if (!capturedTensor || !(capturedTensor instanceof ort.Tensor)) {
    throw new Error(`Invalid tensor: ${capturedTensor?.constructor?.name || typeof capturedTensor}`);
  }
  
  console.log(`   üìù About to queue inference - Input name: "${inputName}", Tensor shape: [${capturedTensor.dims.join(', ')}]`);
  
  let results;
  try {
    // Queue this inference to run after previous ones complete
    // CRITICAL: Get input name DIRECTLY from model inside promise to avoid any closure issues
    detectionInferenceQueue = detectionInferenceQueue.then(async () => {
      try {
        // CRITICAL: Get input name directly from model INSIDE the promise (don't rely on closure)
        if (!detectionModel || !detectionModel.inputNames || detectionModel.inputNames.length === 0) {
          throw new Error('Detection model or input names not available in promise');
        }
        
        const modelInputName = detectionModel.inputNames[0];
        console.log(`   üîß Inside promise - Model input name: "${modelInputName}"`);
        console.log(`   üîß Inside promise - Captured tensor exists: ${!!capturedTensor}`);
        console.log(`   üîß Inside promise - Captured tensor is Tensor: ${capturedTensor instanceof ort.Tensor}`);
        
        if (!modelInputName || typeof modelInputName !== 'string' || modelInputName.trim().length === 0) {
          throw new Error(`Invalid model input name: "${modelInputName}" (type: ${typeof modelInputName})`);
        }
        
        if (!capturedTensor || !(capturedTensor instanceof ort.Tensor)) {
          throw new Error(`Invalid tensor in promise: ${capturedTensor?.constructor?.name || typeof capturedTensor}`);
        }
        
        // Create feeds object fresh inside the promise using model's input name
        // CRITICAL: Use Object.create(null) to avoid prototype issues, then assign
        const feeds = Object.create(null);
        feeds[modelInputName] = capturedTensor;
        
        // Validate feeds object (minimal logging for performance)
        if (!feeds || typeof feeds !== 'object') {
          throw new Error('Failed to create feeds object');
        }
        
        // Use 'in' operator instead of hasOwnProperty (works with Object.create(null))
        if (!(modelInputName in feeds)) {
          throw new Error(`Feeds object missing input key "${modelInputName}"`);
        }
        
        if (!feeds[modelInputName] || !(feeds[modelInputName] instanceof ort.Tensor)) {
          throw new Error(`Invalid tensor in feeds for input "${modelInputName}"`);
        }
        
        // Run inference (reduced logging for performance)
        const inferenceStartTime = Date.now();
        const inferenceResult = await detectionModel.run(feeds);
        const inferenceTime = Date.now() - inferenceStartTime;
        console.log(`   ‚úÖ Detection complete (${inferenceTime}ms). Outputs: ${Object.keys(inferenceResult).join(', ')}`);
        return inferenceResult;
      } catch (inferenceError) {
        // IMMEDIATE error logging to stderr (always flushed)
        process.stderr.write(`\n‚ùå ========== DETECTION INFERENCE FAILED ==========\n`);
        process.stderr.write(`‚ùå Error message: ${inferenceError.message}\n`);
        if (detectionModel && detectionModel.inputNames) {
          process.stderr.write(`üìã Model expects inputs: ${detectionModel.inputNames.join(', ')}\n`);
        } else {
          process.stderr.write(`üìã Model or input names not available\n`);
        }
        process.stderr.write(`üì¶ Captured tensor exists: ${!!capturedTensor}\n`);
        process.stderr.write(`üì¶ Captured tensor is Tensor: ${capturedTensor instanceof ort.Tensor}\n`);
        if (capturedTensor) {
          process.stderr.write(`üì¶ Captured tensor shape: [${capturedTensor.dims?.join(', ') || 'N/A'}]\n`);
        }
        if (inferenceError.stack) {
          process.stderr.write(`‚ùå Error stack: ${inferenceError.stack}\n`);
        }
        process.stderr.write(`‚ùå ===============================================\n`);
        
        // Also log to console.error for normal logging
        console.error(`   ‚ùå ========== DETECTION INFERENCE FAILED ==========`);
        console.error(`   ‚ùå Error message: ${inferenceError.message}`);
        if (detectionModel && detectionModel.inputNames) {
          console.error(`   üìã Model expects inputs: ${detectionModel.inputNames.join(', ')}`);
        } else {
          console.error(`   üìã Model or input names not available`);
        }
        console.error(`   üì¶ Captured tensor exists: ${!!capturedTensor}`);
        console.error(`   üì¶ Captured tensor is Tensor: ${capturedTensor instanceof ort.Tensor}`);
        if (capturedTensor) {
          console.error(`   üì¶ Captured tensor shape: [${capturedTensor.dims?.join(', ') || 'N/A'}]`);
        }
        console.error(`   ‚ùå Error stack: ${inferenceError.stack}`);
        console.error(`   ‚ùå ===============================================`);
        throw inferenceError;
      }
    });
    
    results = await detectionInferenceQueue;
  } catch (error) {
    console.error(`   ‚ùå Detection error: ${error.message}`);
    console.error(`   ‚ùå Error stack: ${error.stack}`);
    throw error;
  }

  // Parse SCRFD output (format: [batch, num_detections, 15])
  // Each detection: [x1, y1, x2, y2, score, landmarks...]
  const detections = [];
  
  // Get results - SCRFD outputs: box_8, score_8, lmk5pt_8, box_16, score_16, lmk5pt_16, box_32, score_32, lmk5pt_32
  // Use the highest resolution output (box_32, score_32, lmk5pt_32) for best accuracy
  // Handle both named outputs (box_32, score_32, etc.) and numeric outputs (448, 451, etc.)
  let boxOutput = results.box_32 || results.box_16 || results.box_8;
  let scoreOutput = results.score_32 || results.score_16 || results.score_8;
  let landmarkOutput = results.lmk5pt_32 || results.lmk5pt_16 || results.lmk5pt_8;
  
  // If outputs are numeric (e.g., "448", "451", etc.), map them based on SCRFD output order and shapes
  // SCRFD outputs are ordered by scale: [scale8_box, scale8_score, scale8_landmarks, scale16_box, scale16_score, scale16_landmarks, scale32_box, scale32_score, scale32_landmarks]
  // Box outputs have shape [batch, num_detections, 4] (4 values per detection)
  // Score outputs have shape [batch, num_detections] (1 value per detection)
  // Landmark outputs have shape [batch, num_detections, 10] (5 keypoints √ó 2 coordinates)
  if (!boxOutput || !scoreOutput) {
    const outputKeys = Object.keys(results);
    const outputNames = detectionModel ? detectionModel.outputNames : [];
    
    // Helper function to identify output type by shape
    const identifyOutputType = (tensor) => {
      if (!tensor || !tensor.dims) return null;
      const dims = tensor.dims;
      // Box: [batch, num_detections, 4] or [num_detections, 4]
      if (dims.length >= 2 && dims[dims.length - 1] === 4) return 'box';
      // Landmarks: [batch, num_detections, 10] or [num_detections, 10] (5 keypoints √ó 2 coordinates)
      if (dims.length >= 2 && dims[dims.length - 1] === 10) return 'landmarks';
      // Score: [batch, num_detections] or [num_detections] - single value per detection
      // Score outputs typically have shape like [1, num_detections] or [num_detections]
      // and the last dimension is NOT 4 (box) or 10 (landmarks)
      if (dims.length >= 1) {
        const lastDim = dims[dims.length - 1];
        // Score should be a reasonable number of detections (not too large)
        // and not match box (4) or landmarks (10) dimensions
        if (lastDim > 0 && lastDim < 100000 && lastDim !== 4 && lastDim !== 10) {
          return 'score';
        }
      }
      return null;
    };
    
    // If we have numeric outputs, try to identify them by shape first, then fall back to order
    if (outputNames.length >= 9) {
      // First, try to identify outputs by shape (most reliable)
      const identifiedOutputs = { box: null, score: null, landmarks: null };
      const scale32Indices = [6, 7, 8]; // Scale 32 (highest resolution)
      const scale16Indices = [3, 4, 5]; // Scale 16
      const scale8Indices = [0, 1, 2];  // Scale 8
      
      // Try scale 32 first (highest resolution)
      for (const idx of scale32Indices) {
        if (idx < outputNames.length && results[outputNames[idx]]) {
          const type = identifyOutputType(results[outputNames[idx]]);
          if (type && !identifiedOutputs[type]) {
            identifiedOutputs[type] = outputNames[idx];
          }
        }
      }
      
      // If we found box and score, use them
      if (identifiedOutputs.box && identifiedOutputs.score) {
        boxOutput = results[identifiedOutputs.box];
        scoreOutput = results[identifiedOutputs.score];
        landmarkOutput = identifiedOutputs.landmarks ? results[identifiedOutputs.landmarks] : null;
        console.log(`   üîß Mapped numeric outputs by shape (scale 32): box=${identifiedOutputs.box}, score=${identifiedOutputs.score}, landmarks=${identifiedOutputs.landmarks || 'none'}`);
      } else {
        // Fall back to order-based mapping for scale 32
        const scale32BoxKey = outputNames[6];
        const scale32ScoreKey = outputNames[7];
        const scale32LandmarkKey = outputNames[8];
        
        if (scale32BoxKey && scale32ScoreKey && results[scale32BoxKey] && results[scale32ScoreKey]) {
          boxOutput = results[scale32BoxKey];
          scoreOutput = results[scale32ScoreKey];
          landmarkOutput = results[scale32LandmarkKey] || null;
          console.log(`   üîß Mapped numeric outputs by order (scale 32): box=${scale32BoxKey}, score=${scale32ScoreKey}, landmarks=${scale32LandmarkKey || 'none'}`);
        } else {
          // Try scale 16
          const scale16BoxKey = outputNames[3];
          const scale16ScoreKey = outputNames[4];
          const scale16LandmarkKey = outputNames[5];
          
          if (scale16BoxKey && scale16ScoreKey && results[scale16BoxKey] && results[scale16ScoreKey]) {
            boxOutput = results[scale16BoxKey];
            scoreOutput = results[scale16ScoreKey];
            landmarkOutput = results[scale16LandmarkKey] || null;
            console.log(`   üîß Mapped numeric outputs (scale 16): box=${scale16BoxKey}, score=${scale16ScoreKey}, landmarks=${scale16LandmarkKey || 'none'}`);
          } else {
            // Try scale 8
            const scale8BoxKey = outputNames[0];
            const scale8ScoreKey = outputNames[1];
            const scale8LandmarkKey = outputNames[2];
            
            if (scale8BoxKey && scale8ScoreKey && results[scale8BoxKey] && results[scale8ScoreKey]) {
              boxOutput = results[scale8BoxKey];
              scoreOutput = results[scale8ScoreKey];
              landmarkOutput = results[scale8LandmarkKey] || null;
              console.log(`   üîß Mapped numeric outputs (scale 8): box=${scale8BoxKey}, score=${scale8ScoreKey}, landmarks=${scale8LandmarkKey || 'none'}`);
            }
          }
        }
      }
    }
  }
  
  if (!boxOutput || !scoreOutput) {
    console.error('   ‚ùå Invalid SCRFD output format');
    console.error(`   üìã Available outputs: ${Object.keys(results).join(', ')}`);
    if (detectionModel && detectionModel.outputNames) {
      console.error(`   üìã Model output names: ${detectionModel.outputNames.join(', ')}`);
      // Log output shapes for debugging
      console.error(`   üìä Output shapes:`);
      for (let i = 0; i < detectionModel.outputNames.length; i++) {
        const key = detectionModel.outputNames[i];
        if (results[key] && results[key].dims) {
          console.error(`      ${key}: [${results[key].dims.join(', ')}]`);
        }
      }
    }
    throw new Error(`Invalid SCRFD output format. Available: ${Object.keys(results).join(', ')}`);
  }
  
  const scores = scoreOutput.data;
  const boxes = boxOutput.data;
  const landmarks = landmarkOutput ? landmarkOutput.data : null;

  // Analyze raw score distribution to determine if logits need normalization
  let minRawScore = Infinity;
  let maxRawScore = -Infinity;
  for (let idx = 0; idx < scores.length; idx++) {
    const val = scores[idx];
    if (val < minRawScore) minRawScore = val;
    if (val > maxRawScore) maxRawScore = val;
  }
  const scoresLookProbabilistic = minRawScore >= 0 && maxRawScore <= 1;
  const logitsDetected = !scoresLookProbabilistic;
  const shouldNormalizeScores = CONFIG.FORCE_SIGMOID_NORMALIZATION || logitsDetected;

  if (shouldNormalizeScores) {
    if (logitsDetected) {
      console.log(`   üîß Normalizing SCRFD logits ‚Üí probabilities (min=${minRawScore.toFixed(4)}, max=${maxRawScore.toFixed(4)})`);
    } else {
      console.log('   üîß Forcing SCRFD sigmoid normalization (scores already appear probabilistic)');
    }
  } else {
    console.log('   ‚ÑπÔ∏è SCRFD scores already appear to be probabilities (0-1 range) - skipping extra sigmoid to avoid double-normalization');
  }

  // Log tensor shapes for debugging
  console.log(`   üìä Box output shape: [${boxOutput.dims.join(', ')}]`);
  console.log(`   üìä Score output shape: [${scoreOutput.dims.join(', ')}]`);
  if (landmarkOutput) {
    console.log(`   üìä Landmark output shape: [${landmarkOutput.dims.join(', ')}]`);
  }

  // Parse SCRFD output - format depends on model version
  // SCRFD typically outputs: boxes [batch, num_anchors, 4], scores [batch, num_anchors]
  // Box format is usually [x1, y1, x2, y2] in pixel coordinates OR [center_x, center_y, width, height]
  // We need to determine the format based on the values
  
  // Get dimensions from tensor shape
  const boxDims = boxOutput.dims;
  const scoreDims = scoreOutput.dims;
  
  // SCRFD outputs are 2D arrays: [num_anchors, features]
  // Boxes: [num_anchors, 4 or 10] - 4 for box coords, 10 if including landmarks
  // Scores: [num_anchors, 1 or num_classes] - usually [num_anchors] or [num_anchors, 1]
  
  // Handle 2D arrays correctly
  let numAnchors, boxFeatures, scoreFeatures;
  
  if (boxDims.length === 2) {
    // Shape is [num_anchors, features]
    numAnchors = boxDims[0];
    boxFeatures = boxDims[1];
  } else if (boxDims.length === 1) {
    // Flattened array - assume 4 values per box
    numAnchors = Math.floor(boxDims[0] / 4);
    boxFeatures = 4;
  } else {
    // Unknown format, try to infer
    numAnchors = boxDims[boxDims.length - 2] || Math.floor(boxes.length / 4);
    boxFeatures = boxDims[boxDims.length - 1] || 4;
  }
  
  if (scoreDims.length >= 2) {
    // Last dimension represents feature count (e.g., 1 score, 2 classes, etc.)
    scoreFeatures = scoreDims[scoreDims.length - 1];
  } else {
    scoreFeatures = 1;
  }
  
  // Determine how many detections are present in score tensor (handles [batch, anchors, features])
  const scoreAnchors = scoreDims.length >= 2
    ? scoreDims[scoreDims.length - 2]
    : (scoreDims[0] || scores.length);
  
  // Use the minimum number of anchors from boxes and scores
  const maxDetections = Math.min(numAnchors, scoreAnchors || scores.length);
  
  console.log(`   üìä Parsing up to ${maxDetections} potential detections (${numAnchors} anchors, ${boxFeatures} box features, ${scoreFeatures} score features)`);
  
  // Extract scores correctly from 2D array
  // SCRFD outputs scores as [num_anchors, num_scales] or [num_anchors, 1]
  // For SCRFD models, the face score is typically at index 0 of the features array
  // Scores may be raw logits (need sigmoid) or already probabilities (0-1 range)
  
  // Helper function to apply sigmoid when logits detected/forced
  const sigmoid = (x) => 1 / (1 + Math.exp(-x));

  const clampScore = (value) => {
    if (!isFinite(value)) {
      return 0;
    }
    return Math.max(0, Math.min(1, value));
  };

  /**
   * SCRFD classification heads emit either:
   *  - single channel logits/probabilities (already sigmoid'd)
   *  - two channel logits: [background, face]
   *  - multi-channel logits (per stride) that still contain one face score
   * Convert every scenario into a calibrated probability so our MIN_FACE_QUALITY
   * gate always compares apples-to-apples (0-1 probabilities).
   */
  const computeNormalizedScore = (featureScores) => {
    if (!featureScores || featureScores.length === 0) {
      return 0;
    }

    const finiteScores = featureScores.filter((val) => Number.isFinite(val));
    const allProbabilities = finiteScores.length === featureScores.length &&
      finiteScores.every((val) => val >= 0 && val <= 1);

    if (featureScores.length === 1) {
      const raw = finiteScores.length ? finiteScores[0] : featureScores[0] || 0;
      const normalized = shouldNormalizeScores ? sigmoid(raw) : raw;
      return clampScore(normalized);
    }

    if (featureScores.length === 2) {
      const background = Number.isFinite(featureScores[0]) ? featureScores[0] : 0;
      const face = Number.isFinite(featureScores[1]) ? featureScores[1] : 0;

      if (shouldNormalizeScores) {
        // For two-class logits, probability = sigmoid(face - background)
        return clampScore(sigmoid(face - background));
      }

      if (allProbabilities) {
        const denom = background + face;
        if (denom > 0) {
          return clampScore(face / denom);
        }
      }

      // Fallback: treat face entry as already normalized (scrfd_mbf outputs [bg, face_prob])
      return clampScore(face);
    }

    // Multi-feature fallback: prefer positive logits (foreground anchors)
    const positiveOnly = finiteScores.filter((val) => val > 0);
    let candidate = positiveOnly.length > 0
      ? Math.max(...positiveOnly)
      : (finiteScores.length > 0 ? Math.max(...finiteScores) : 0);

    if (!isFinite(candidate)) {
      candidate = 0;
    }

    const normalized = shouldNormalizeScores ? sigmoid(candidate) : candidate;
    return clampScore(normalized);
  };
  
  const extractedScores = [];
  if (scoreFeatures > 1) {
    // Multi-feature array: [num_anchors, features]
    for (let i = 0; i < maxDetections; i++) {
      const baseIdx = i * scoreFeatures;
      const featureScores = [];
      for (let j = 0; j < scoreFeatures; j++) {
        featureScores.push(scores[baseIdx + j]);
      }
      extractedScores.push(computeNormalizedScore(featureScores));
    }
  } else {
    // 1D array or already flattened
    for (let i = 0; i < maxDetections; i++) {
      extractedScores.push(computeNormalizedScore([scores[i]]));
    }
  }
  
  // Debug: Log some raw score values to understand the format
  if (scoreFeatures > 1 && extractedScores.length > 0) {
    const sampleIdx = 0;
    const sampleBase = sampleIdx * scoreFeatures;
    const sampleFeatures = [];
    for (let j = 0; j < Math.min(scoreFeatures, 10); j++) {
      sampleFeatures.push(scores[sampleBase + j].toFixed(4));
    }
    
    // Find which index had the max positive value
    const positiveScores = [];
    let maxIdx = 0;
    let maxVal = scores[sampleBase];
    for (let j = 0; j < scoreFeatures; j++) {
      const val = scores[sampleBase + j];
      if (val > 0) positiveScores.push({idx: j, val: val});
      if (val > maxVal) {
        maxVal = val;
        maxIdx = j;
      }
    }
    const maxPositive = positiveScores.length > 0 ? Math.max(...positiveScores.map(p => p.val)) : maxVal;
    const processedScore0 = extractedScores[0];
    
    console.log(`   üîç Sample score features for anchor 0: [${sampleFeatures.join(', ')}]`);
    console.log(`   üîç Using max positive (idx ${maxIdx}): raw=${maxVal.toFixed(4)}, ${shouldNormalizeScores ? 'sigmoid=' : 'direct='}${processedScore0.toFixed(4)} (${(processedScore0 * 100).toFixed(1)}%)`);
  }
  
  // üêõ DEBUG: Log top scores to diagnose detection issues
  const topScores = Array.from(extractedScores).sort((a, b) => b - a).slice(0, 10);
  console.log(`   üîç Top 10 detection scores: ${topScores.map(s => (s * 100).toFixed(1)).join('%, ')}%`);
  console.log(`   üîç Detection threshold: ${(CONFIG.MIN_DETECTION_SCORE * 100).toFixed(1)}%`);
  
  // Determine box format by checking first box
  // SCRFD typically outputs boxes in center-size format: [center_x, center_y, width, height]
  // Values can be normalized (0-1), offset-normalized (-0.5 to 0.5), or pixel coordinates
  let boxFormat = 'unknown';
  if (boxes.length >= boxFeatures) {
    // Get first box values (first boxFeatures values)
    const firstBoxValues = [];
    for (let i = 0; i < Math.min(4, boxFeatures); i++) {
      firstBoxValues.push(boxes[i]);
    }
    const maxVal = Math.max(...firstBoxValues.map(Math.abs));
    const minVal = Math.min(...firstBoxValues);
    
    // SCRFD typically outputs boxes in center-size format: [center_x, center_y, width, height]
    // For normalized values in -1 to 1 range, default to center-size format
    // (SCRFD models from InsightFace typically use center-size, not corner format)
    if (maxVal < 1.0 && minVal >= -1.0) {
      // Default to center-size format for SCRFD (most common)
      boxFormat = 'normalized_center_size';
    } else if (maxVal < 640 && minVal >= 0) {
      // Values are 0-640, likely pixel coordinates
      // Default to center-size format for SCRFD
      boxFormat = 'pixel_center_size';
    } else {
      // Values might be center+size format or other
      boxFormat = 'center_size';
    }
    console.log(`   üîç Detected box format: ${boxFormat} (first box values: [${firstBoxValues.map(v => v.toFixed(3)).join(', ')}])`);
  }
  
  // üè¶ BANK-GRADE: Parse detections and convert to normalized coordinates (0-1)
  // First, collect all detections that pass the score threshold
  const validDetections = [];
  for (let i = 0; i < maxDetections; i++) {
    const score = extractedScores[i];
    
    // Filter by score FIRST to avoid processing invalid detections
    if (score < CONFIG.MIN_DETECTION_SCORE) {
      continue;
    }
    
    validDetections.push({
      index: i,
      score: score
    });
  }
  
  console.log(`   üìä Found ${validDetections.length} detections above threshold (${(CONFIG.MIN_DETECTION_SCORE * 100).toFixed(1)}%)`);
  
  // Now parse only the valid detections
  for (const detection of validDetections) {
    const i = detection.index;
    const score = detection.score;
    
    // Calculate indices for 2D array access
    // Boxes are stored as [num_anchors, features] - access as boxes[i * boxFeatures + j]
    const boxBaseIdx = i * boxFeatures;
    const landmarkBaseIdx = landmarks ? i * (landmarks.length / maxDetections) : null;
    
    // Extract box coordinates (first 4 values)
    // SCRFD typically outputs [x1, y1, x2, y2] or [center_x, center_y, width, height]
    // DEBUG: Log all 10 features for first few detections to understand format
    if (i < 3 && validDetections.length <= 3) {
      const allFeatures = [];
      for (let f = 0; f < Math.min(10, boxFeatures); f++) {
        allFeatures.push(boxes[boxBaseIdx + f].toFixed(4));
      }
      console.log(`   üîç Box features for detection ${i}: [${allFeatures.join(', ')}]`);
    }
    
    const boxVal0 = boxes[boxBaseIdx];
    const boxVal1 = boxes[boxBaseIdx + 1];
    const boxVal2 = boxes[boxBaseIdx + 2];
    const boxVal3 = boxes[boxBaseIdx + 3];
    
    // Parse box based on detected format
    let x1_detection, y1_detection, x2_detection, y2_detection;
    
    if (boxFormat === 'normalized_center_size') {
      // SCRFD outputs boxes - trying multiple format interpretations
      // Based on logs showing tiny heights, the format might be different than expected
      
      // Check if values are offset-normalized (have negative values)
      if (boxVal0 < 0 || boxVal1 < 0 || boxVal2 < 0 || boxVal3 < 0) {
        // Try format [x1, y1, x2, y2] first (standard corner format)
        const x1_candidate1 = (boxVal0 + 0.5) * 640;
        const y1_candidate1 = (boxVal1 + 0.5) * 640;
        const x2_candidate1 = (boxVal2 + 0.5) * 640;
        const y2_candidate1 = (boxVal3 + 0.5) * 640;
        
        // Try format [x1, y2, x2, y1] (swapped y-coords)
        const x1_candidate2 = (boxVal0 + 0.5) * 640;
        const y2_candidate2 = (boxVal1 + 0.5) * 640;
        const x2_candidate2 = (boxVal2 + 0.5) * 640;
        const y1_candidate2 = (boxVal3 + 0.5) * 640;
        
        // Choose the format that produces valid boxes (y1 < y2, reasonable height)
        const height1 = Math.abs(y2_candidate1 - y1_candidate1);
        const height2 = Math.abs(y2_candidate2 - y1_candidate2);
        
        // Use format that gives larger, more reasonable height
        if (height2 > height1 && height2 > 50) {
          // Use swapped format
          x1_detection = x1_candidate2;
          y1_detection = Math.min(y1_candidate2, y2_candidate2);
          x2_detection = x2_candidate2;
          y2_detection = Math.max(y1_candidate2, y2_candidate2);
        } else {
          // Use standard format
          x1_detection = Math.min(x1_candidate1, x2_candidate1);
          y1_detection = Math.min(y1_candidate1, y2_candidate1);
          x2_detection = Math.max(x1_candidate1, x2_candidate1);
          y2_detection = Math.max(y1_candidate1, y2_candidate1);
        }
      } else {
        // Standard normalized (0-1) - try standard corner format
        x1_detection = Math.min(boxVal0, boxVal2) * 640;
        y1_detection = Math.min(boxVal1, boxVal3) * 640;
        x2_detection = Math.max(boxVal0, boxVal2) * 640;
        y2_detection = Math.max(boxVal1, boxVal3) * 640;
      }
    } else if (boxFormat === 'normalized_corners') {
      // Boxes are in normalized corner format [x1, y1, x2, y2] (0-1 or -1 to 1 range)
      // Handle negative values (offset-normalized format)
      // Convert to pixel coordinates in 640x640 detection space
      // If values are negative, they might be offset-normalized (center at 0, range -0.5 to 0.5)
      if (boxVal0 < 0 || boxVal1 < 0) {
        // Offset-normalized: add 0.5 to center, then scale
        x1_detection = (boxVal0 + 0.5) * 640;
        y1_detection = (boxVal1 + 0.5) * 640;
        x2_detection = (boxVal2 + 0.5) * 640;
        y2_detection = (boxVal3 + 0.5) * 640;
      } else {
        // Standard normalized (0-1)
        x1_detection = boxVal0 * 640;
        y1_detection = boxVal1 * 640;
        x2_detection = boxVal2 * 640;
        y2_detection = boxVal3 * 640;
      }
    } else if (boxFormat === 'pixel_center_size') {
      // Boxes are in pixel center-size format [center_x, center_y, width, height]
      const center_x = boxVal0;
      const center_y = boxVal1;
      const width_detection = Math.abs(boxVal2);
      const height_detection = Math.abs(boxVal3);
      
      // Convert center format to corner format: [x1, y1, x2, y2]
      x1_detection = center_x - width_detection / 2;
      y1_detection = center_y - height_detection / 2;
      x2_detection = center_x + width_detection / 2;
      y2_detection = center_y + height_detection / 2;
    } else if (boxFormat === 'pixel_corners') {
      // Boxes are in pixel corner format [x1, y1, x2, y2]
      x1_detection = boxVal0;
      y1_detection = boxVal1;
      x2_detection = boxVal2;
      y2_detection = boxVal3;
    } else {
      // Default: assume center+size format [center_x, center_y, width, height]
      const center_x = boxVal0;
      const center_y = boxVal1;
      const width_detection = Math.abs(boxVal2);
      const height_detection = Math.abs(boxVal3);
      
      // Convert center format to corner format: [x1, y1, x2, y2]
      x1_detection = center_x - width_detection / 2;
      y1_detection = center_y - height_detection / 2;
      x2_detection = center_x + width_detection / 2;
      y2_detection = center_y + height_detection / 2;
    }
    
    // Ensure coordinates are within bounds and ordered correctly
    x1_detection = Math.max(0, Math.min(640, x1_detection));
    y1_detection = Math.max(0, Math.min(640, y1_detection));
    x2_detection = Math.max(0, Math.min(640, x2_detection));
    y2_detection = Math.max(0, Math.min(640, y2_detection));
    
    // Final check: ensure x1 < x2 and y1 < y2
    if (x2_detection < x1_detection) {
      [x1_detection, x2_detection] = [x2_detection, x1_detection];
    }
    if (y2_detection < y1_detection) {
      [y1_detection, y2_detection] = [y2_detection, y1_detection];
    }
    
    // Convert detection coordinates (640x640) back to canonical coordinates
    // Detection image was created using 'cover' which scales and crops
    // IMPORTANT: SCRFD outputs coordinates relative to the detection image (640x640)
    // With 'cover', the image is scaled by 'scale' and then cropped by 'cropOffset'
    // The detection image shows the center portion of the scaled image
    // Formula: canonical_coord = (detection_coord + crop_offset) / scale
    // Example: If detection y=100 and cropOffsetY=106.4, scaled y=206.4, canonical y=206.4/0.8=258
    // üêõ FIX: Ensure coordinates are ordered (x1 < x2, y1 < y2) before conversion
    // After swapping above, coordinates are guaranteed to be ordered correctly
    const x1_canonical = (x1_detection + preprocessed.cropOffsetX) / preprocessed.scale;
    const y1_canonical = (y1_detection + preprocessed.cropOffsetY) / preprocessed.scale;
    const x2_canonical = (x2_detection + preprocessed.cropOffsetX) / preprocessed.scale;
    const y2_canonical = (y2_detection + preprocessed.cropOffsetY) / preprocessed.scale;
    
    // Clamp to canonical image bounds
    const x1_canonical_clamped = Math.max(0, Math.min(canonicalWidth, x1_canonical));
    const y1_canonical_clamped = Math.max(0, Math.min(canonicalHeight, y1_canonical));
    const x2_canonical_clamped = Math.max(0, Math.min(canonicalWidth, x2_canonical));
    const y2_canonical_clamped = Math.max(0, Math.min(canonicalHeight, y2_canonical));
    
    // Normalize to [0, 1] relative to canonical image size
    const x1 = x1_canonical_clamped / canonicalWidth;
    const y1 = y1_canonical_clamped / canonicalHeight;
    const x2 = x2_canonical_clamped / canonicalWidth;
    const y2 = y2_canonical_clamped / canonicalHeight;
    
    const width_normalized = x2 - x1;
    const height_normalized = y2 - y1;
    
    // Extract landmarks if available - convert from detection space to canonical, then normalize
    let faceLandmarks = null;
    if (landmarks && landmarkBaseIdx !== null) {
      // Landmarks might be in the box array (if boxFeatures >= 10) or separate array
      // SCRFD typically has landmarks starting at index 4 in the box array
      let landmarkStartIdx;
      if (boxFeatures >= 10) {
        // Landmarks are in the box array starting at index 4
        landmarkStartIdx = boxBaseIdx + 4;
      } else if (landmarkOutput) {
        // Landmarks are in separate array
        const landmarkDims = landmarkOutput.dims;
        const landmarkFeatures = landmarkDims.length >= 2 ? landmarkDims[1] : 10;
        landmarkStartIdx = i * landmarkFeatures;
      } else {
        landmarkStartIdx = null;
      }
      
      if (landmarkStartIdx !== null && landmarkStartIdx + 9 < Math.max(boxes.length, landmarks.length)) {
        // Extract landmarks (5 keypoints √ó 2 coordinates = 10 values)
        // Use boxes array if boxFeatures >= 10, otherwise use landmarks array
        const landmarkArray = boxFeatures >= 10 ? boxes : landmarks;
        let leftEyeX_detection = landmarkArray[landmarkStartIdx];
        let leftEyeY_detection = landmarkArray[landmarkStartIdx + 1];
        let rightEyeX_detection = landmarkArray[landmarkStartIdx + 2];
        let rightEyeY_detection = landmarkArray[landmarkStartIdx + 3];
        let noseX_detection = landmarkArray[landmarkStartIdx + 4];
        let noseY_detection = landmarkArray[landmarkStartIdx + 5];
        let leftMouthX_detection = landmarkArray[landmarkStartIdx + 6];
        let leftMouthY_detection = landmarkArray[landmarkStartIdx + 7];
        let rightMouthX_detection = landmarkArray[landmarkStartIdx + 8];
        let rightMouthY_detection = landmarkArray[landmarkStartIdx + 9];
        
        // If boxes are normalized, landmarks are likely also normalized
        if (boxFormat === 'normalized_corners') {
          // Handle offset-normalized format
          if (leftEyeX_detection < 0 || leftEyeY_detection < 0) {
            // Offset-normalized: add 0.5 to center, then scale
            leftEyeX_detection = (leftEyeX_detection + 0.5) * 640;
            leftEyeY_detection = (leftEyeY_detection + 0.5) * 640;
            rightEyeX_detection = (rightEyeX_detection + 0.5) * 640;
            rightEyeY_detection = (rightEyeY_detection + 0.5) * 640;
            noseX_detection = (noseX_detection + 0.5) * 640;
            noseY_detection = (noseY_detection + 0.5) * 640;
            leftMouthX_detection = (leftMouthX_detection + 0.5) * 640;
            leftMouthY_detection = (leftMouthY_detection + 0.5) * 640;
            rightMouthX_detection = (rightMouthX_detection + 0.5) * 640;
            rightMouthY_detection = (rightMouthY_detection + 0.5) * 640;
          } else {
            // Standard normalized (0-1)
            leftEyeX_detection *= 640;
            leftEyeY_detection *= 640;
            rightEyeX_detection *= 640;
            rightEyeY_detection *= 640;
            noseX_detection *= 640;
            noseY_detection *= 640;
            leftMouthX_detection *= 640;
            leftMouthY_detection *= 640;
            rightMouthX_detection *= 640;
            rightMouthY_detection *= 640;
          }
        
        // Convert to canonical coordinates (add crop offset, then divide by scale)
        const leftEyeX_canonical = Math.max(0, Math.min(canonicalWidth, (leftEyeX_detection + preprocessed.cropOffsetX) / preprocessed.scale));
        const leftEyeY_canonical = Math.max(0, Math.min(canonicalHeight, (leftEyeY_detection + preprocessed.cropOffsetY) / preprocessed.scale));
        const rightEyeX_canonical = Math.max(0, Math.min(canonicalWidth, (rightEyeX_detection + preprocessed.cropOffsetX) / preprocessed.scale));
        const rightEyeY_canonical = Math.max(0, Math.min(canonicalHeight, (rightEyeY_detection + preprocessed.cropOffsetY) / preprocessed.scale));
        const noseX_canonical = Math.max(0, Math.min(canonicalWidth, (noseX_detection + preprocessed.cropOffsetX) / preprocessed.scale));
        const noseY_canonical = Math.max(0, Math.min(canonicalHeight, (noseY_detection + preprocessed.cropOffsetY) / preprocessed.scale));
        const leftMouthX_canonical = Math.max(0, Math.min(canonicalWidth, (leftMouthX_detection + preprocessed.cropOffsetX) / preprocessed.scale));
        const leftMouthY_canonical = Math.max(0, Math.min(canonicalHeight, (leftMouthY_detection + preprocessed.cropOffsetY) / preprocessed.scale));
        const rightMouthX_canonical = Math.max(0, Math.min(canonicalWidth, (rightMouthX_detection + preprocessed.cropOffsetX) / preprocessed.scale));
        const rightMouthY_canonical = Math.max(0, Math.min(canonicalHeight, (rightMouthY_detection + preprocessed.cropOffsetY) / preprocessed.scale));
        
        // Normalize to [0, 1]
        const leftEyeX = leftEyeX_canonical / canonicalWidth;
        const leftEyeY = leftEyeY_canonical / canonicalHeight;
        const rightEyeX = rightEyeX_canonical / canonicalWidth;
        const rightEyeY = rightEyeY_canonical / canonicalHeight;
        const noseX = noseX_canonical / canonicalWidth;
        const noseY = noseY_canonical / canonicalHeight;
        const leftMouthX = leftMouthX_canonical / canonicalWidth;
        const leftMouthY = leftMouthY_canonical / canonicalHeight;
        const rightMouthX = rightMouthX_canonical / canonicalWidth;
        const rightMouthY = rightMouthY_canonical / canonicalHeight;
        
        faceLandmarks = {
          leftEye: { x: leftEyeX, y: leftEyeY },
          rightEye: { x: rightEyeX, y: rightEyeY },
          nose: { x: noseX, y: noseY },
          leftMouth: { x: leftMouthX, y: leftMouthY },
          rightMouth: { x: rightMouthX, y: rightMouthY },
        };
      }
    }
    }
    
    // Validate face size using normalized coordinates (convert back to pixels for validation)
    // Use clamped canonical coordinates for accurate pixel calculation
    const faceWidth_pixels = (x2_canonical_clamped - x1_canonical_clamped);
    const faceHeight_pixels = (y2_canonical_clamped - y1_canonical_clamped);
    
    // üêõ DEBUG: Log ALL detections with score > threshold (not just first 5 in loop order)
    if (score > CONFIG.MIN_DETECTION_SCORE) {
      const passesWidth = faceWidth_pixels >= CONFIG.MIN_FACE_SIZE;
      const passesHeight = faceHeight_pixels >= CONFIG.MIN_FACE_SIZE;
      const status = (passesWidth && passesHeight) ? '‚úì PASS' : '‚úó REJECT';
      console.log(`   üîç Detection ${i}: ${status} - score=${(score * 100).toFixed(1)}%, size=${faceWidth_pixels.toFixed(0)}x${faceHeight_pixels.toFixed(0)}px (min: ${CONFIG.MIN_FACE_SIZE}px)`);
      if (!passesWidth || !passesHeight) {
        console.log(`      ‚ö†Ô∏è Rejected: ${!passesWidth ? `width too small (${faceWidth_pixels.toFixed(0)} < ${CONFIG.MIN_FACE_SIZE})` : ''} ${!passesHeight ? `height too small (${faceHeight_pixels.toFixed(0)} < ${CONFIG.MIN_FACE_SIZE})` : ''}`);
        console.log(`      üìê Box detection: [${x1_detection.toFixed(1)}, ${y1_detection.toFixed(1)}, ${x2_detection.toFixed(1)}, ${y2_detection.toFixed(1)}] (640x640 space)`);
        console.log(`      üìê Box canonical: [${x1_canonical_clamped.toFixed(1)}, ${y1_canonical_clamped.toFixed(1)}, ${x2_canonical_clamped.toFixed(1)}, ${y2_canonical_clamped.toFixed(1)}] (${canonicalWidth}x${canonicalHeight} space)`);
        console.log(`      üîß Conversion: scale=${preprocessed.scale.toFixed(3)}, cropOffset=(${preprocessed.cropOffsetX.toFixed(1)}, ${preprocessed.cropOffsetY.toFixed(1)})`);
      }
    }
    
    if (score > CONFIG.MIN_DETECTION_SCORE && faceWidth_pixels >= CONFIG.MIN_FACE_SIZE && faceHeight_pixels >= CONFIG.MIN_FACE_SIZE) {
      detections.push({
        box: {
          x: x1,           // üè¶ BANK-GRADE: Normalized coordinates (0-1)
          y: y1,
          width: width_normalized,
          height: height_normalized,
        },
        boxPixels: {      // Pixel coordinates for backward compatibility / validation
          x: x1_canonical_clamped,
          y: y1_canonical_clamped,
          width: faceWidth_pixels,
          height: faceHeight_pixels,
        },
        score: score,
        landmarks: faceLandmarks,
      });
    }
  }
  
  console.log(`   ‚úÖ Found ${detections.length} faces with score > ${(CONFIG.MIN_DETECTION_SCORE * 100).toFixed(0)}% (threshold: ${(CONFIG.MIN_DETECTION_SCORE * 100).toFixed(0)}%)`);
  
  // ENTERPRISE: Apply Non-Maximum Suppression (NMS) to filter duplicate/overlapping detections
  // This prevents the same face from being counted multiple times
  // Lower IoU threshold (0.2) = more aggressive filtering (removes more overlapping detections)
  // 0.2 (20%) threshold ensures even slight overlaps are filtered (reduces duplicate detections from SCRFD multi-scale)
  let filteredDetections = applyNMS(detections, 0.2); // 0.2 IoU threshold for very aggressive NMS (reduces multiple detections)
  console.log(`   üîç After NMS filtering: ${filteredDetections.length} unique face(s)`);
  
  // ENTERPRISE QUALITY GATE 4: Face count validation - MUST be exactly 1 face
  if (filteredDetections.length === 0) {
    throw new Error('No face detected. Please ensure your face is visible, well-lit, and facing the camera directly.');
  }
  
  // üêõ FIX: Robust multi-detection handling
  // After NMS, if multiple detections remain, decide whether they're:
  //   - a tight cluster around ONE real face (multi-scale duplicates) ‚Üí accept best one
  //   - or clearly separated faces (different people) ‚Üí reject
  if (filteredDetections.length > CONFIG.MAX_FACES_ALLOWED) {
    console.warn(`‚ö†Ô∏è Multiple detections after NMS filtering: ${filteredDetections.length} faces`);

    // Always compare everything against the best-scoring detection
    const sortedByScore = [...filteredDetections].sort((a, b) => b.score - a.score);
    const best = sortedByScore[0];

    let hasClearlySeparateFace = false;

    for (let i = 1; i < sortedByScore.length; i++) {
      const det = sortedByScore[i];

      const iou = calculateIoU(best.box, det.box);
      const sizeDiff = Math.abs(best.box.width - det.box.width) / Math.max(best.box.width, det.box.width);
      const centerDist = Math.sqrt(
        Math.pow((best.box.x + best.box.width / 2) - (det.box.x + det.box.width / 2), 2) +
        Math.pow((best.box.y + best.box.height / 2) - (det.box.y + det.box.height / 2), 2)
      );
      const avgSize = (best.box.width + det.box.width) / 2;
      const centerDistRatio = centerDist / avgSize;

      console.log(`   üìä Detection vs best [${i}]: IoU=${(iou * 100).toFixed(1)}%, sizeDiff=${(sizeDiff * 100).toFixed(1)}%, centerDistRatio=${(centerDistRatio * 100).toFixed(1)}%`);

      // Heuristic:
      // - If box is reasonably close to best (clustered) ‚Üí treat as duplicate
      // - Only mark as "separate face" if it is both far away in position AND very different in size
      const isClusteredDuplicate =
        iou > 0.05 ||              // any meaningful overlap
        sizeDiff < 0.70 ||         // sizes within 70%
        centerDistRatio < 1.20;    // centers within 120% of average face size

      if (!isClusteredDuplicate) {
        hasClearlySeparateFace = true;
      }
    }

    if (hasClearlySeparateFace) {
      console.error(`   ‚ùå At least one detection is clearly a separate face - REJECTING`);
      throw new Error(`Multiple faces detected (${filteredDetections.length} faces). Please ensure only ONE person is in the frame. This is required for accurate identification.`);
    }

    // All remaining detections form a tight cluster around the same face ‚Üí keep best one only
    filteredDetections = [best];
    console.log(`   ‚úÖ All detections form a tight cluster around one face - accepting best detection (score: ${(best.score * 100).toFixed(1)}%)`);
  }
  
  // Only ONE face after NMS - proceed with it
  filteredDetections.sort((a, b) => b.score - a.score);
  const bestDetection = filteredDetections[0];
  console.log(`‚úÖ Single face confirmed after NMS filtering (score: ${(bestDetection.score * 100).toFixed(1)}%)`);
  
  // üè¶ BANK-GRADE: Face size validation using normalized coordinates
  // Convert normalized coordinates to pixels for validation
  const faceWidth_pixels = bestDetection.box.width * canonicalWidth;
  const faceHeight_pixels = bestDetection.box.height * canonicalHeight;
  const faceSize_pixels = Math.min(faceWidth_pixels, faceHeight_pixels);
  
  if (faceSize_pixels < CONFIG.MIN_FACE_SIZE) {
    throw new Error(`Face too small: ${faceSize_pixels.toFixed(0)}px (minimum: ${CONFIG.MIN_FACE_SIZE}px). Please move closer to the camera.`);
  }
  
  if (faceSize_pixels > CONFIG.MAX_FACE_SIZE) {
    throw new Error(`Face too large: ${faceSize_pixels.toFixed(0)}px (maximum: ${CONFIG.MAX_FACE_SIZE}px). Please move further from the camera.`);
  }
  
  // ENTERPRISE QUALITY GATE 6: Face quality validation
  if (bestDetection.score < CONFIG.MIN_FACE_QUALITY) {
    throw new Error(`Face detection quality too low: ${(bestDetection.score * 100).toFixed(1)}% (minimum: ${(CONFIG.MIN_FACE_QUALITY * 100).toFixed(0)}%). Please ensure good lighting and face the camera directly.`);
  }
  
  // ENTERPRISE QUALITY GATE 7: Facial landmarks validation
  if (CONFIG.REQUIRE_LANDMARKS) {
    if (!bestDetection.landmarks) {
      console.warn(`   ‚ö†Ô∏è Facial landmarks not available from SCRFD output. This may indicate the model output format changed.`);
      console.warn(`   üí° Continuing without landmark validation, but matching accuracy may be reduced.`);
      // Don't throw error - allow registration to continue without landmarks
      // Landmarks are helpful but not strictly required for face recognition
    } else {
      // Validate all 5 keypoints are detected and reasonable
      const { leftEye, rightEye, nose, leftMouth, rightMouth } = bestDetection.landmarks;
      
      // Log landmark values for debugging
      console.log(`   üìç Landmark values:`, {
        leftEye: leftEye ? `(${leftEye.x.toFixed(1)}, ${leftEye.y.toFixed(1)})` : 'missing',
        rightEye: rightEye ? `(${rightEye.x.toFixed(1)}, ${rightEye.y.toFixed(1)})` : 'missing',
        nose: nose ? `(${nose.x.toFixed(1)}, ${nose.y.toFixed(1)})` : 'missing',
        leftMouth: leftMouth ? `(${leftMouth.x.toFixed(1)}, ${leftMouth.y.toFixed(1)})` : 'missing',
        rightMouth: rightMouth ? `(${rightMouth.x.toFixed(1)}, ${rightMouth.y.toFixed(1)})` : 'missing',
      });
      
      // üè¶ BANK-GRADE: Validate landmarks using normalized coordinates (0-1)
      // Landmarks are already normalized in the new implementation
      const landmarksValid = 
        leftEye && rightEye && nose && leftMouth && rightMouth &&
        // Check if landmarks are within normalized bounds [0, 1]
        leftEye.x >= 0 && leftEye.y >= 0 && leftEye.x <= 1 && leftEye.y <= 1 &&
        rightEye.x >= 0 && rightEye.y >= 0 && rightEye.x <= 1 && rightEye.y <= 1 &&
        nose.x >= 0 && nose.y >= 0 && nose.x <= 1 && nose.y <= 1 &&
        leftMouth.x >= 0 && leftMouth.y >= 0 && leftMouth.x <= 1 && leftMouth.y <= 1 &&
        rightMouth.x >= 0 && rightMouth.y >= 0 && rightMouth.x <= 1 && rightMouth.y <= 1;
      
      if (!landmarksValid) {
        console.warn(`   ‚ö†Ô∏è Facial landmarks detected but values are out of bounds or invalid.`);
        console.warn(`   üí° Image dimensions: ${imageWidth}x${imageHeight}`);
        console.warn(`   üí° Continuing without strict landmark validation, but matching accuracy may be reduced.`);
        // Don't throw error - allow registration to continue
        // The face detection itself is valid (score > threshold), so we can proceed
      } else {
        // Only run geometry validation and liveness check if landmarks are valid
        // üè¶ BANK-GRADE: Validate facial geometry using normalized coordinates
        // Convert normalized eye positions to pixels for distance calculation
        const eyeDistance_pixels = Math.sqrt(
          Math.pow((rightEye.x - leftEye.x) * canonicalWidth, 2) + 
          Math.pow((rightEye.y - leftEye.y) * canonicalHeight, 2)
        );
        const faceWidth_pixels = bestDetection.box.width * canonicalWidth;
        const eyeDistanceRatio = eyeDistance_pixels / faceWidth_pixels;
        
        // Eyes should be roughly 20-75% of face width apart (typical human proportions, relaxed for low-quality cameras and wider-set eyes)
        if (eyeDistanceRatio < CONFIG.MIN_EYE_DISTANCE_RATIO || eyeDistanceRatio > CONFIG.MAX_EYE_DISTANCE_RATIO) {
          console.warn(`   ‚ö†Ô∏è Unusual eye spacing detected (${(eyeDistanceRatio * 100).toFixed(1)}% of face width, expected ${(CONFIG.MIN_EYE_DISTANCE_RATIO * 100).toFixed(0)}-${(CONFIG.MAX_EYE_DISTANCE_RATIO * 100).toFixed(0)}%). This may indicate poor face detection.`);
        }
        
        // ENTERPRISE QUALITY GATE 8: Basic liveness detection using facial geometry
        if (CONFIG.ENABLE_LIVENESS_CHECK) {
          const livenessResult = checkLiveness(bestDetection.landmarks, bestDetection.box);
          if (!livenessResult.isLive) {
            throw new Error(`Liveness check failed: ${livenessResult.reason}`);
          }
          console.log(`   ‚úÖ Liveness check passed: ${livenessResult.score.toFixed(2)} (symmetry: ${(livenessResult.symmetry * 100).toFixed(1)}%, eye ratio: ${(livenessResult.eyeDistanceRatio * 100).toFixed(1)}%)`);
        }
        
        console.log(`   ‚úÖ Facial landmarks validated: All 5 keypoints detected (eyes, nose, mouth)`);
      }
    }
  }
  
  console.log(`   ‚úÖ Quality gates passed: Face size=${faceSize_pixels.toFixed(0)}px, Quality=${(bestDetection.score * 100).toFixed(1)}%, Count=1, Landmarks=${bestDetection.landmarks ? 'Yes' : 'No'}`);

  // üêõ CRITICAL FIX: Return filteredDetections (after NMS), not detections (before NMS)
  // This ensures validatePreview sees the correct face count (1 after NMS, not 5 before NMS)
  return filteredDetections;
}

/**
 * üë§ GENDER DETECTION: Detect gender from face region
 * Uses facial landmarks and features to estimate gender
 * Can be replaced with ONNX gender classification model for better accuracy
 * 
 * @param {Object} detection - Face detection result with landmarks
 * @param {Buffer} canonicalBuffer - Canonical image buffer
 * @param {number} canonicalWidth - Canonical image width
 * @param {number} canonicalHeight - Canonical image height
 * @returns {Object} - { gender: 'MAN'|'WOMAN', confidence: 0-1 }
 */
async function detectGender(detection, canonicalBuffer, canonicalWidth, canonicalHeight) {
  try {
    console.log('üë§ detectGender called - detection:', {
      hasLandmarks: !!detection.landmarks,
      landmarksKeys: detection.landmarks ? Object.keys(detection.landmarks) : 'none'
    });
    
    // If gender model is available, use it
    if (genderModel) {
      // TODO: Implement ONNX gender model inference
      // For now, fall through to heuristic method
    }
    
    // Heuristic-based gender detection using facial landmarks
    // This is a placeholder that can be replaced with a real ONNX model
    if (!detection.landmarks) {
      console.log('‚ö†Ô∏è No landmarks available for gender detection');
      return { gender: 'UNKNOWN', confidence: 0.5 };
    }
    
    const { leftEye, rightEye, nose, leftMouth, rightMouth } = detection.landmarks;
    if (!leftEye || !rightEye || !nose) {
      console.log('‚ö†Ô∏è Missing required landmarks for gender detection:', {
        leftEye: !!leftEye,
        rightEye: !!rightEye,
        nose: !!nose
      });
      return { gender: 'UNKNOWN', confidence: 0.5 };
    }
    
    console.log('üë§ Landmarks available, calculating gender...');
    
    // Convert normalized coordinates to pixels
    const leftEyeX = leftEye.x * canonicalWidth;
    const leftEyeY = leftEye.y * canonicalHeight;
    const rightEyeX = rightEye.x * canonicalWidth;
    const rightEyeY = rightEye.y * canonicalHeight;
    const noseX = nose.x * canonicalWidth;
    const noseY = nose.y * canonicalHeight;
    
    // Calculate facial feature ratios
    const eyeDistance = Math.sqrt(
      Math.pow(rightEyeX - leftEyeX, 2) + 
      Math.pow(rightEyeY - leftEyeY, 2)
    );
    
    const eyeCenterX = (leftEyeX + rightEyeX) / 2;
    const eyeCenterY = (leftEyeY + rightEyeY) / 2;
    
    // Face width estimation (using eye distance as reference)
    const faceWidth = eyeDistance * 2.5; // Approximate face width
    
    // Jaw width estimation (nose to eye center distance)
    const noseToEyeCenter = Math.sqrt(
      Math.pow(noseX - eyeCenterX, 2) +
      Math.pow(noseY - eyeCenterY, 2)
    );
    
    // Feature ratios that tend to differ between genders
    // Note: These are simplified heuristics and may not be accurate
    // A real ONNX gender model would be much more accurate
    const eyeToFaceRatio = eyeDistance / faceWidth;
    const noseToFaceRatio = noseToEyeCenter / faceWidth;
    
    // Simple heuristic: Men tend to have wider faces and larger features
    // This is a very basic approximation - should be replaced with ML model
    let genderScore = 0.5; // Neutral starting point
    
    // Adjust score based on feature ratios (very simplified)
    if (eyeToFaceRatio < 0.25) {
      genderScore += 0.15; // Wider face -> more likely male
    } else if (eyeToFaceRatio > 0.30) {
      genderScore -= 0.15; // Narrower face -> more likely female
    }
    
    if (noseToFaceRatio > 0.15) {
      genderScore += 0.10; // Larger nose -> more likely male
    } else if (noseToFaceRatio < 0.12) {
      genderScore -= 0.10; // Smaller nose -> more likely female
    }
    
    // Clamp score to 0-1 range
    genderScore = Math.max(0.3, Math.min(0.7, genderScore));
    
    // Determine gender and confidence
    const isMale = genderScore > 0.5;
    const confidence = Math.abs(genderScore - 0.5) * 2; // Convert to 0-1 confidence
    
    const result = {
      gender: isMale ? 'MAN' : 'WOMAN',
      confidence: Math.max(0.6, confidence) // Minimum 60% confidence for heuristic method
    };
    
    console.log('üë§ Gender detection result:', result);
    return result;
    
  } catch (error) {
    console.error('‚ùå Gender detection error:', error.message);
    console.error('   Stack:', error.stack);
    return { gender: 'UNKNOWN', confidence: 0.5 };
  }
}

/**
 * üè¶ BANK-GRADE: Preprocess face crop for ArcFace recognition using canonical image
 * Uses normalized coordinates (0-1) from detection and converts to pixel coordinates for cropping
 * 
 * @param {Buffer} canonicalBuffer - Canonical preprocessed image buffer
 * @param {Object} normalizedBox - Normalized face box coordinates (0-1): { x, y, width, height }
 * @param {number} canonicalWidth - Canonical image width
 * @param {number} canonicalHeight - Canonical image height
 * @returns {ort.Tensor} - Preprocessed tensor for ArcFace (1, 3, 112, 112)
 */
async function preprocessForRecognition(canonicalBuffer, normalizedBox, canonicalWidth, canonicalHeight) {
  // üè¶ BANK-GRADE: Convert normalized coordinates (0-1) to pixel coordinates
  // Normalized coordinates are size-invariant and consistent across preprocessing
  const left = Math.max(0, Math.floor(normalizedBox.x * canonicalWidth));
  const top = Math.max(0, Math.floor(normalizedBox.y * canonicalHeight));
  const width = Math.min(
    Math.floor(normalizedBox.width * canonicalWidth),
    Math.floor(canonicalWidth - left) // Ensure we don't exceed image bounds
  );
  const height = Math.min(
    Math.floor(normalizedBox.height * canonicalHeight),
    Math.floor(canonicalHeight - top) // Ensure we don't exceed image bounds
  );
  
  // Validate dimensions are positive integers
  if (width <= 0 || height <= 0) {
    throw new Error(`Invalid face box dimensions: width=${width}, height=${height} (normalized: ${JSON.stringify(normalizedBox)})`);
  }
  
  console.log(`   üè¶ Cropping face: normalized (${normalizedBox.x.toFixed(3)}, ${normalizedBox.y.toFixed(3)}, ${normalizedBox.width.toFixed(3)}, ${normalizedBox.height.toFixed(3)}) ‚Üí pixels (${left}, ${top}, ${width}, ${height})`);
  
  // Crop face region from canonical image
  // CRITICAL: Ensure exact dimensions (112x112 RGB) to match ONNX model input requirements
  const cropResult = await sharp(canonicalBuffer)
    .extract({
      left: left,
      top: top,
      width: width,
      height: height,
    })
    .resize(CONFIG.TARGET_SIZE, CONFIG.TARGET_SIZE, { 
      fit: 'fill',  // Fill the entire 112x112 area (may crop or pad)
      position: 'center'
    })
    .removeAlpha() // Remove alpha channel - we only need RGB
    .raw() // Get raw pixel data
    .toBuffer({ resolveWithObject: true });

  const cropData = cropResult.data;
  const cropInfo = cropResult.info;

  // Validate exact dimensions (must be exactly 112x112 RGB for ONNX model)
  const expectedWidth = CONFIG.TARGET_SIZE;
  const expectedHeight = CONFIG.TARGET_SIZE;
  const expectedChannels = 3; // RGB
  const expectedPixels = expectedWidth * expectedHeight * expectedChannels;
  const actualPixels = cropData.length;
  
  if (cropInfo.width !== expectedWidth || cropInfo.height !== expectedHeight) {
    throw new Error(`Image resize failed: Expected ${expectedWidth}x${expectedHeight} but got ${cropInfo.width}x${cropInfo.height}`);
  }
  
  if (cropInfo.channels !== expectedChannels) {
    throw new Error(`Image channel mismatch: Expected ${expectedChannels} channels (RGB) but got ${cropInfo.channels}`);
  }
  
  if (actualPixels !== expectedPixels) {
    console.error(`‚ùå Tensor dimension mismatch!`);
    console.error(`   Expected: ${expectedPixels} pixels (${expectedWidth}x${expectedHeight}x${expectedChannels})`);
    console.error(`   Actual: ${actualPixels} pixels`);
    console.error(`   Crop info: width=${cropInfo.width}, height=${cropInfo.height}, channels=${cropInfo.channels}`);
    throw new Error(`Image preprocessing failed: Expected ${expectedPixels} pixels but got ${actualPixels}. Image: ${cropInfo.width}x${cropInfo.height}, channels: ${cropInfo.channels}`);
  }

  console.log(`   ‚úÖ Face crop validated: ${cropInfo.width}x${cropInfo.height}x${cropInfo.channels}, ${actualPixels} pixels`);

  // Create tensor: [1, 3, 112, 112] in CHW format (Channel-Height-Width)
  const tensorShape = [1, 3, CONFIG.TARGET_SIZE, CONFIG.TARGET_SIZE];
  const reshaped = new Float32Array(1 * 3 * CONFIG.TARGET_SIZE * CONFIG.TARGET_SIZE);
  
  // Convert HWC (Height-Width-Channel) to CHW (Channel-Height-Width) format for ONNX
  // Sharp raw buffer is RGB interleaved: [R0, G0, B0, R1, G1, B1, R2, G2, B2, ...]
  // We need CHW: [R0...R(112*112-1), G0...G(112*112-1), B0...B(112*112-1)]
  const tensorSize = CONFIG.TARGET_SIZE; // 112
  const tensorChannels = 3; // RGB
  
  for (let c = 0; c < tensorChannels; c++) {
    for (let h = 0; h < tensorSize; h++) {
      for (let w = 0; w < tensorSize; w++) {
        // Source: HWC format (RGB interleaved)
        const srcIdx = (h * tensorSize + w) * tensorChannels + c;
        // Destination: CHW format (all R, then all G, then all B)
        const dstIdx = c * tensorSize * tensorSize + h * tensorSize + w;
        
        // Normalize pixel value: (pixel - 127.5) / 128.0
        // ArcFace expects values in range approximately [-1, 1]
        reshaped[dstIdx] = (cropData[srcIdx] - 127.5) / 128.0;
      }
    }
  }

  // Validate tensor shape before returning
  const expectedSize = tensorShape.reduce((a, b) => a * b, 1);
  if (reshaped.length !== expectedSize) {
    throw new Error(`Tensor size mismatch: Expected ${expectedSize} elements but got ${reshaped.length}. Shape: [${tensorShape.join(', ')}]`);
  }

  console.log(`   ‚úÖ Tensor created: shape=[${tensorShape.join(', ')}], size=${reshaped.length} elements`);

  return new ort.Tensor('float32', reshaped, tensorShape);
}

/**
 * üè¶ BANK-GRADE Phase 5: Extract face embedding from ID document
 * ID documents have standardized photos - use as stable anchor template
 * 
 * @param {Buffer} imageBuffer - ID document image buffer
 * @returns {Object} - Embedding with quality metadata { embedding, quality }
 */
async function generateIDEmbedding(imageBuffer) {
  console.log('üÜî ====== EXTRACTING ID DOCUMENT EMBEDDING ======');
  console.log(`   üì¶ ID image buffer size: ${imageBuffer?.length || 0} bytes`);
  
  // üè¶ BANK-GRADE Phase 5: Store original threshold for restoration
  const originalThreshold = CONFIG.MIN_DETECTION_SCORE;
  
  try {
    await loadModels();
  } catch (err) {
    console.error('‚ùå ONNX models not loaded:', err?.message || err);
    throw new Error('ONNX models are not available. Please run: npm run download-models');
  }
  
  try {
    // Use same preprocessing as regular embedding, but with stricter quality checks for ID
    console.log('üè¶ Applying canonical preprocessing for ID document...');
    const canonicalData = await preprocessCanonical(imageBuffer, null);
    console.log(`   ‚úÖ Canonical preprocessing complete: ${canonicalData.originalWidth}x${canonicalData.originalHeight} ‚Üí ${canonicalData.canonicalWidth}x${canonicalData.canonicalHeight}`);
    
    // Detect faces in ID document (use lower threshold for ID documents)
    console.log('üîç Detecting face in ID document (using relaxed threshold for ID photos)...');
    
    // üè¶ BANK-GRADE Phase 5: ID documents often have smaller faces - use lower detection threshold
    // Temporarily lower the detection threshold for ID documents (30% instead of 50%)
    CONFIG.MIN_DETECTION_SCORE = 0.30; // 30% threshold for ID documents (more lenient)
    console.log(`   üîß Using relaxed detection threshold: 30% (instead of 50%) for ID document`);
    
    let detections;
    try {
      detections = await detectFaces(canonicalData.canonicalBuffer, canonicalData.canonicalWidth, canonicalData.canonicalHeight);
      console.log(`   ‚úÖ Face detection complete: ${detections.length} face(s) found`);
    } finally {
      // Always restore original threshold
      CONFIG.MIN_DETECTION_SCORE = originalThreshold;
    }
    
    if (!detections || detections.length === 0) {
      throw new Error('No face detected in ID document. Please ensure the ID photo is clear and visible.');
    }
    
    // Use the largest/most confident face (ID documents typically have one face)
    let bestDetection = detections[0];
    for (const detection of detections) {
      if (detection.score > bestDetection.score) {
        bestDetection = detection;
      }
    }
    
    console.log(`‚úÖ ID face detected - Confidence: ${(bestDetection.score * 100).toFixed(1)}%`);
    
    // üè¶ BANK-GRADE Phase 5: Validate ID face quality (more lenient than selfie - ID photos are often lower quality)
    // ID documents have standardized photos but may be lower resolution/quality than live selfies
    // Accept 50%+ for ID documents (vs 70%+ for selfies) - ID photos are often lower quality than live selfies
    const ID_MIN_CONFIDENCE = 0.50; // 50% minimum for ID documents (more lenient for ID photo quality variations)
    if (bestDetection.score < ID_MIN_CONFIDENCE) {
      throw new Error(`ID face detection quality too low: ${(bestDetection.score * 100).toFixed(1)}% (minimum: 50%). Please ensure the ID photo is clear and well-lit.`);
    }
    
    // Generate embedding from ID face
    const recognitionStartTime = Date.now();
    const faceTensor = await preprocessForRecognition(canonicalData.canonicalBuffer, bestDetection.box, canonicalData.canonicalWidth, canonicalData.canonicalHeight);
    const preprocessingTime = Date.now() - recognitionStartTime;
    console.log(`üßÆ ID preprocessing complete (${preprocessingTime}ms), generating 512-d embedding...`);
    
    // Run recognition model
    const inputName = recognitionModel.inputNames[0];
    const feeds = { [inputName]: faceTensor };
    
    const inferenceStartTime = Date.now();
    const outputs = await recognitionModel.run(feeds);
    const inferenceTime = Date.now() - inferenceStartTime;
    console.log(`‚úÖ ID inference complete (${inferenceTime}ms). Outputs: ${Object.keys(outputs).join(', ')}`);
    
    // Extract embedding from output
    const outputName = recognitionModel.outputNames[0];
    const embeddingTensor = outputs[outputName];
    
    if (!embeddingTensor || !embeddingTensor.data) {
      throw new Error('Failed to extract embedding from ID document');
    }
    
    // Convert tensor to array and normalize
    const embedding = Array.from(embeddingTensor.data);
    const norm = Math.sqrt(embedding.reduce((sum, val) => sum + val * val, 0));
    const normalizedEmbedding = norm > 0 ? embedding.map(val => val / norm) : embedding;
    
    // Extract quality metrics
    // üè¶ BANK-GRADE Phase 5: Calculate face dimensions from normalized box coordinates
    // bestDetection.box is an object: { x, y, width, height } (all normalized 0-1)
    // Convert to pixels using canonical dimensions
    const faceWidth_pixels = bestDetection.box ? (bestDetection.box.width * canonicalData.canonicalWidth) : 0;
    const faceHeight_pixels = bestDetection.box ? (bestDetection.box.height * canonicalData.canonicalHeight) : 0;
    const faceSize_pixels = faceWidth_pixels * faceHeight_pixels;
    
    const quality = {
      score: bestDetection.score,
      sharpness: canonicalData.qualityMetrics?.blurScore || 0.75,
      blurVariance: canonicalData.qualityMetrics?.blurVariance || 100,
      brightness: canonicalData.qualityMetrics?.brightness || 0.5,
      faceSize: faceSize_pixels || 0,
      faceWidth: faceWidth_pixels || 0,
      faceHeight: faceHeight_pixels || 0,
      detectionScore: bestDetection.score,
      extractedAt: new Date()
    };
    
    const totalTime = Date.now() - recognitionStartTime;
    console.log(`‚úÖ 512-d ID embedding generated in ${totalTime}ms`);
    console.log(`   Quality: ${(quality.score * 100).toFixed(1)}%, Sharpness: ${(quality.sharpness * 100).toFixed(1)}%`);
    
    return {
      embedding: normalizedEmbedding,
      quality
    };
  } catch (error) {
    // üè¶ BANK-GRADE Phase 5: Restore original threshold if it was changed
    if (typeof originalThreshold !== 'undefined' && CONFIG.MIN_DETECTION_SCORE !== originalThreshold) {
      CONFIG.MIN_DETECTION_SCORE = originalThreshold;
      console.log(`   üîß Restored original detection threshold: ${(originalThreshold * 100).toFixed(0)}%`);
    }
    console.error('‚ùå Error extracting ID embedding:', error);
    throw error;
  }
}

/**
 * üè¶ BANK-GRADE: Generate 512-d face embedding using ArcFace with canonical preprocessing
 * Returns embedding with quality metadata for bank-grade matching
 * 
 * @param {Buffer} imageBuffer - Original image buffer
 * @param {string} deviceFingerprint - Optional device fingerprint for adaptive quality thresholds
 * @returns {Object} - Embedding with quality metadata
 */
async function generateEmbedding(imageBuffer, deviceFingerprint = null) {
  const totalStartTime = Date.now();
  
  console.log('üßÆ ====== STARTING EMBEDDING GENERATION (BANK-GRADE) ======');
  console.log(`   üì¶ Image buffer size: ${imageBuffer?.length || 0} bytes`);
  
  try {
    await loadModels();
  } catch (err) {
    console.error('‚ùå ONNX models not loaded:', err?.message || err);
    throw new Error('ONNX models are not available. Please run: npm run download-models');
  }

  try {
    // üè¶ BANK-GRADE: Canonical preprocessing (single size for detection + recognition)
    // Pass deviceFingerprint for adaptive quality thresholds based on device history
    console.log('üè¶ Applying canonical preprocessing...');
    const canonicalData = await preprocessCanonical(imageBuffer, deviceFingerprint);
    console.log(`   ‚úÖ Canonical preprocessing complete: ${canonicalData.originalWidth}x${canonicalData.originalHeight} ‚Üí ${canonicalData.canonicalWidth}x${canonicalData.canonicalHeight}`);
    
    // üè¶ BANK-GRADE: Detect faces using canonical image (returns normalized coordinates)
    console.log('üîç Detecting faces with SCRFD (canonical image)...');
    const detections = await detectFaces(canonicalData.canonicalBuffer, canonicalData.canonicalWidth, canonicalData.canonicalHeight);
    console.log(`   ‚úÖ Face detection complete: ${detections.length} face(s) found`);
    
    if (!detections || detections.length === 0) {
      throw new Error('No face detected in image. Please ensure your face is visible and well-lit.');
    }

    // Use the largest/most confident face
    let bestDetection = detections[0];
    for (const detection of detections) {
      if (detection.score > bestDetection.score) {
        bestDetection = detection;
      }
    }

    console.log(`‚úÖ Face detected - Confidence: ${(bestDetection.score * 100).toFixed(1)}%`);

    // üè¶ BANK-GRADE: Generate embedding using canonical image with normalized coordinates
    const recognitionStartTime = Date.now();
    const faceTensor = await preprocessForRecognition(canonicalData.canonicalBuffer, bestDetection.box, canonicalData.canonicalWidth, canonicalData.canonicalHeight);
    const preprocessingTime = Date.now() - recognitionStartTime;
    console.log(`üßÆ Preprocessing complete (${preprocessingTime}ms), generating 512-d embedding...`);
    
    // Use actual input name from model (this model uses 'input.1', not 'blob')
    if (!recognitionModel || !recognitionModel.inputNames || recognitionModel.inputNames.length === 0) {
      console.error('‚ùå Recognition model has no input names!');
      throw new Error('Recognition model input names not available');
    }
    
    const inputName = recognitionModel.inputNames[0];
    
    // CRITICAL: Capture inputName and tensor in local variables to ensure closure works correctly
    const capturedInputName = inputName;
    const capturedTensor = faceTensor;
    
    // Validate captured values BEFORE queuing
    if (!capturedInputName || typeof capturedInputName !== 'string' || capturedInputName.trim().length === 0) {
      throw new Error(`Invalid input name: "${capturedInputName}" (type: ${typeof capturedInputName})`);
    }
    
    if (!capturedTensor || !(capturedTensor instanceof ort.Tensor)) {
      throw new Error(`Invalid tensor: ${capturedTensor?.constructor?.name || typeof capturedTensor}`);
    }
    
    // Use mutex to ensure thread-safe inference (ONNX Runtime sessions are not thread-safe)
    const inferenceStartTime = Date.now();
    recognitionInferenceQueue = recognitionInferenceQueue.then(async () => {
      try {
        // Create feeds object fresh inside the promise to ensure it's not lost
        // CRITICAL: Use Object.create(null) to avoid prototype issues
        const feeds = Object.create(null);
        feeds[capturedInputName] = capturedTensor;
        
        // Double-check feeds object was created correctly
        if (!feeds || typeof feeds !== 'object') {
          process.stderr.write(`\n‚ùå [RECOGNITION] Failed to create feeds object\n`);
          throw new Error('Failed to create feeds object');
        }
        
        // Use 'in' operator instead of hasOwnProperty (works with Object.create(null))
        if (!(capturedInputName in feeds)) {
          process.stderr.write(`\n‚ùå [RECOGNITION] Feeds missing key "${capturedInputName}". Keys: ${Object.keys(feeds).join(', ')}\n`);
          throw new Error(`Feeds object missing input key "${capturedInputName}". Keys: ${Object.keys(feeds).join(', ')}`);
        }
        
        if (!feeds[capturedInputName] || !(feeds[capturedInputName] instanceof ort.Tensor)) {
          process.stderr.write(`\n‚ùå [RECOGNITION] Invalid tensor in feeds for "${capturedInputName}"\n`);
          throw new Error(`Invalid tensor in feeds for input "${capturedInputName}"`);
        }
        
        console.log(`   üîß Feeds object created with key: "${capturedInputName}"`);
        console.log(`   üì¶ Feeds object keys: ${Object.keys(feeds).join(', ')}`);
        console.log(`   üì¶ Feed value is Tensor: ${feeds[capturedInputName] instanceof ort.Tensor}`);
        console.log(`   üì¶ Feed value shape: [${feeds[capturedInputName].dims.join(', ')}]`);
        console.log(`   üì¶ Feed value type: ${feeds[capturedInputName].type}`);
        
        // Force flush console output
        if (process.stdout.isTTY) {
          process.stdout.write('');
        }
        
        // Run inference with validated feeds object
        process.stdout.write(`\nüöÄ [RECOGNITION] Calling recognitionModel.run() now...\n`);
        const inferenceResult = await recognitionModel.run(feeds);
        const inferenceTime = Date.now() - inferenceStartTime;
        console.log(`   ‚úÖ Inference complete (${inferenceTime}ms). Outputs: ${Object.keys(inferenceResult).join(', ')}`);
        return inferenceResult;
      } catch (inferenceError) {
        // IMMEDIATE error logging to stderr (always flushed)
        process.stderr.write(`\n‚ùå ========== RECOGNITION INFERENCE FAILED ==========\n`);
        process.stderr.write(`‚ùå Error message: ${inferenceError.message}\n`);
        if (recognitionModel && recognitionModel.inputNames) {
          process.stderr.write(`üìã Model expects inputs: ${recognitionModel.inputNames.join(', ')}\n`);
        } else {
          process.stderr.write(`üìã Model or input names not available\n`);
        }
        process.stderr.write(`üì¶ Using input name: "${capturedInputName}" (type: ${typeof capturedInputName})\n`);
        process.stderr.write(`üì¶ Captured tensor exists: ${!!capturedTensor}\n`);
        process.stderr.write(`üì¶ Captured tensor is Tensor: ${capturedTensor instanceof ort.Tensor}\n`);
        if (capturedTensor) {
          process.stderr.write(`üì¶ Captured tensor shape: [${capturedTensor.dims?.join(', ') || 'N/A'}]\n`);
        }
        if (inferenceError.stack) {
          process.stderr.write(`‚ùå Error stack: ${inferenceError.stack}\n`);
        }
        process.stderr.write(`‚ùå ===============================================\n`);
        
        // Also log to console.error for normal logging
        console.error(`   ‚ùå Recognition inference failed`);
        if (recognitionModel && recognitionModel.inputNames) {
          console.error(`   üìã Model expects inputs: ${recognitionModel.inputNames.join(', ')}`);
        } else {
          console.error(`   üìã Model or input names not available`);
        }
        console.error(`   üì¶ Using input name: "${capturedInputName}" (type: ${typeof capturedInputName})`);
        console.error(`   üì¶ Captured tensor exists: ${!!capturedTensor}`);
        console.error(`   üì¶ Captured tensor is Tensor: ${capturedTensor instanceof ort.Tensor}`);
        if (capturedTensor) {
          console.error(`   üì¶ Captured tensor shape: [${capturedTensor.dims?.join(', ') || 'N/A'}]`);
        }
        console.error(`   ‚ùå Error message: ${inferenceError.message}`);
        console.error(`   ‚ùå Error stack: ${inferenceError.stack}`);
        throw inferenceError;
      }
    });
    
    const results = await recognitionInferenceQueue;
    
    // Extract embedding (512-d vector)
    // Adjust output name based on your model
    const embedding = Array.from(results.output?.data || results.fc1?.data || Object.values(results)[0].data);
    
    if (!embedding || embedding.length !== 512) {
      throw new Error(`Invalid embedding size: ${embedding?.length || 0} (expected 512)`);
    }

    // Normalize embedding (L2 normalization for cosine similarity)
    const norm = Math.sqrt(embedding.reduce((sum, val) => sum + val * val, 0));
    const normalizedEmbedding = embedding.map(val => val / norm);

    const embeddingTotalTime = Date.now() - totalStartTime;
    console.log(`‚úÖ 512-d embedding generated in ${embeddingTotalTime}ms (${(embeddingTotalTime/1000).toFixed(1)}s)`);
    
    // Performance warning if too slow
    if (embeddingTotalTime > 10000) {
      console.warn(`‚ö†Ô∏è [PERF] Embedding generation took ${(embeddingTotalTime/1000).toFixed(1)}s - this is slower than expected`);
    }

    // üè¶ BANK-GRADE: Calculate face quality metrics for storage
    const faceWidth_pixels = bestDetection.box.width * canonicalData.canonicalWidth;
    const faceHeight_pixels = bestDetection.box.height * canonicalData.canonicalHeight;
    const faceSize = faceWidth_pixels * faceHeight_pixels;
    
    // Calculate pose angles from landmarks if available
    let yaw = 0, pitch = 0, roll = 0;
    if (bestDetection.landmarks) {
      // Estimate angles from landmark positions (simplified)
      const eyeDistance = Math.sqrt(
        Math.pow((bestDetection.landmarks.rightEye.x - bestDetection.landmarks.leftEye.x) * canonicalData.canonicalWidth, 2) +
        Math.pow((bestDetection.landmarks.rightEye.y - bestDetection.landmarks.leftEye.y) * canonicalData.canonicalHeight, 2)
      );
      // Simple pose estimation (can be enhanced with more sophisticated methods)
      yaw = Math.min(15, Math.abs(0.5 - (bestDetection.landmarks.leftEye.x + bestDetection.landmarks.rightEye.x) / 2) * 30);
      pitch = Math.min(15, Math.abs(0.5 - bestDetection.landmarks.nose.y) * 30);
      roll = 0; // Would need more calculation
    }

    // üè¶ BANK-GRADE: Return embedding with comprehensive quality metadata
    return {
      embedding: normalizedEmbedding,  // L2-normalized 512-d embedding
      quality: {
        score: bestDetection.score,           // Detection confidence (0-1)
        sharpness: canonicalData.quality.blurScore,  // Blur/sharpness score (0-1)
        blurVariance: canonicalData.quality.blurVariance,  // Laplacian variance
        brightness: canonicalData.quality.brightness,  // Brightness (0-1)
        faceSize: faceSize,                   // Face size in pixels¬≤
        faceWidth: faceWidth_pixels,
        faceHeight: faceHeight_pixels,
        pose: { yaw, pitch, roll },          // Pose angles in degrees
        detectionScore: bestDetection.score,
        faceCount: detections.length,
      },
      qualityMetrics: canonicalData.qualityMetrics, // üè¶ BANK-GRADE: Quality metrics for device tracking
      deviceQuality: canonicalData.deviceQuality,   // üè¶ BANK-GRADE: Device quality record (if available)
      isLowQualityDevice: canonicalData.isLowQualityDevice, // üè¶ BANK-GRADE: Device quality flag
      // Backward compatibility
      detectionScore: bestDetection.score,
      faceCount: detections.length,
    };
  } catch (error) {
    const errorMessage = error?.message || String(error) || 'Unknown error';
    console.error('‚ùå Error generating embedding:', errorMessage);
    throw error instanceof Error ? error : new Error(errorMessage);
  }
}

/**
 * Cosine similarity for comparing embeddings
 */
function cosineSimilarity(embedding1, embedding2) {
  if (embedding1.length !== embedding2.length) {
    return 0;
  }
  
  let dotProduct = 0;
  let norm1 = 0;
  let norm2 = 0;
  
  for (let i = 0; i < embedding1.length; i++) {
    dotProduct += embedding1[i] * embedding2[i];
    norm1 += embedding1[i] * embedding1[i];
    norm2 += embedding2[i] * embedding2[i];
  }
  
  norm1 = Math.sqrt(norm1);
  norm2 = Math.sqrt(norm2);
  
  if (norm1 === 0 || norm2 === 0) return 0;
  
  return dotProduct / (norm1 * norm2);
}

/**
 * Track failed match for active learning
 * ENTERPRISE: Records failed attempts to improve matching accuracy
 */
async function trackFailedMatch(embeddingData, bestMatch, bestSimilarity, threshold, extraData = {}) {
  try {
    if (!CONFIG.ENABLE_ACTIVE_LEARNING) return;
    
    const FailedMatch = require('../models/FailedMatch');
    
    const reason = bestMatch 
      ? (bestSimilarity < CONFIG.ABSOLUTE_MINIMUM_SIMILARITY ? 'below_threshold' : 'ambiguous_match')
      : 'no_match';
    
    // Extract quality score (can be object or number)
    const qualityScore = typeof embeddingData.quality === 'object'
      ? (embeddingData.quality?.score || embeddingData.quality?.detectionScore || 0.75)
      : (embeddingData.quality || 0.75);
    
    await FailedMatch.create({
      staffId: bestMatch?.staff?._id || null,
      embedding: embeddingData?.embedding || embeddingData || null,
      bestSimilarity,
      bestMatchStaffId: bestMatch?.staff?._id || null,
      bestMatchStaffName: bestMatch?.staff?.name || null,
      quality: qualityScore, // Extract numeric score from quality object
      reason: extraData.reason || reason,
      reviewed: false,
      extraData: extraData // Store additional context
    });
    
    console.log(`üìä Failed match tracked for active learning (reason: ${reason}, similarity: ${(bestSimilarity * 100).toFixed(2)}%)`);
    
    // Check if threshold adjustment is needed
    await checkAndAdjustThresholds();
  } catch (error) {
    // Fail silently - don't break matching if tracking fails
    console.warn('‚ö†Ô∏è Failed to track failed match:', error.message);
  }
}

/**
 * Check and adjust thresholds based on failed matches (active learning)
 * ENTERPRISE: Automatically improves matching accuracy over time
 */
async function checkAndAdjustThresholds() {
  try {
    if (!CONFIG.ENABLE_ACTIVE_LEARNING) return;
    
    const FailedMatch = require('../models/FailedMatch');
    
    // Count recent failed matches (last 7 days)
    const sevenDaysAgo = new Date(Date.now() - 7 * 24 * 60 * 60 * 1000);
    const recentFailures = await FailedMatch.countDocuments({
      timestamp: { $gte: sevenDaysAgo },
      reviewed: false
    });
    
    if (recentFailures >= CONFIG.FAILED_MATCH_THRESHOLD) {
      console.log(`üìä Active learning: ${recentFailures} failed matches detected (threshold: ${CONFIG.FAILED_MATCH_THRESHOLD})`);
      console.log(`   ‚ö†Ô∏è Consider reviewing failed matches and adjusting thresholds if needed.`);
      console.log(`   üí° Current thresholds: min=${(CONFIG.MIN_SIMILARITY_THRESHOLD * 100).toFixed(0)}%, abs_min=${(CONFIG.ABSOLUTE_MINIMUM_SIMILARITY * 100).toFixed(0)}%`);
      
      // Note: Automatic threshold adjustment is disabled for safety
      // Admin should review failed matches manually and adjust if needed
    }
  } catch (error) {
    console.warn('‚ö†Ô∏è Active learning check failed:', error.message);
  }
}

/**
 * Validate time-based pattern for clock-in/out
 * ENTERPRISE: Checks if clock time matches expected work hours
 */
function validateTimePattern(clockType, timestamp) {
  try {
    if (!CONFIG.ENABLE_TIME_VALIDATION) {
      return { valid: true };
    }
    
    const now = new Date(timestamp);
    const hour = now.getHours();
    const minute = now.getMinutes();
    
    // Only validate clock-in times
    if (clockType !== 'in') {
      return { valid: true };
    }
    
    // Calculate expected time window
    const expectedHour = CONFIG.EXPECTED_CLOCK_IN_HOUR;
    const expectedMinute = CONFIG.EXPECTED_CLOCK_IN_MINUTE;
    const tolerance = CONFIG.TIME_TOLERANCE_MINUTES;
    
    // Convert to minutes since midnight for easier comparison
    const currentMinutes = hour * 60 + minute;
    const expectedMinutes = expectedHour * 60 + expectedMinute;
    const diffMinutes = Math.abs(currentMinutes - expectedMinutes);
    
    if (diffMinutes <= tolerance) {
      return { valid: true };
    }
    
    // Outside tolerance window - warn but don't block
    const expectedTime = `${String(expectedHour).padStart(2, '0')}:${String(expectedMinute).padStart(2, '0')}`;
    const currentTime = `${String(hour).padStart(2, '0')}:${String(minute).padStart(2, '0')}`;
    const diffHours = Math.floor(diffMinutes / 60);
    const diffMins = diffMinutes % 60;
    
    return {
      valid: true, // Don't block, just warn
      warning: `Clock-in time ${currentTime} is ${diffHours > 0 ? `${diffHours}h ` : ''}${diffMins}min ${currentMinutes > expectedMinutes ? 'after' : 'before'} expected time (${expectedTime} ¬±${tolerance}min). This may be flagged for review.`
    };
  } catch (error) {
    console.warn('‚ö†Ô∏è Time validation failed:', error.message);
    return { valid: true }; // Fail open
  }
}

/**
 * üè¶ BANK-GRADE Phase 3: Generate device fingerprint from request headers
 * Creates a consistent device identifier for verification
 * 
 * @param {Object} requestHeaders - Express request headers
 * @returns {string} - Device fingerprint hash
 */
function normalizeHeaders(requestHeaders = {}) {
  const normalized = {};
  if (!requestHeaders) {
    return normalized;
  }

  Object.keys(requestHeaders).forEach(key => {
    normalized[key.toLowerCase()] = requestHeaders[key];
  });

  return normalized;
}

function parseDeviceInfoHeader(requestHeaders = {}) {
  const headers = normalizeHeaders(requestHeaders);
  const headerValue = headers['x-device-info'] || headers['x_device_info'] || headers['x-deviceinfo'];

  if (!headerValue) {
    return null;
  }

  if (typeof headerValue === 'object') {
    return headerValue;
  }

  try {
    return JSON.parse(headerValue);
  } catch (error) {
    console.warn('‚ö†Ô∏è Failed to parse x-device-info header:', error.message);
    return null;
  }
}

function generateDeviceFingerprint(requestHeaders, options = {}) {
  if (!CONFIG.ENABLE_DEVICE_FINGERPRINTING) {
    return options.includeMeta ? { fingerprint: null, deviceInfo: null } : null;
  }
  
  const crypto = require('crypto');
  const headers = normalizeHeaders(requestHeaders);
  const deviceInfo = parseDeviceInfoHeader(requestHeaders);
  const fingerprintValues = {};
  const fingerprintData = [];
  
  CONFIG.DEVICE_FINGERPRINT_FIELDS.forEach(field => {
    const lowerField = field.toLowerCase();
    const kebabField = field.replace(/([a-z])([A-Z])/g, '$1-$2').toLowerCase();
    const noDashField = kebabField.replace(/-/g, '');
    
    let value = requestHeaders?.[field] ??
                headers[lowerField] ??
                headers[kebabField] ??
                headers[noDashField] ??
                headers[`x-${lowerField}`] ??
                headers[`x-${kebabField}`] ??
                headers[`x-device-${lowerField}`] ??
                headers[`x-device-${kebabField}`] ??
                headers[`x-device-${field.toLowerCase()}`] ??
                headers[`x-${field.toLowerCase()}`];
    
    if (!value && deviceInfo && Object.prototype.hasOwnProperty.call(deviceInfo, field)) {
      value = deviceInfo[field];
    }
    
    const sanitizedValue = value !== undefined && value !== null ? String(value) : '';
    fingerprintValues[field] = sanitizedValue;
    fingerprintData.push(`${field}:${sanitizedValue}`);
  });
  
  if (deviceInfo && !fingerprintValues['x-device-info']) {
    fingerprintData.push(`x-device-info:${JSON.stringify(deviceInfo)}`);
  }
  
  const fingerprintString = fingerprintData.join('|');
  const secret = CONFIG.DEVICE_FINGERPRINT_SECRET || 'faceclock-device-secret';
  const hash = crypto.createHmac('sha256', secret).update(fingerprintString).digest('hex');
  const fingerprint = hash.substring(0, 32);
  
  if (options.includeMeta) {
    return {
      fingerprint,
      deviceInfo,
      fingerprintComponents: fingerprintValues,
    };
  }
  
  return fingerprint;
}

function normalizeDeviceInfo(deviceInfo) {
  if (!deviceInfo || typeof deviceInfo !== 'object') {
    return null;
  }

  const normalized = {};
  const setValue = (key, value) => {
    if (value !== undefined && value !== null && value !== '') {
      normalized[key] = value;
    }
  };

  setValue('platform', deviceInfo.platform || deviceInfo.Platform);
  setValue('brand', deviceInfo.brand || deviceInfo.deviceBrand);
  setValue('manufacturer', deviceInfo.manufacturer);
  setValue('modelName', deviceInfo.modelName || deviceInfo.model);
  setValue('osVersion', deviceInfo.osVersion || deviceInfo.os);
  setValue('appVersion', deviceInfo.appVersion);
  setValue('buildNumber', deviceInfo.buildNumber);
  setValue('language', deviceInfo.language);
  setValue('timezone', deviceInfo.timezone || deviceInfo.timeZone);
  setValue('deviceId', deviceInfo.deviceId || deviceInfo.deviceID);
  setValue('deviceHash', deviceInfo.deviceHash || deviceInfo.hash);
  setValue('screenWidth', deviceInfo.screenWidth);
  setValue('screenHeight', deviceInfo.screenHeight);
  setValue('screenScale', deviceInfo.screenScale);
  setValue('deviceType', deviceInfo.deviceType);
  setValue('totalMemory', deviceInfo.totalMemory);

  normalized.raw = deviceInfo;
  return normalized;
}

function createTrustedDeviceEntry(fingerprint, deviceInfo = null, overrides = {}) {
  if (!fingerprint) {
    return null;
  }

  const now = new Date();
  const normalizedInfo = normalizeDeviceInfo(deviceInfo);

  return {
    fingerprint,
    label: overrides.label || 'Registered Device',
    status: overrides.status || 'trusted',
    addedAt: overrides.addedAt || now,
    lastSeenAt: overrides.lastSeenAt || now,
    ...(normalizedInfo ? { deviceInfo: normalizedInfo } : {}),
  };
}

/**
 * üè¶ BANK-GRADE Phase 3: Calculate temporal signal (0-1) based on recent matches
 * Returns confidence boost from temporal consistency (recent successful matches)
 * 
 * @param {string} staffId - Staff member ID
 * @param {number} baseSimilarity - Base face similarity (must be ‚â•75% to apply)
 * @returns {Promise<number>} - Temporal signal (0-1), max CONFIG.TEMPORAL_MAX_BOOST
 */
async function calculateTemporalSignal(staffId, baseSimilarity) {
  // Only apply temporal signal if base similarity is high enough
  if (baseSimilarity < CONFIG.TEMPORAL_MIN_BASE_SIMILARITY) {
    return 0;
  }
  
  try {
    const ClockLog = require('../models/ClockLog');
    const now = new Date();
    const windowStart = new Date(now.getTime() - CONFIG.TEMPORAL_WINDOW_HOURS * 60 * 60 * 1000);
    
    // Count recent successful clock-ins within window
    const recentLogs = await ClockLog.find({
      staffId: staffId,
      timestamp: { $gte: windowStart },
      confidence: { $gte: CONFIG.MIN_SIMILARITY_THRESHOLD }
    }).sort({ timestamp: -1 }).limit(10).lean();
    
    if (recentLogs.length === 0) {
      return 0; // No recent matches, no temporal signal
    }
    
    // Calculate temporal signal: more recent = higher signal, decays over time
    let totalSignal = 0;
    for (const log of recentLogs) {
      const hoursAgo = (now - new Date(log.timestamp)) / (60 * 60 * 1000);
      const decayFactor = Math.max(0, 1 - (hoursAgo / CONFIG.TEMPORAL_WINDOW_HOURS));
      // Confidence is stored as 0-100 (percentage), so divide by 100 to get 0-1
      const confidence = log.confidence / 100;
      totalSignal += decayFactor * confidence; // Weight by confidence
    }
    
    // Normalize to 0-1 signal (weight is applied in fusion formula, not here)
    // The signal represents confidence from temporal consistency (0 = no history, 1 = strong history)
    const normalizedSignal = Math.min(1.0, totalSignal / recentLogs.length);
    
    if (normalizedSignal > 0) {
      console.log(`   ‚è∞ Temporal signal: ${(normalizedSignal * 100).toFixed(2)}% (${recentLogs.length} recent match(es) within ${CONFIG.TEMPORAL_WINDOW_HOURS}h)`);
    }
    
    // Return 0-1 signal (weight is applied in fusion formula)
    return normalizedSignal;
  } catch (error) {
    console.warn('‚ö†Ô∏è Error calculating temporal signal:', error.message);
    return 0; // Fail silently, don't break matching
  }
}

/**
 * üè¶ BANK-GRADE Phase 3: Calculate device signal (0-1) based on device consistency
 * Returns confidence boost if device matches previous successful clock-ins
 * 
 * @param {string} staffId - Staff member ID
 * @param {string} deviceFingerprint - Current device fingerprint
 * @returns {Promise<number>} - Device signal (0-1)
 */
async function calculateDeviceSignal(staffId, deviceFingerprint) {
  if (!deviceFingerprint || !CONFIG.ENABLE_DEVICE_FINGERPRINTING) {
    return 0;
  }
  
  try {
    const ClockLog = require('../models/ClockLog');
    const now = new Date();
    const windowStart = new Date(now.getTime() - 7 * 24 * 60 * 60 * 1000); // Last 7 days
    
    // Find recent clock-ins with same device
    const recentLogs = await ClockLog.find({
      staffId: staffId,
      timestamp: { $gte: windowStart },
      deviceFingerprint: deviceFingerprint, // Must match device
      confidence: { $gte: CONFIG.MIN_SIMILARITY_THRESHOLD }
    }).sort({ timestamp: -1 }).limit(5).lean();
    
    if (recentLogs.length === 0) {
      return 0; // No recent matches from this device
    }
    
    // Device consistency signal (0-1)
    const deviceSignal = Math.min(1.0, recentLogs.length / 5); // Max at 5 matches
    
    if (deviceSignal > 0) {
      console.log(`   üì± Device signal: ${(deviceSignal * 100).toFixed(2)}% (${recentLogs.length} recent match(es) from same device)`);
    }
    
    // Return 0-1 signal (weight is applied in fusion formula, not here)
    return deviceSignal;
  } catch (error) {
    console.warn('‚ö†Ô∏è Error calculating device signal:', error.message);
    return 0;
  }
}

/**
 * üè¶ BANK-GRADE Phase 3: Calculate location signal (0-1) based on location consistency
 * Returns confidence boost if location matches assigned location (already validated)
 * 
 * @param {boolean} locationValid - Whether location validation passed
 * @returns {number} - Location signal (0-1)
 */
function calculateLocationSignal(locationValid) {
  if (!locationValid) {
    return 0; // Location invalid, no signal
  }
  
  // Location is valid, return 1.0 (100% confidence)
  // The weight (CONFIG.FUSION_WEIGHTS.location) is applied in the fusion formula
  return 1.0;
}

/**
 * üè¶ BANK-GRADE Phase 4: Enhanced liveness detection
 * Checks for depth variation, motion, and other liveness indicators
 * 
 * @param {Object} detection - Face detection result with landmarks
 * @param {Object} metadata - Additional metadata (can include motion/depth data if available)
 * @returns {Object} - Liveness result { isLive: boolean, score: number, reasons: string[] }
 */
function checkEnhancedLiveness(detection, metadata = {}) {
  if (!CONFIG.ENABLE_ENHANCED_LIVENESS) {
    return { isLive: true, score: 1.0, reasons: [] }; // Skip if disabled
  }
  
  const reasons = [];
  let score = 1.0;
  
  // Basic liveness checks (existing)
  if (CONFIG.ENABLE_LIVENESS_CHECK) {
    const eyeDistance = detection.landmarks ? 
      Math.sqrt(
        Math.pow(detection.landmarks.rightEye.x - detection.landmarks.leftEye.x, 2) +
        Math.pow(detection.landmarks.rightEye.y - detection.landmarks.leftEye.y, 2)
      ) : 0;
    
    const faceWidth = detection.box.width || 0;
    const eyeDistanceRatio = faceWidth > 0 ? eyeDistance / faceWidth : 0;
    
    if (eyeDistanceRatio < CONFIG.MIN_EYE_DISTANCE_RATIO || eyeDistanceRatio > CONFIG.MAX_EYE_DISTANCE_RATIO) {
      score -= 0.3;
      reasons.push('Unusual eye spacing (possible photo)');
    }
  }
  
  // Enhanced checks (if metadata available)
  if (metadata.depthVariation !== undefined) {
    if (metadata.depthVariation < CONFIG.LIVENESS_DEPTH_THRESHOLD) {
      score -= 0.3;
      reasons.push('Low depth variation (possible 2D image)');
    }
  }
  
  if (metadata.motion !== undefined) {
    if (metadata.motion < CONFIG.LIVENESS_MOTION_THRESHOLD) {
      score -= 0.2;
      reasons.push('No motion detected (possible static image)');
    }
  }
  
  const isLive = score >= 0.6; // At least 60% confidence
  
  return {
    isLive,
    score: Math.max(0, Math.min(1, score)),
    reasons
  };
}

/**
 * üè¶ BANK-GRADE Phase 4: Calculate risk score (0-1) based on multiple factors
 * Higher score = higher risk (more suspicious)
 * 
 * @param {Object} signals - Signal values { face, temporal, device, location }
 * @param {Object} context - Context data { baseSimilarity, quality, locationValid, etc }
 * @returns {Object} - Risk score { score: number, level: string, factors: string[] }
 */
function normalizeScore(value) {
  if (typeof value !== 'number' || !isFinite(value)) {
    return 0;
  }
  if (value > 1) {
    return Math.min(1, value / 100);
  }
  if (value < 0) {
    return 0;
  }
  return value;
}

function calculateRiskScore(signals, context) {
  let riskScore = 0;
  const factors = [];
  const weights = CONFIG.RISK_WEIGHTS || {};
  
  const normalizedSignals = {
    face: normalizeScore(signals.face),
    temporal: normalizeScore(signals.temporal),
    device: normalizeScore(signals.device),
    location: normalizeScore(signals.location),
  };
  
  const normalizedQuality = normalizeScore(context.quality ?? 0.75);
  const normalizedLiveness = normalizeScore(context.liveness ?? 1);
  const locationValid = context.locationValid !== false;
  
  // Face similarity risk (lower similarity = higher risk)
  const normalizedFaceSimilarity = normalizeScore(context.baseSimilarity);
  const faceRisk = 1 - normalizedFaceSimilarity;
  riskScore += faceRisk * (weights.face ?? 0.5);
  if (faceRisk > 0.3) {
    factors.push(`Face similarity only ${(normalizedFaceSimilarity * 100).toFixed(1)}%`);
  }
  
  // Quality risk (lower quality = higher risk)
  const qualityRisk = 1 - normalizedQuality;
  riskScore += qualityRisk * (weights.quality ?? 0.2);
  if (qualityRisk > 0.3) {
    factors.push(`Image quality only ${(normalizedQuality * 100).toFixed(1)}%`);
  }
  
  // Temporal risk (no recent matches = higher risk)
  const temporalRisk = 1 - normalizedSignals.temporal;
  riskScore += temporalRisk * (weights.temporal ?? 0.1);
  if (normalizedSignals.temporal < 0.1) {
    factors.push('No recent successful clock-ins from this account');
  }
  
  // Device risk (new device = higher risk)
  const deviceRisk = 1 - normalizedSignals.device;
  riskScore += deviceRisk * (weights.device ?? 0.1);
  if (normalizedSignals.device < 0.1 && CONFIG.ENABLE_DEVICE_FINGERPRINTING) {
    factors.push('Device not recognized as trusted');
  }
  
  // Location risk (invalid location = higher risk)
  const locationRisk = locationValid ? 0 : 1;
  riskScore += locationRisk * (weights.location ?? 0.1);
  if (!locationValid) {
    factors.push('Outside assigned location/geofence');
  }
  
  // Liveness risk (low liveness score = higher risk)
  const livenessRisk = 1 - normalizedLiveness;
  riskScore += livenessRisk * (weights.liveness ?? 0);
  if (normalizedLiveness < 0.7) {
    factors.push('Liveness confidence too low');
  }
  
  // Normalize to 0-1
  riskScore = Math.max(0, Math.min(1, riskScore));
  
  // Determine risk level
  let level = 'low';
  if (riskScore >= CONFIG.RISK_THRESHOLDS.critical) {
    level = 'critical';
  } else if (riskScore >= CONFIG.RISK_THRESHOLDS.high) {
    level = 'high';
  } else if (riskScore >= CONFIG.RISK_THRESHOLDS.medium) {
    level = 'medium';
  }
  
  return {
    score: riskScore,
    level,
    factors
  };
}

/**
 * üè¶ BANK-GRADE: Compute weighted centroid template from multiple embeddings
 * Uses quality-weighted averaging to create a single representative template
 * 
 * @param {Array} embeddings - Array of 512-d normalized embeddings
 * @param {Array} qualities - Array of quality objects (matching embedding indices)
 * @returns {Array} - 512-d normalized centroid embedding
 */
function computeCentroidTemplate(embeddings, qualities) {
  if (!embeddings || embeddings.length === 0) {
    throw new Error('No embeddings provided for centroid computation');
  }
  
  if (embeddings.length === 1) {
    return embeddings[0]; // Single embedding, return as-is
  }
  
  // Compute quality weights (normalized to sum to 1.0)
  const weights = qualities.map((q, idx) => {
    // Weight by quality score (0-1), sharpness (0-1), and detection score (0-1)
    const qualityWeight = (q?.score || 0.5) * 0.4;
    const sharpnessWeight = (q?.sharpness || 0.5) * 0.3;
    const detectionWeight = (q?.detectionScore || 0.5) * 0.3;
    return qualityWeight + sharpnessWeight + detectionWeight;
  });
  
  const totalWeight = weights.reduce((sum, w) => sum + w, 0);
  
  // Handle edge case: if total weight is 0 or very small, use equal weights
  let normalizedWeights;
  if (totalWeight < 0.001) {
    console.warn(`‚ö†Ô∏è Total weight is very small (${totalWeight}), using equal weights for centroid`);
    normalizedWeights = weights.map(() => 1.0 / embeddings.length);
  } else {
    normalizedWeights = weights.map(w => w / totalWeight);
  }
  
  // Compute weighted average embedding
  const centroid = new Array(512).fill(0);
  let validEmbeddings = 0;
  for (let i = 0; i < embeddings.length; i++) {
    const embedding = embeddings[i];
    const weight = normalizedWeights[i];
    
    if (!embedding || embedding.length !== 512) {
      console.warn(`‚ö†Ô∏è Skipping invalid embedding at index ${i} (length: ${embedding?.length || 0})`);
      continue;
    }
    
    // Validate weight is a valid number
    if (isNaN(weight) || !isFinite(weight)) {
      console.warn(`‚ö†Ô∏è Invalid weight at index ${i} (${weight}), using equal weight`);
      const equalWeight = 1.0 / embeddings.length;
      for (let j = 0; j < 512; j++) {
        centroid[j] += embedding[j] * equalWeight;
      }
    } else {
      for (let j = 0; j < 512; j++) {
        centroid[j] += embedding[j] * weight;
      }
    }
    validEmbeddings++;
  }
  
  if (validEmbeddings === 0) {
    throw new Error('No valid embeddings found for centroid computation');
  }
  
  // Normalize centroid to unit length (L2 normalization)
  const norm = Math.sqrt(centroid.reduce((sum, val) => sum + val * val, 0));
  
  if (norm < 0.001) {
    console.error(`‚ùå Centroid norm is too small (${norm}), centroid computation may have failed`);
    // Fallback: use first valid embedding as centroid
    for (let i = 0; i < embeddings.length; i++) {
      if (embeddings[i] && Array.isArray(embeddings[i]) && embeddings[i].length === 512) {
        const fallbackNorm = Math.sqrt(embeddings[i].reduce((sum, val) => sum + val * val, 0));
        if (fallbackNorm > 0.001) {
          console.warn(`‚ö†Ô∏è Using first valid embedding as fallback centroid`);
          return embeddings[i].map(val => val / fallbackNorm);
        }
      }
    }
    throw new Error('Failed to compute centroid: all embeddings are invalid');
  }
  
  const normalizedCentroid = centroid.map(val => val / norm);
  
  console.log(`   üè¶ Centroid template computed from ${validEmbeddings}/${embeddings.length} embeddings (weights: ${normalizedWeights.map(w => w.toFixed(3)).join(', ')}, norm: ${norm.toFixed(3)})`);
  
  return normalizedCentroid;
}

/**
 * üè¶ BANK-GRADE: Find best matching staff with centroid fusion, adaptive gap rules, and calibrated thresholds
 */
async function findMatchingStaff(embeddingData, staffList, options = {}) {
  const matchingStartTime = Date.now();
  
  if (!embeddingData) {
    console.error('‚ùå findMatchingStaff: embeddingData is null or undefined');
    return null;
  }
  
  const embedding = embeddingData.embedding || embeddingData;
  const qualityData = embeddingData.quality || { score: 0.75 }; // Default quality if not provided
  
  if (!embedding || !Array.isArray(embedding) || embedding.length === 0) {
    console.error('‚ùå findMatchingStaff: Invalid embedding');
    return null;
  }

  // Validate embedding is 512-d (ArcFace)
  if (embedding.length !== 512) {
    console.error(`‚ùå findMatchingStaff: Embedding size mismatch. Expected 512-d (ArcFace), got ${embedding.length}-d`);
    return null;
  }
  
  if (!staffList || !Array.isArray(staffList) || staffList.length === 0) {
    console.error('‚ùå findMatchingStaff: Invalid staff list');
    return null;
  }
  
  // üè¶ BANK-GRADE: Calibrated, quality-adaptive thresholds
  // Separate thresholds for enrollment vs daily use
  const useType = options.useType || 'daily'; // 'enrollment' or 'daily'
  const isSameDevice = options.sameDevice || false;
  
  // Determine quality tier - quality can be an object or a number
  const qualityScore = typeof qualityData === 'object' ? (qualityData.score || qualityData.detectionScore || 0.75) : (qualityData || 0.75);
  let qualityTier = 'medium';
  if (qualityScore >= 0.85) {
    qualityTier = 'high';
  } else if (qualityScore < 0.70) {
    qualityTier = 'low';
  }
  
  // Get threshold from calibrated config
  let threshold;
  if (useType === 'enrollment') {
    threshold = CONFIG.THRESHOLDS[qualityTier.toUpperCase() + '_QUALITY'].enrollment;
  } else if (isSameDevice) {
    threshold = CONFIG.THRESHOLDS[qualityTier.toUpperCase() + '_QUALITY'].sameDevice;
  } else {
    threshold = CONFIG.THRESHOLDS[qualityTier.toUpperCase() + '_QUALITY'].daily;
  }
  
  threshold = Math.max(threshold, CONFIG.ABSOLUTE_MINIMUM_SIMILARITY); // Never below absolute minimum
  
  console.log(`üè¶ Bank-grade matching - Threshold: ${(threshold * 100).toFixed(1)}% (${qualityTier} quality, ${useType}, ${isSameDevice ? 'same device' : 'different device'})`);
  
  console.log(`üîç Matching with ArcFace - Threshold: ${(threshold * 100).toFixed(1)}% (quality: ${(qualityScore * 100).toFixed(1)}%)`);
  console.log(`üì¶ Clock-in embedding length: ${embedding.length}, normalized: ${embedding.length === 512 ? 'Yes' : 'No'}`);
  
  let bestMatch = null;
  let bestSimilarity = 0;
  const candidates = [];
  let nearThresholdCandidates = []; // üö® CRITICAL: Track candidates within 3% below threshold for ambiguity checking
  
  console.log(`üîç Comparing with ${staffList.length} registered staff members...`);
  console.log(`üìã Staff list: ${staffList.map(s => s.name).join(', ')}`);
  
  for (const staff of staffList) {
    // Support multiple embeddings per person
    let staffEmbeddings = [];
    
    if (staff.faceEmbeddings && Array.isArray(staff.faceEmbeddings) && staff.faceEmbeddings.length > 0) {
      staffEmbeddings = staff.faceEmbeddings;
      console.log(`   üì¶ ${staff.name}: Found ${staffEmbeddings.length} stored embedding(s)`);
    } else {
      const singleEmbedding = staff.decryptedEmbedding || staff.faceEmbedding;
      if (singleEmbedding && Array.isArray(singleEmbedding) && singleEmbedding.length > 0) {
        staffEmbeddings = [singleEmbedding];
        console.log(`   üì¶ ${staff.name}: Found 1 stored embedding (legacy format)`);
      }
    }
    
    if (staffEmbeddings.length === 0) {
      console.warn(`   ‚ö†Ô∏è ${staff.name}: No embeddings found, skipping...`);
      continue;
    }
    
    // Verify first embedding is valid
    if (staffEmbeddings[0] && Array.isArray(staffEmbeddings[0])) {
      const firstEmbedding = staffEmbeddings[0];
      const firstNorm = Math.sqrt(firstEmbedding.reduce((sum, val) => sum + val * val, 0));
      console.log(`   üìä ${staff.name}: First embedding - length: ${firstEmbedding.length}, norm: ${firstNorm.toFixed(3)}, normalized: ${Math.abs(firstNorm - 1.0) < 0.01 ? 'Yes' : 'No'}`);
    }
    
    // üè¶ BANK-GRADE: Use centroid fusion (centroid + max similarity fusion)
    // Centroid represents average appearance, max handles edge cases
    let centroidEmbedding = null;
    let centroidSimilarity = 0;
    let maxSimilarity = 0;
    const similarities = [];
    
    // Check if staff has precomputed centroid
    if (staff.centroidEmbedding && Array.isArray(staff.centroidEmbedding) && staff.centroidEmbedding.length === 512) {
      centroidEmbedding = staff.centroidEmbedding;
      console.log(`   üè¶ ${staff.name}: Using precomputed centroid template`);
    } else if (staffEmbeddings.length > 1) {
      // Compute centroid from individual embeddings and qualities
      // üêõ CRITICAL FIX: Better quality data handling for ALL staff (existing and new)
      let qualities = staff.embeddingQualities;
      
      // If quality data is missing or incomplete, compute quality scores from embeddings
      // This helps existing staff who don't have quality data stored
      if (!qualities || qualities.length !== staffEmbeddings.length) {
        console.warn(`   ‚ö†Ô∏è ${staff.name}: Quality data mismatch (${qualities?.length || 0} qualities vs ${staffEmbeddings.length} embeddings)`);
        
        // üè¶ BANK-GRADE FIX: Compute quality scores from embedding norms (better than defaults)
        // Higher norm typically indicates better quality embedding
        qualities = staffEmbeddings.map((emb, idx) => {
          if (!emb || !Array.isArray(emb) || emb.length !== 512) {
            return { score: 0.5, sharpness: 0.5, detectionScore: 0.5 }; // Low quality for invalid
          }
          
          // Compute norm as quality indicator (normalized embeddings should have norm ~1.0)
          const norm = Math.sqrt(emb.reduce((sum, val) => sum + val * val, 0));
          const normQuality = Math.min(1.0, norm); // Quality based on how close to 1.0
          
          // If we have existing quality data for this index, use it
          if (qualities && qualities[idx]) {
            return qualities[idx];
          }
          
          // Otherwise, estimate quality from norm
          // Norm close to 1.0 = good quality, far from 1.0 = lower quality
          const estimatedScore = Math.max(0.6, Math.min(0.9, normQuality * 0.9 + 0.1));
          
          return {
            score: estimatedScore,
            sharpness: estimatedScore,
            detectionScore: estimatedScore,
          };
        });
        
        console.log(`   üîß ${staff.name}: Computed quality scores from embedding norms (better than defaults)`);
      }
      
      // Validate all quality records are valid
      const validQualities = qualities.filter(q => q && typeof q.score === 'number');
      if (validQualities.length !== staffEmbeddings.length) {
        console.warn(`   ‚ö†Ô∏è ${staff.name}: Some quality records invalid, using defaults for missing ones`);
        qualities = staffEmbeddings.map((emb, idx) => {
          if (qualities[idx] && typeof qualities[idx].score === 'number') {
            return qualities[idx];
          }
          // Default quality
          return { score: 0.75, sharpness: 0.75, detectionScore: 0.75 };
        });
      }
      
      // Compute centroid with validated quality data
      try {
        centroidEmbedding = computeCentroidTemplate(staffEmbeddings, qualities);
        
        // üè¶ BANK-GRADE FIX: Validate centroid was computed successfully
        if (!centroidEmbedding || !Array.isArray(centroidEmbedding) || centroidEmbedding.length !== 512) {
          throw new Error('Centroid computation returned invalid result');
        }
        
        // Validate centroid norm (should be ~1.0 after normalization)
        const centroidNorm = Math.sqrt(centroidEmbedding.reduce((sum, val) => sum + val * val, 0));
        if (centroidNorm < 0.5 || centroidNorm > 1.5) {
          console.warn(`   ‚ö†Ô∏è ${staff.name}: Centroid norm is unusual (${centroidNorm.toFixed(3)}), expected ~1.0`);
        }
        
        console.log(`   üè¶ ${staff.name}: Computed centroid template from ${staffEmbeddings.length} embeddings`);
      } catch (centroidError) {
        console.error(`   ‚ùå ${staff.name}: Centroid computation failed: ${centroidError.message}`);
        // Fallback: use first valid embedding as centroid
        const firstValid = staffEmbeddings.find(e => e && Array.isArray(e) && e.length === 512);
        if (firstValid) {
          const firstNorm = Math.sqrt(firstValid.reduce((sum, val) => sum + val * val, 0));
          centroidEmbedding = firstNorm > 0.001 ? firstValid.map(val => val / firstNorm) : firstValid;
          console.warn(`   ‚ö†Ô∏è ${staff.name}: Using first embedding as centroid fallback`);
        } else {
          console.error(`   ‚ùå ${staff.name}: No valid embeddings found, skipping centroid`);
          centroidEmbedding = null;
        }
      }
    } else {
      // Single embedding - use as both centroid and max
      centroidEmbedding = staffEmbeddings[0];
      console.log(`   üè¶ ${staff.name}: Single embedding (used as centroid)`);
    }
    
    // üè¶ BANK-GRADE Phase 5: Compute ID embedding similarity (stable anchor template)
    let idSimilarity = 0;
    if (staff.idEmbedding && Array.isArray(staff.idEmbedding) && staff.idEmbedding.length === 512) {
      // Ensure ID embedding is normalized
      const idNorm = Math.sqrt(staff.idEmbedding.reduce((sum, val) => sum + val * val, 0));
      if (idNorm > 0) {
        const normalizedID = Math.abs(idNorm - 1.0) > 0.01
          ? staff.idEmbedding.map(val => val / idNorm)
          : staff.idEmbedding;
        idSimilarity = cosineSimilarity(embedding, normalizedID);
        // Validate idSimilarity is a valid number
        if (isNaN(idSimilarity) || !isFinite(idSimilarity)) {
          console.warn(`   ‚ö†Ô∏è ${staff.name}: Invalid ID similarity (${idSimilarity}), using 0`);
          idSimilarity = 0;
        } else {
          console.log(`   üÜî ${staff.name} - ID similarity: ${(idSimilarity * 100).toFixed(2)}%`);
        }
      }
    }
    
    // Compute centroid similarity (weighted average appearance)
    if (centroidEmbedding && Array.isArray(centroidEmbedding) && centroidEmbedding.length === 512) {
      // Ensure centroid is normalized
      const centroidNorm = Math.sqrt(centroidEmbedding.reduce((sum, val) => sum + val * val, 0));
      if (centroidNorm > 0) {
        const normalizedCentroid = Math.abs(centroidNorm - 1.0) > 0.01
          ? centroidEmbedding.map(val => val / centroidNorm)
          : centroidEmbedding;
        centroidSimilarity = cosineSimilarity(embedding, normalizedCentroid);
        // Validate centroidSimilarity is a valid number
        if (isNaN(centroidSimilarity) || !isFinite(centroidSimilarity)) {
          console.warn(`   ‚ö†Ô∏è ${staff.name}: Invalid centroid similarity (${centroidSimilarity}), using 0`);
          centroidSimilarity = 0;
        }
      } else {
        console.warn(`   ‚ö†Ô∏è ${staff.name}: Centroid norm is 0, using 0 similarity`);
        centroidSimilarity = 0;
      }
    } else {
      console.warn(`   ‚ö†Ô∏è ${staff.name}: Invalid centroid embedding, using 0 similarity`);
      centroidSimilarity = 0;
    }
    
    // üè¶ BANK-GRADE: Compute max similarity across individual embeddings (for fusion)
    for (let i = 0; i < staffEmbeddings.length; i++) {
      const staffEmbedding = staffEmbeddings[i];
      
      if (!staffEmbedding || !Array.isArray(staffEmbedding) || staffEmbedding.length === 0) {
        continue;
      }
      
      // ONLY support 512-d embeddings (ONNX ArcFace) - 128-d embeddings are no longer supported
      if (staffEmbedding.length !== 512) {
        console.error(`‚ùå ${staff.name}: Invalid embedding size: ${staffEmbedding.length} (expected 512-d ArcFace)`);
        continue;
      }
      
      // Ensure embedding is normalized
      const staffNorm = Math.sqrt(staffEmbedding.reduce((sum, val) => sum + val * val, 0));
      if (staffNorm <= 0) {
        console.warn(`   ‚ö†Ô∏è ${staff.name}: Embedding ${i + 1} has zero norm, skipping`);
        continue;
      }
      
      let normalizedStaffEmbedding = staffEmbedding;
      if (Math.abs(staffNorm - 1.0) > 0.01) {
        normalizedStaffEmbedding = staffEmbedding.map(val => val / staffNorm);
      }
      
      const similarity = cosineSimilarity(embedding, normalizedStaffEmbedding);
      
      // Validate similarity is a valid number
      if (isNaN(similarity) || !isFinite(similarity)) {
        console.warn(`   ‚ö†Ô∏è ${staff.name}: Invalid similarity for embedding ${i + 1} (${similarity}), skipping`);
        continue;
      }
      
      similarities.push(similarity);
      
      if (similarity > maxSimilarity) {
        maxSimilarity = similarity;
      }
      
      console.log(`   üîç ${staff.name} - Embedding ${i + 1}/${staffEmbeddings.length}: ${(similarity * 100).toFixed(2)}%`);
    }
    
    // Validate maxSimilarity before fusion
    if (isNaN(maxSimilarity) || !isFinite(maxSimilarity)) {
      console.warn(`   ‚ö†Ô∏è ${staff.name}: Invalid maxSimilarity (${maxSimilarity}), using 0`);
      maxSimilarity = 0;
    }
    
    // üè¶ BANK-GRADE: Centroid fusion (0.7 * centroid + 0.3 * max)
    // Centroid captures average appearance, max handles edge cases
    // üêõ CRITICAL FIX: If maxSimilarity is above threshold, use it to prevent false rejections
    // This ensures strong individual matches aren't reduced below threshold by fusion
    let selfieSimilarity; // Similarity from selfie embeddings (centroid + max fusion)
    if (maxSimilarity === 0 && centroidSimilarity > 0) {
      // Edge case: Only centroid available, use it directly (fairness)
      selfieSimilarity = centroidSimilarity;
      console.warn(`   ‚ö†Ô∏è ${staff.name}: No valid individual embeddings, using centroid similarity only (${(centroidSimilarity * 100).toFixed(2)}%)`);
    } else if (centroidSimilarity === 0 && maxSimilarity > 0) {
      // Edge case: Only max available, use it directly (fairness)
      selfieSimilarity = maxSimilarity;
      console.warn(`   ‚ö†Ô∏è ${staff.name}: Centroid unavailable, using max similarity only (${(maxSimilarity * 100).toFixed(2)}%)`);
    } else {
      // Normal case: Use weighted fusion
      const fusedSimilarity = CONFIG.CENTROID_FUSION_WEIGHT * centroidSimilarity + CONFIG.MAX_FUSION_WEIGHT * maxSimilarity;
      
      // üêõ CRITICAL FIX: If maxSimilarity is above threshold, use the higher of fused or max
      // This prevents strong matches (e.g., 83.42%) from being reduced below threshold by fusion
      // Example: max=83.42%, centroid=76%, fused=78.23% ‚Üí use 83.42% (max) to ensure match succeeds
      if (maxSimilarity >= threshold && fusedSimilarity < threshold) {
        console.warn(`   ‚ö†Ô∏è ${staff.name}: Max similarity (${(maxSimilarity * 100).toFixed(2)}%) above threshold but fusion (${(fusedSimilarity * 100).toFixed(2)}%) below - using max to prevent false rejection`);
        selfieSimilarity = maxSimilarity; // Use max to ensure match succeeds
      } else if (maxSimilarity >= threshold * 0.95 && fusedSimilarity < threshold) {
        // If max is very close to threshold (within 5%), also use max
        console.warn(`   ‚ö†Ô∏è ${staff.name}: Max similarity (${(maxSimilarity * 100).toFixed(2)}%) close to threshold, fusion (${(fusedSimilarity * 100).toFixed(2)}%) below - using max for fairness`);
        selfieSimilarity = maxSimilarity;
      } else {
        // Normal case: Use fused similarity
        selfieSimilarity = fusedSimilarity;
      }
    }
    
    // üè¶ BANK-GRADE Phase 5: Use max(ID similarity, selfie similarity) as base similarity
    // ID embedding is the stable anchor - if it matches well, use it
    // If selfie matches better, use that (handles appearance changes)
    let baseSimilarity;
    if (idSimilarity > 0) {
      // ID embedding available - use max of ID and selfie
      baseSimilarity = Math.max(idSimilarity, selfieSimilarity);
      if (idSimilarity > selfieSimilarity) {
        console.log(`   üÜî ${staff.name}: Using ID similarity (${(idSimilarity * 100).toFixed(2)}%) > selfie (${(selfieSimilarity * 100).toFixed(2)}%)`);
      } else {
        console.log(`   üì∏ ${staff.name}: Using selfie similarity (${(selfieSimilarity * 100).toFixed(2)}%) > ID (${(idSimilarity * 100).toFixed(2)}%)`);
      }
    } else {
      // No ID embedding - use selfie only
      baseSimilarity = selfieSimilarity;
    }
    
    // Validate baseSimilarity
    if (isNaN(baseSimilarity) || !isFinite(baseSimilarity) || baseSimilarity <= 0) {
      console.error(`   ‚ùå ${staff.name}: Invalid baseSimilarity (${baseSimilarity}) - centroid: ${centroidSimilarity}, max: ${maxSimilarity}`);
      continue; // Skip this staff member
    }
    
    console.log(`   üè¶ ${staff.name} - Centroid: ${(centroidSimilarity * 100).toFixed(2)}%, Max: ${(maxSimilarity * 100).toFixed(2)}%, Fusion: ${(baseSimilarity * 100).toFixed(2)}%`);
    
    // Log comparison for debugging
    if (similarities.length > 1) {
      const minSimilarity = Math.min(...similarities);
      const avgSimilarity = similarities.reduce((a, b) => a + b, 0) / similarities.length;
      console.log(`   üìä ${staff.name} - Similarity range: ${(minSimilarity * 100).toFixed(2)}% - ${(maxSimilarity * 100).toFixed(2)}% (avg: ${(avgSimilarity * 100).toFixed(2)}%)`);
    }
    
    // üè¶ BANK-GRADE Phase 3: Multi-signal fusion (face, temporal, device, location)
    // Replace additive boost with weighted fusion for better security
    let temporalSignal = 0;
    let deviceSignal = 0;
    let locationSignal = 0;
    
    // Calculate temporal signal (only if base similarity is high enough)
    if (baseSimilarity >= CONFIG.TEMPORAL_MIN_BASE_SIMILARITY) {
      temporalSignal = await calculateTemporalSignal(staff._id, baseSimilarity);
    }
    
    // Calculate device signal (if device fingerprinting is enabled)
    if (options.deviceFingerprint && CONFIG.ENABLE_DEVICE_FINGERPRINTING) {
      deviceSignal = await calculateDeviceSignal(staff._id, options.deviceFingerprint);
    }
    
    // Calculate location signal (if location validation passed)
    locationSignal = calculateLocationSignal(options.locationValid !== false);
    
    // üè¶ BANK-GRADE: Multi-signal fusion (weighted combination)
    // final_score = w_face * face_score + w_temporal * temporal_signal + w_device * device_signal + w_location * location_signal
    // üö® CRITICAL FIX: When multiple candidates exist and face scores are close, reduce or ignore non-face signals
    // This prevents auxiliary signals from pushing an incorrect candidate above threshold
    const hasAdditionalSignals = temporalSignal > 0 || deviceSignal > 0;
    let fusedScore;
    
    // üö® CRITICAL FIX: Check if we're in an ambiguous situation (will be checked later, but prepare here)
    // We'll check this in the matching loop context - for now, use conservative fusion
    const SAFE_IGNORE_GAP = 0.06; // 6% - if face scores are within this, ignore non-face signals
    const isAmbiguousContext = false; // Will be set when we have all candidates
    
    if (!hasAdditionalSignals) {
      // üêõ CRITICAL FIX: No temporal/device signals: Use face score directly (don't penalize it with 80% weight)
      // Location validation is handled separately, we don't need to boost score for it
      fusedScore = baseSimilarity;
      console.log(`   üè¶ ${staff.name}: No temporal/device signals - Using face score directly: ${(fusedScore * 100).toFixed(2)}% (not penalized by fusion)`);
    } else {
      // Has temporal/device signals: Use full weighted fusion
      // NOTE: If ambiguous context detected later, we'll override this to use face score only
      fusedScore = 
        CONFIG.FUSION_WEIGHTS.face * baseSimilarity +
        CONFIG.FUSION_WEIGHTS.temporal * temporalSignal +
        CONFIG.FUSION_WEIGHTS.device * deviceSignal +
        CONFIG.FUSION_WEIGHTS.location * locationSignal;
    }
    
    // üêõ CRITICAL FIX: If baseSimilarity (face score) is above threshold, don't let fusion reduce it below threshold
    // This prevents strong face matches from being rejected due to fusion
    // Example: baseSimilarity=78.23% (above 72%), but fused=67.58% (below) ‚Üí use baseSimilarity
    let similarity;
    if (baseSimilarity >= threshold && fusedScore < threshold) {
      // Face score is above threshold but fusion reduced it below ‚Üí use face score
      console.warn(`   ‚ö†Ô∏è ${staff.name}: Base similarity (${(baseSimilarity * 100).toFixed(2)}%) above threshold but fusion (${(fusedScore * 100).toFixed(2)}%) below - using base similarity to prevent false rejection`);
      similarity = baseSimilarity;
    } else if (baseSimilarity >= threshold * 0.95 && fusedScore < threshold) {
      // Face score is very close to threshold (within 5%) but fusion below ‚Üí use face score for fairness
      console.warn(`   ‚ö†Ô∏è ${staff.name}: Base similarity (${(baseSimilarity * 100).toFixed(2)}%) close to threshold, fusion (${(fusedScore * 100).toFixed(2)}%) below - using base similarity for fairness`);
      similarity = baseSimilarity;
    } else if (!hasAdditionalSignals && baseSimilarity > fusedScore) {
      // üêõ CRITICAL FIX: If no additional signals and face score is higher than fused, use face score
      // This ensures we don't penalize face score when there's no context to fuse
      similarity = baseSimilarity;
    } else {
      // Normal case: Use fused score (capped at 1.0)
      similarity = Math.min(1.0, fusedScore);
    }
    
    // Log multi-signal fusion details
    if (temporalSignal > 0 || deviceSignal > 0 || locationSignal > 0) {
      console.log(`   üè¶ ${staff.name} - Multi-signal fusion:`);
      console.log(`      Face: ${(baseSimilarity * 100).toFixed(2)}% (weight: ${(CONFIG.FUSION_WEIGHTS.face * 100).toFixed(0)}%)`);
      if (temporalSignal > 0) {
        console.log(`      Temporal: ${(temporalSignal * 100).toFixed(2)}% (weight: ${(CONFIG.FUSION_WEIGHTS.temporal * 100).toFixed(0)}%)`);
      }
      if (deviceSignal > 0) {
        console.log(`      Device: ${(deviceSignal * 100).toFixed(2)}% (weight: ${(CONFIG.FUSION_WEIGHTS.device * 100).toFixed(0)}%)`);
      }
      if (locationSignal > 0) {
        console.log(`      Location: ${(locationSignal * 100).toFixed(2)}% (weight: ${(CONFIG.FUSION_WEIGHTS.location * 100).toFixed(0)}%)`);
      }
      console.log(`      Fused score: ${(similarity * 100).toFixed(2)}%`);
    }
    
    const matchStatus = similarity >= threshold ? '‚úÖ MATCH' : '‚ùå below threshold';
    const gapFromThreshold = similarity >= threshold ? '' : ` (${((threshold - similarity) * 100).toFixed(1)}% below threshold)`;
    console.log(`   üë§ ${staff.name}: ${(similarity * 100).toFixed(2)}% ${matchStatus}${gapFromThreshold} (best of ${staffEmbeddings.length} embedding(s))`);
    
    // üè¶ BANK-GRADE Phase 4: Calculate risk score
    const signals = {
      face: baseSimilarity,
      temporal: temporalSignal,
      device: deviceSignal,
      location: locationSignal,
    };
    const riskScore = calculateRiskScore(signals, {
      baseSimilarity,
      quality: qualityScore, // Use the already-extracted qualityScore
      locationValid: options.locationValid !== false,
    });
    
    if (riskScore.score > CONFIG.RISK_THRESHOLDS.low) {
      console.log(`   ‚ö†Ô∏è Risk score: ${(riskScore.score * 100).toFixed(1)}% (${riskScore.level})`);
      if (riskScore.factors.length > 0) {
        console.log(`      Factors: ${riskScore.factors.join(', ')}`);
      }
    }
    if (similarities.length > 1) {
      const minSimilarity = Math.min(...similarities);
      const maxSimilarity = Math.max(...similarities);
      const avgSimilarity = similarities.reduce((a, b) => a + b, 0) / similarities.length;
      console.log(`      ‚îî‚îÄ Similarity range: ${(minSimilarity * 100).toFixed(2)}% - ${(maxSimilarity * 100).toFixed(2)}% (avg: ${(avgSimilarity * 100).toFixed(2)}%, using best: ${(maxSimilarity * 100).toFixed(2)}%)`);
    }
    
      // CRITICAL: Always track the best similarity across ALL staff (even if below threshold)
      // This ensures we find the actual best match, not just the first one above threshold
      if (similarity > bestSimilarity) {
        bestSimilarity = similarity;
        bestMatch = { 
          staff, 
          similarity, 
          baseSimilarity, 
          signals: { temporal: temporalSignal, device: deviceSignal, location: locationSignal },
          riskScore,
          centroidSimilarity,
          maxSimilarity,
          allSimilarities: similarities // Store all similarities for detailed logging
        };
        console.log(`   ‚≠ê New best match: ${staff.name} with ${(similarity * 100).toFixed(2)}% (best of ${staffEmbeddings.length} embeddings)`);
      }
    
    // Add to candidates if above threshold
    if (similarity >= threshold) {
      // üè¶ BANK-GRADE FIX: Store baseSimilarity for gap rule check
      // Also store all signal values for diagnostic logging
      candidates.push({ 
        staff, 
        similarity, 
        baseSimilarity,
        signals: { temporal: temporalSignal, device: deviceSignal, location: locationSignal },
        maxSimilarity,
        centroidSimilarity,
        allSimilarities: similarities
      });
      
      // Early exit on very high confidence (only if above threshold)
      if (similarity >= CONFIG.VERY_HIGH_CONFIDENCE_THRESHOLD) {
        console.log(`‚ö° Early exit: Found very high confidence match (${(similarity * 100).toFixed(1)}%)`);
        // Don't break - continue to see all similarities for debugging
        // But we know this is the best match
      }
    } else if (similarity >= threshold - 0.03) {
      // üö® CRITICAL FIX: Also track near-threshold candidates (within 3% below threshold)
      // This prevents false positives when the actual person is just slightly below threshold
      // Store as near-threshold candidate for gap checking
      if (!nearThresholdCandidates) nearThresholdCandidates = [];
      nearThresholdCandidates.push({ 
        staff, 
        similarity, 
        baseSimilarity,
        signals: { temporal: temporalSignal, device: deviceSignal, location: locationSignal },
        maxSimilarity,
        centroidSimilarity,
        allSimilarities: similarities,
        isNearThreshold: true
      });
      console.log(`   ‚ö†Ô∏è ${staff.name}: Near-threshold candidate (${(similarity * 100).toFixed(2)}%, ${((threshold - similarity) * 100).toFixed(2)}% below threshold) - Will check for ambiguity`);
    }
  }
  
  console.log(`\nüìä ========== MATCHING RESULTS ==========`);
  console.log(`üìä Best match: ${bestMatch ? bestMatch.staff.name : 'None'} - ${(bestSimilarity * 100).toFixed(2)}%`);
  console.log(`üìä Candidates above threshold (${(threshold * 100).toFixed(1)}%): ${candidates.filter(c => !c.isNearThreshold).length}`);
  if (nearThresholdCandidates && nearThresholdCandidates.length > 0) {
    console.log(`üìä Near-threshold candidates (within 3% below): ${nearThresholdCandidates.length}`);
  }
  if (candidates.length > 0) {
    console.log(`üìä All candidates:`);
    candidates.forEach((c, idx) => {
      const label = c.isNearThreshold ? ' (near-threshold)' : '';
      console.log(`   ${idx + 1}. ${c.staff.name}: ${(c.similarity * 100).toFixed(2)}% (face: ${(c.baseSimilarity * 100).toFixed(2)}%)${label}`);
    });
  }
  if (nearThresholdCandidates && nearThresholdCandidates.length > 0 && !candidates.some(c => c.isNearThreshold)) {
    console.log(`üìä Near-threshold candidates (not included in gap check):`);
    nearThresholdCandidates.forEach((c, idx) => {
      console.log(`   ${idx + 1}. ${c.staff.name}: ${(c.similarity * 100).toFixed(2)}% (face: ${(c.baseSimilarity * 100).toFixed(2)}%, ${((threshold - c.similarity) * 100).toFixed(2)}% below threshold)`);
    });
  }
  console.log(`üìä ======================================\n`);
  
  // üè¶ BANK-GRADE: Validate candidates with adaptive gap rules
  if (candidates.length > 0) {
    // üö® CRITICAL FIX: When multiple candidates exist and face scores are close, use ONLY face score (ignore fusion)
    // This prevents auxiliary signals from pushing an incorrect candidate above threshold
    const SAFE_IGNORE_GAP = 0.06; // 6% - if face scores are within this, ignore non-face signals
    
    // First, check if top 2 face scores are close (before sorting)
    if (candidates.length > 1) {
      // Sort by baseSimilarity (face score) to check proximity
      const sortedByFace = [...candidates].sort((a, b) => (b.baseSimilarity || b.similarity) - (a.baseSimilarity || a.similarity));
      const topFace = sortedByFace[0].baseSimilarity || sortedByFace[0].similarity;
      const secondFace = sortedByFace[1].baseSimilarity || sortedByFace[1].similarity;
      const faceGap = topFace - secondFace;
      
      if (faceGap < SAFE_IGNORE_GAP) {
        console.warn(`‚ö†Ô∏è AMBIGUOUS FACE SCORES: Top 2 candidates have face scores within ${(faceGap * 100).toFixed(2)}%`);
        console.warn(`   Top: ${sortedByFace[0].staff.name} - ${(topFace * 100).toFixed(2)}%`);
        console.warn(`   Second: ${sortedByFace[1].staff.name} - ${(secondFace * 100).toFixed(2)}%`);
        console.warn(`   üö® Using ONLY face scores (ignoring temporal/device/location fusion) for safety`);
        
        // Override similarity to use only baseSimilarity (face score) for all candidates
        candidates.forEach(c => {
          c.similarity = c.baseSimilarity || c.similarity;
          c.fusionIgnored = true; // Flag that fusion was ignored
        });
      }
    }
    
    // üêõ CRITICAL FIX: Sort by baseSimilarity if it's significantly higher than similarity (fusion penalized it)
    // Otherwise use similarity (fused score). This ensures fair ranking.
    candidates.sort((a, b) => {
      // If baseSimilarity is >5% higher than similarity, fusion penalized it unfairly - use baseSimilarity
      // If fusion was ignored, use baseSimilarity directly
      const aScore = (a.fusionIgnored || (a.baseSimilarity && a.baseSimilarity > a.similarity + 0.05)) ? a.baseSimilarity : a.similarity;
      const bScore = (b.fusionIgnored || (b.baseSimilarity && b.baseSimilarity > b.similarity + 0.05)) ? b.baseSimilarity : b.similarity;
      return bScore - aScore;
    });
    const topMatch = candidates[0];
    
    // üö® CRITICAL FIX: Ambiguous candidate guard - reject when top 2 face scores are too close
    // This prevents false matches when two people have similar embeddings
    // üêõ FIX: Only reject if gap is VERY small (<2%) AND both scores are high confidence
    // This allows legitimate matches with 2-3% gap (like 79.64% vs 79.17%)
    if (candidates.length > 1) {
      const topFace = topMatch.baseSimilarity || topMatch.similarity;
      const secondMatch = candidates[1];
      const secondFace = secondMatch.baseSimilarity || secondMatch.similarity;
      const faceGap = topFace - secondFace;
      
      // Only reject if gap is VERY small (<2%) AND both are high confidence (‚â•80%)
      // This prevents false matches while allowing legitimate matches with small gaps
      const AMBIGUOUS_MIN_FACE = CONFIG.HIGH_CONFIDENCE_THRESHOLD; // 0.80
      const AMBIGUOUS_MAX_GAP = 0.02; // 2% - only reject if gap is extremely small
      
      if (topFace >= AMBIGUOUS_MIN_FACE && secondFace >= AMBIGUOUS_MIN_FACE && faceGap <= AMBIGUOUS_MAX_GAP) {
        console.error(`‚ùå AMBIGUOUS HIGH-CONFIDENCE MATCH - REJECTED (gap too small)`);
        console.error(`   Top match: ${topMatch.staff.name} - ${(topFace * 100).toFixed(2)}% (face score)`);
        console.error(`   Second match: ${secondMatch.staff.name} - ${(secondFace * 100).toFixed(2)}% (face score)`);
        console.error(`   Gap: ${(faceGap * 100).toFixed(2)}% (required: >${(AMBIGUOUS_MAX_GAP * 100).toFixed(0)}% for safety)`);
        console.error(`   ‚ö†Ô∏è Both candidates have very high face confidence (‚â•80%) but gap is extremely small - REJECTING to prevent false match`);
        console.error(`   üí° This requires manual review or re-enrollment with better quality images`);
        
        // Track failed match for admin review
        try {
          await trackFailedMatch(null, {
            staff: topMatch.staff,
            similarity: topMatch.similarity,
            baseSimilarity: topFace
          }, topFace, CONFIG.MIN_SIMILARITY_THRESHOLD, {
            reason: 'ambiguous_high_confidence',
            secondMatch: { 
              staff: secondMatch.staff,
              similarity: secondMatch.similarity,
              baseSimilarity: secondFace 
            },
            gap: faceGap,
          });
        } catch (err) {
          // trackFailedMatch might fail - that's okay, we still reject
          console.warn(`   ‚ö†Ô∏è Could not track failed match: ${err.message}`);
        }
        
        return null; // Force manual review or fallback (PIN/ID)
      }
    }
    
    // üè¶ BANK-GRADE: Adaptive gap rules (high confidence no gap, medium requires gap)
    let requiresGap = false;
    let gapRequired = CONFIG.ADAPTIVE_GAP_REQUIRED; // 5% default gap (reduced from 8%)
    
    // üè¶ BANK-GRADE FIX: Check baseSimilarity (face score) separately from fused score
    // High face confidence should override borderline fused scores
    const baseSimilarity = topMatch.baseSimilarity || topMatch.similarity;
    const faceConfidenceHigh = baseSimilarity >= 0.85; // High face confidence threshold
    const faceConfidenceVeryHigh = baseSimilarity >= 0.80; // Very high face confidence (‚â•80%)
    
    // üêõ CRITICAL FIX: Adaptive gap requirement based on top match confidence
    // Higher confidence matches need smaller gaps (more lenient)
    if (topMatch.similarity >= 0.80) {
      // Very high confidence (‚â•80%): Reduce gap requirement to 3%
      gapRequired = 0.03; // 3% for very high confidence
      console.log(`üè¶ Very high confidence match (${(topMatch.similarity * 100).toFixed(2)}% ‚â• 80%) - Reduced gap requirement to 3%`);
    } else if (topMatch.similarity >= 0.75) {
      // High confidence (‚â•75%): Reduce gap requirement to 4%
      gapRequired = 0.04; // 4% for high confidence
      console.log(`üè¶ High confidence match (${(topMatch.similarity * 100).toFixed(2)}% ‚â• 75%) - Reduced gap requirement to 4%`);
    }
    
    // üö® CRITICAL FIX: ALWAYS require gap check when multiple candidates exist OR when near-threshold candidates exist
    // This prevents false matches when two people have similar scores, even if one is just below threshold
    if (candidates.length > 1) {
      // Multiple candidates - ALWAYS require gap check, even for high confidence
      requiresGap = true;
      console.log(`üîç Multiple candidates (${candidates.length}) - Gap check REQUIRED (minimum: ${(gapRequired * 100).toFixed(0)}%)`);
      console.log(`   Top match: ${topMatch.staff.name} - ${(topMatch.similarity * 100).toFixed(2)}% (face: ${(baseSimilarity * 100).toFixed(2)}%)`);
    } else if (nearThresholdCandidates && nearThresholdCandidates.length > 0) {
      // üö® CRITICAL FIX: Single candidate above threshold BUT near-threshold candidates exist
      // This prevents false positives when the actual person is just slightly below threshold
      // Example: Sethu (71.75%) vs Cebisile (87.05%) - Sethu is only 0.3% below threshold
      requiresGap = true;
      const bestNearThreshold = nearThresholdCandidates.sort((a, b) => (b.baseSimilarity || b.similarity) - (a.baseSimilarity || a.similarity))[0];
      const nearThresholdGap = topMatch.similarity - (bestNearThreshold.baseSimilarity || bestNearThreshold.similarity);
      console.log(`üîç Single candidate above threshold BUT near-threshold candidate(s) detected - Gap check REQUIRED`);
      console.log(`   Top match: ${topMatch.staff.name} - ${(topMatch.similarity * 100).toFixed(2)}% (face: ${(baseSimilarity * 100).toFixed(2)}%)`);
      console.log(`   Near-threshold: ${bestNearThreshold.staff.name} - ${((bestNearThreshold.baseSimilarity || bestNearThreshold.similarity) * 100).toFixed(2)}% (${((threshold - (bestNearThreshold.baseSimilarity || bestNearThreshold.similarity)) * 100).toFixed(2)}% below threshold)`);
      console.log(`   Gap: ${(nearThresholdGap * 100).toFixed(2)}% - Will validate against minimum gap requirement`);
      
      // Add best near-threshold candidate to candidates array for gap checking
      candidates.push(bestNearThreshold);
    } else {
      // Single candidate - no gap check needed
      requiresGap = false;
      console.log(`‚úÖ Single candidate - No gap check needed`);
    }
    
    // Validate minimum similarity
    if (topMatch.similarity < CONFIG.MEDIUM_CONFIDENCE_MIN) {
      // Low confidence (<70%): Reject
      console.error(`‚ùå Low confidence match (${(topMatch.similarity * 100).toFixed(2)}% < 70%) - Rejecting`);
      return null;
    }
    
    // Track if gap check passed (for conditional margin check)
    let gapCheckPassed = false;
    
    // Check gap if required
    if (requiresGap && candidates.length > 1) {
      const secondMatch = candidates[1];
      
      // üêõ CRITICAL FIX: Use same scoring logic as sorting for gap checking
      // If baseSimilarity is >5% higher than similarity, fusion penalized it unfairly - use baseSimilarity
      const topScoreForGap = (topMatch.baseSimilarity && topMatch.baseSimilarity > topMatch.similarity + 0.05) 
        ? topMatch.baseSimilarity 
        : topMatch.similarity;
      const secondScoreForGap = (secondMatch.baseSimilarity && secondMatch.baseSimilarity > secondMatch.similarity + 0.05) 
        ? secondMatch.baseSimilarity 
        : secondMatch.similarity;
      
      const similarityGap = topScoreForGap - secondScoreForGap;
      
      // üö® CRITICAL FIX: If second match is a near-threshold candidate (very close to threshold), require LARGER gap
      // This prevents false positives when the actual person is just slightly below threshold
      // Example: Top match 87%, near-threshold 71.75% (0.3% below) -> requires 10%+ gap for safety
      if (secondMatch.isNearThreshold) {
        const distanceBelowThreshold = threshold - secondScoreForGap;
        if (distanceBelowThreshold <= 0.01) {
          // Very close to threshold (within 1%) - this is suspicious, require larger gap (10%+)
          const minGapForNearThreshold = 0.10; // 10% minimum gap
          if (gapRequired < minGapForNearThreshold) {
            gapRequired = minGapForNearThreshold;
            console.warn(`üö® Near-threshold candidate is very close to threshold (${(distanceBelowThreshold * 100).toFixed(2)}% below)`);
            console.warn(`   Requiring larger gap (${(minGapForNearThreshold * 100).toFixed(0)}%) to prevent false positive`);
          }
        } else if (distanceBelowThreshold <= 0.02) {
          // Close to threshold (within 2%) - require moderate gap (7%+)
          const minGapForNearThreshold = 0.07; // 7% minimum gap
          if (gapRequired < minGapForNearThreshold) {
            gapRequired = minGapForNearThreshold;
            console.warn(`‚ö†Ô∏è Near-threshold candidate is close to threshold (${(distanceBelowThreshold * 100).toFixed(2)}% below)`);
            console.warn(`   Requiring moderate gap (${(minGapForNearThreshold * 100).toFixed(0)}%) for safety`);
          }
        }
      }
      
      // üêõ CRITICAL FIX: Adaptive gap requirement based on confidence level
      // High confidence matches (‚â•75%) can have smaller gaps (1-2%)
      // Lower confidence matches need larger gaps (4-5%)
      const allScores = candidates.map(c => c.baseSimilarity || c.similarity);
      const scoreRange = Math.max(...allScores) - Math.min(...allScores);
      let adjustedGapRequired = gapRequired;
      
      // üö® CRITICAL FIX: Set gap requirement based on top score's confidence level
      const topScore = topMatch.baseSimilarity || topMatch.similarity;
      const isHighConfidence = topScore >= 0.75; // High confidence match
      const isVeryHighConfidence = topScore >= 0.80; // Very high confidence match
      
      // Set gap requirement based on confidence level (not actual gap size)
      if (isVeryHighConfidence) {
        // Very high confidence (‚â•80%) - minimal gap requirement (1%)
        adjustedGapRequired = 0.01; // Only 1% gap required
        console.log(`‚úÖ Very high confidence match (${(topScore * 100).toFixed(1)}% ‚â• 80%) - Minimal gap requirement (1%)`);
      } else if (isHighConfidence) {
        // High confidence (‚â•75%) - small gap requirement (2%)
        adjustedGapRequired = 0.02; // 2% gap for high confidence
        console.log(`‚úÖ High confidence match (${(topScore * 100).toFixed(1)}% ‚â• 75%) - Small gap requirement (2%)`);
      } else if (scoreRange < 0.10 && candidates.length >= 2) {
        // Low confidence scores that are close - require larger gap for safety
        adjustedGapRequired = Math.max(gapRequired, 0.08); // 8% for low confidence ambiguous matches
        console.warn(`‚ö†Ô∏è Candidates have close low-confidence scores (range: ${(scoreRange * 100).toFixed(2)}%) - Requiring larger gap (${(adjustedGapRequired * 100).toFixed(0)}%) for safety`);
      }
      
      // üö® CRITICAL FIX: For high confidence matches, accept if gap is meaningful (>0.3%)
      // For lower confidence, require the full gap requirement
      // Note: topScore, isHighConfidence, and isVeryHighConfidence are already declared above
      
      // For high confidence, accept if gap is >0.3% (meaningful difference)
      // For lower confidence, require the full gap requirement
      const minGapForHighConfidence = 0.003; // 0.3% - minimal meaningful gap
      const effectiveGapRequired = (isHighConfidence && similarityGap >= minGapForHighConfidence) 
        ? minGapForHighConfidence  // Accept high confidence with minimal gap
        : adjustedGapRequired;     // Require full gap for lower confidence
      
      // Adaptive tolerance based on gap requirement
      const tolerance = effectiveGapRequired < 0.04 ? 0.002 : (CONFIG.ADAPTIVE_GAP_TOLERANCE * 0.5); // 0.2% for small gaps, 0.5% for larger
      const gapWithTolerance = effectiveGapRequired - tolerance;
      
      if (similarityGap < gapWithTolerance) {
        // Gap is too small - REJECT to prevent false matches
        console.error(`‚ùå AMBIGUOUS MATCH - REJECTED (gap too small)`);
        const topFaceScore = (topMatch.baseSimilarity || topMatch.similarity) * 100;
        const secondFaceScore = (secondMatch.baseSimilarity || secondMatch.similarity) * 100;
        console.error(`   Top match: ${topMatch.staff.name} - ${(topMatch.similarity * 100).toFixed(2)}% (face: ${topFaceScore.toFixed(2)}%)`);
        console.error(`   Second match: ${secondMatch.staff.name} - ${(secondMatch.similarity * 100).toFixed(2)}% (face: ${secondFaceScore.toFixed(2)}%)`);
        console.error(`   Gap: ${(similarityGap * 100).toFixed(2)}% (required: ${(effectiveGapRequired * 100).toFixed(1)}%, tolerance: ${(tolerance * 100).toFixed(1)}%)`);
        console.error(`   ‚ö†Ô∏è Too close to call - REJECTING to prevent false match`);
        console.error(`   üí° This prevents misidentification when two people have similar face scores`);
        return null;
      } else {
        gapCheckPassed = true; // Mark gap check as passed
        if (isHighConfidence && similarityGap >= minGapForHighConfidence) {
          console.log(`‚úÖ High confidence match with meaningful gap - Gap: ${(similarityGap * 100).toFixed(2)}% (top: ${(topMatch.similarity * 100).toFixed(2)}%, second: ${(secondMatch.similarity * 100).toFixed(2)}%)`);
          console.log(`   Minimal gap requirement (0.3%) for high confidence - PASSED`);
        } else {
          console.log(`‚úÖ Clear distinction - Gap: ${(similarityGap * 100).toFixed(2)}% (top: ${(topMatch.similarity * 100).toFixed(2)}%, second: ${(secondMatch.similarity * 100).toFixed(2)}%)`);
          console.log(`   Required gap: ${(effectiveGapRequired * 100).toFixed(0)}% - PASSED`);
        }
      }
    } else if (requiresGap && candidates.length === 1) {
      // Only one candidate, gap check not needed
      gapCheckPassed = true; // No gap check needed = considered passed
      console.log(`‚úÖ Single candidate - No gap check needed`);
    } else if (!requiresGap) {
      // No gap check required = considered passed
      gapCheckPassed = true;
    }
    
    // üö® CRITICAL: Must meet absolute minimum (70%, increased from 65% to prevent false matches)
    if (topMatch.similarity < CONFIG.ABSOLUTE_MINIMUM_SIMILARITY) {
      console.error(`‚ùå Match rejected - below absolute minimum (70%)`);
      console.error(`   Similarity: ${(topMatch.similarity * 100).toFixed(2)}%`);
      console.error(`   Required: ${(CONFIG.ABSOLUTE_MINIMUM_SIMILARITY * 100).toFixed(0)}%`);
      console.error(`   ENTERPRISE: For 100% accuracy, we require minimum 70% similarity (CRITICAL FIX: increased from 65% to prevent false matches).`);
      return null;
    }
    
    // üö® CRITICAL FIX: Conditional margin check based on gap check and temporal signals
    // This prevents false positives when unregistered users match just above threshold
    // BUT allows legitimate users with temporal history or clear gap distinction
    const thresholdMargin = topMatch.similarity - threshold;
    
    // üêõ FIX: Skip margin check if gap check passed (clear distinction already validated)
    if (gapCheckPassed) {
      console.log(`‚úÖ Gap check passed - Skipping margin check (clear distinction already validated)`);
    } else {
      // Margin check is required - determine threshold based on temporal signals
      let MARGINAL_MATCH_THRESHOLD;
      const hasTemporalSignals = topMatch.signals && topMatch.signals.temporal > 0;
      
      if (hasTemporalSignals) {
        // User has temporal history (proven user) - reduce margin requirement to 3-5%
        MARGINAL_MATCH_THRESHOLD = 0.04; // 4% - reduced from 8% for users with temporal history
        console.log(`üìä Temporal signals detected (${(topMatch.signals.temporal * 100).toFixed(2)}%) - Using reduced margin requirement (4%)`);
      } else {
        // No temporal signals - keep strict 8% margin to prevent unregistered users
        MARGINAL_MATCH_THRESHOLD = 0.08; // 8% - strict margin for users without temporal history
        console.log(`üìä No temporal signals - Using strict margin requirement (8%)`);
      }
      
      if (thresholdMargin < MARGINAL_MATCH_THRESHOLD) {
        // Match is too close to threshold - this is suspicious, especially for unregistered users
        console.error(`‚ùå MARGINAL MATCH REJECTED - Too close to threshold`);
        console.error(`   Match: ${topMatch.staff.name} - ${(topMatch.similarity * 100).toFixed(2)}%`);
        console.error(`   Threshold: ${(threshold * 100).toFixed(1)}%`);
        console.error(`   Margin: ${(thresholdMargin * 100).toFixed(2)}% (required: ${(MARGINAL_MATCH_THRESHOLD * 100).toFixed(0)}%)`);
        console.error(`   ‚ö†Ô∏è Match is only ${(thresholdMargin * 100).toFixed(2)}% above threshold - REJECTING to prevent false positive`);
        if (hasTemporalSignals) {
          console.error(`   üí° Even with temporal history, margin is too small for safety`);
        } else {
          console.error(`   üí° This prevents unregistered users from clocking in with marginal matches`);
        }
        
        // Check if there are near-threshold candidates that might be the actual person
        if (nearThresholdCandidates && nearThresholdCandidates.length > 0) {
          const bestNearThreshold = nearThresholdCandidates.sort((a, b) => (b.baseSimilarity || b.similarity) - (a.baseSimilarity || a.similarity))[0];
          const nearThresholdScore = bestNearThreshold.baseSimilarity || bestNearThreshold.similarity;
          const distanceFromThreshold = threshold - nearThresholdScore;
          
          if (distanceFromThreshold <= 0.02) {
            // Near-threshold candidate is very close (within 2%) - this suggests the actual person is just below threshold
            console.error(`   ‚ö†Ô∏è Near-threshold candidate ${bestNearThreshold.staff.name} is ${(distanceFromThreshold * 100).toFixed(2)}% below threshold`);
            console.error(`   This suggests the match above threshold may be a false positive`);
          }
        }
        
        return null;
      } else {
        if (hasTemporalSignals) {
          console.log(`‚úÖ Margin check passed with temporal signals - Margin: ${(thresholdMargin * 100).toFixed(2)}% (required: ${(MARGINAL_MATCH_THRESHOLD * 100).toFixed(0)}%)`);
        } else {
          console.log(`‚úÖ Margin check passed - Margin: ${(thresholdMargin * 100).toFixed(2)}% (required: ${(MARGINAL_MATCH_THRESHOLD * 100).toFixed(0)}%)`);
        }
      }
    }
    
    // Warn if match is close but acceptable (only if margin check was performed)
    if (!gapCheckPassed && thresholdMargin < 0.08) { // Less than 8% above threshold
      console.warn(`‚ö†Ô∏è Match is close to threshold (${(thresholdMargin * 100).toFixed(2)}% above)`);
      console.warn(`   This may indicate a marginal match. Consider re-registering for better accuracy.`);
    }
    
    bestMatch = topMatch;
    bestSimilarity = topMatch.similarity;
  }
  
  if (!bestMatch || bestSimilarity < threshold) {
    console.error(`‚ùå NO MATCH FOUND - Rejected for security (100% accuracy requirement)`);
    console.error(`   Best similarity: ${(bestSimilarity * 100).toFixed(2)}%`);
    console.error(`   Required threshold: ${(threshold * 100).toFixed(1)}%`);
    console.error(`   Absolute minimum: ${(CONFIG.ABSOLUTE_MINIMUM_SIMILARITY * 100).toFixed(0)}%`);
    console.error(`\nüìä Possible reasons (100% accuracy requirement):`);
    console.error(`   1. Person not registered in system`);
    console.error(`   2. Face quality too low (lighting, angle, distance)`);
    console.error(`   3. Face too different from registration photos (appearance changes)`);
    console.error(`   4. Image quality issues (blur, brightness, camera quality)`);
    console.error(`   5. Multiple people in frame (detected and rejected for security)`);
    console.error(`   6. Similarity gap too small (ambiguous match - prevented false positive)`);
    
    // ENTERPRISE: Track failed match for active learning
    if (CONFIG.ENABLE_ACTIVE_LEARNING) {
      trackFailedMatch(embeddingData, bestMatch, bestSimilarity, threshold).catch(err => {
        console.warn('‚ö†Ô∏è Failed to track failed match for active learning:', err.message);
      });
    }
    
    return null;
  }
  
  let confidenceLevel = 'Medium';
  if (bestSimilarity >= CONFIG.VERY_HIGH_CONFIDENCE_THRESHOLD) {
    confidenceLevel = 'Very High';
  } else if (bestSimilarity >= CONFIG.HIGH_CONFIDENCE_THRESHOLD) {
    confidenceLevel = 'High';
  }
  
  const matchingTime = Date.now() - matchingStartTime;
  console.log(`‚úÖ Match found: ${bestMatch.staff.name} - Similarity: ${(bestSimilarity * 100).toFixed(1)}% (${confidenceLevel} confidence)`);
  console.log(`‚ö° Matching time: ${matchingTime}ms`);
  
  // üè¶ BANK-GRADE Phase 3 & 4: Return comprehensive match result with signals and risk score
  // üêõ CRITICAL FIX: Extract quality score from qualityData (defined at line 2344)
  // qualityData is always defined (has default fallback), so this is safe
  const finalQualityScore = typeof qualityData === 'object' 
    ? (qualityData.score || qualityData.detectionScore || qualityData.quality || 0.75) 
    : (qualityData || 0.75);
  
  // üêõ CRITICAL FIX: Ensure qualityData is accessible (it's defined at function start)
  if (typeof qualityData === 'undefined') {
    console.error('‚ùå CRITICAL: qualityData is undefined! This should never happen.');
    console.error('   embeddingData:', embeddingData ? 'exists' : 'null');
    console.error('   embeddingData.quality:', embeddingData?.quality ? 'exists' : 'missing');
  }
  
  return {
    staff: bestMatch.staff,
    similarity: bestSimilarity,
    confidenceLevel,
    quality: finalQualityScore, // ‚úÖ Fixed: Use finalQualityScore (derived from qualityData)
    qualityData: qualityData, // ‚úÖ Also return full qualityData object for debugging
    baseSimilarity: bestMatch.baseSimilarity || bestSimilarity,
    signals: bestMatch.signals || {
      face: bestMatch.baseSimilarity || bestSimilarity,
      temporal: 0,
      device: 0,
      location: 0,
    },
    riskScore: bestMatch.riskScore || { score: 0, level: 'low', factors: [] },
  };
}

/**
 * üéØ LIGHTWEIGHT PREVIEW VALIDATION: Quick validation for frontend preview frames
 * Uses same quality gates as full processing but NO embedding generation (fast!)
 * Returns specific feedback so users know exactly what to fix
 * 
 * ‚ö° OPTIMIZED: Only runs face detection, no embedding generation
 * ‚ö° FAST: 200-500ms (vs 1-2s for full processing)
 * ‚ö° SPECIFIC: Returns exact issues and feedback
 */
async function validatePreview(imageBuffer) {
  const startTime = Date.now();
  
  try {
    // Ensure models are loaded
    await loadModels();
    
    // üè¶ BANK-GRADE: Use canonical preprocessing for preview validation too
    let canonicalData;
    try {
      canonicalData = await preprocessCanonical(imageBuffer);
    } catch (preprocessError) {
      // Preprocessing errors (quality gates) - convert to user-friendly feedback
      const errorMsg = preprocessError.message.toLowerCase();
      
      if (errorMsg.includes('too small') || errorMsg.includes('minimum')) {
        return {
          ready: false,
          quality: 0,
          issues: ['image_too_small'],
          feedback: 'Image resolution too low. Please use a better camera.',
          metadata: { faceCount: 0 }
        };
      }
      
      if (errorMsg.includes('brightness')) {
        return {
          ready: false,
          quality: 0,
          issues: ['lighting'],
          feedback: 'Lighting issue detected. Please adjust lighting.',
          metadata: { faceCount: 0 }
        };
      }
      
      if (errorMsg.includes('blurry') || errorMsg.includes('sharpness')) {
        return {
          ready: false,
          quality: 0,
          issues: ['blur'],
          feedback: 'Image is too blurry. Hold still and ensure camera is focused.',
          metadata: { faceCount: 0 }
        };
      }
      
      throw preprocessError;
    }
    
    // ‚ö° LIGHTWEIGHT: Only run detection, NO embedding generation
    // Use canonical image for detection (same as full processing)
    let detections;
    try {
      detections = await detectFaces(canonicalData.canonicalBuffer, canonicalData.canonicalWidth, canonicalData.canonicalHeight);
    } catch (detectError) {
      // detectFaces throws errors for quality gate failures - convert to user-friendly feedback
      const errorMsg = detectError.message.toLowerCase();
      
      if (errorMsg.includes('no face') || errorMsg.includes('face detected')) {
        return {
          ready: false,
          quality: 0,
          issues: ['no_face'],
          feedback: 'Position your face in the circle',
          metadata: {
            faceCount: 0,
            angle: 0,
            size: 'none',
            distance: 'unknown'
          }
        };
      }
      
      if (errorMsg.includes('multiple faces') || errorMsg.includes('more than one')) {
        return {
          ready: false,
          quality: 0,
          issues: ['multiple_faces'],
          feedback: 'Multiple faces detected. Please ensure only you are in frame',
          metadata: {
            faceCount: 2,
            angle: 0,
            size: 'multiple',
            distance: 'unknown'
          }
        };
      }
      
      if (errorMsg.includes('too small') || errorMsg.includes('move closer')) {
        return {
          ready: false,
          quality: 0,
          issues: ['face_too_small'],
          feedback: 'Please move closer to the camera',
          metadata: {
            faceCount: 1,
            angle: 0,
            size: 'too_small',
            distance: 'far'
          }
        };
      }
      
      if (errorMsg.includes('too large') || errorMsg.includes('move further')) {
        return {
          ready: false,
          quality: 0,
          issues: ['face_too_large'],
          feedback: 'Please move slightly farther away',
          metadata: {
            faceCount: 1,
            angle: 0,
            size: 'too_large',
            distance: 'too_close'
          }
        };
      }
      
      if (errorMsg.includes('too blurry') || errorMsg.includes('blur')) {
        return {
          ready: false,
          quality: 0,
          issues: ['blur'],
          feedback: 'Image is too blurry. Please hold still and ensure camera is focused',
          metadata: {
            faceCount: 1,
            angle: 0,
            size: 'good',
            distance: 'good'
          }
        };
      }
      
      if (errorMsg.includes('brightness')) {
        return {
          ready: false,
          quality: 0,
          issues: ['lighting'],
          feedback: 'Adjust lighting. Not too dark or too bright',
          metadata: {
            faceCount: 1,
            angle: 0,
            size: 'good',
            distance: 'good'
          }
        };
      }
      
      // Generic error - provide helpful feedback based on error type
      console.error('‚ùå Face detection failed in preview validation:', detectError.message);
      const genericErrorMsg = detectError.message.toLowerCase();
      let userFeedback = 'Unable to detect face. Please ensure your face is visible.';
      
      if (genericErrorMsg.includes('liveness') || genericErrorMsg.includes('symmetry')) {
        userFeedback = 'Face not centered properly. Look straight at the camera and center your face in the frame.';
      } else if (genericErrorMsg.includes('quality')) {
        userFeedback = 'Face quality too low. Please ensure good lighting and face the camera directly.';
      } else if (genericErrorMsg.includes('landmark')) {
        userFeedback = 'Facial features not clearly visible. Please ensure your face is well-lit and centered.';
      }
      
      return {
        ready: false,
        quality: 0,
        issues: ['detection_failed'],
        feedback: userFeedback,
        metadata: {
          error: detectError.message
        }
      };
    }
    
    if (!detections || detections.length === 0) {
      return {
        ready: false,
        quality: 0,
        issues: ['no_face'],
        feedback: 'Position your face in the circle',
        metadata: {
          faceCount: 0,
          angle: 0,
          size: 'none',
          distance: 'unknown'
        }
      };
    }
    
    // Get best detection (detectFaces already returns sorted by score)
    const bestDetection = detections[0];
    
    // üë§ GENDER DETECTION: Detect gender from face
    let genderResult = { gender: 'UNKNOWN', confidence: 0.5 };
    try {
      console.log('üë§ Starting gender detection...');
      genderResult = await detectGender(bestDetection, canonicalData.canonicalBuffer, canonicalData.canonicalWidth, canonicalData.canonicalHeight);
      console.log(`üë§ ‚úÖ Gender detected: ${genderResult.gender} (confidence: ${(genderResult.confidence * 100).toFixed(1)}%)`);
    } catch (genderError) {
      console.error('‚ùå Gender detection failed:', genderError.message);
      console.error('   Error stack:', genderError.stack);
      // Continue without gender - not critical for validation
      genderResult = { gender: 'UNKNOWN', confidence: 0.5 };
    }
    
    // Collect all issues
    const issues = [];
    let feedback = '';
    
    // üêõ FIX: Check face count - only reject if clearly multiple different people
    // detectFaces already does NMS filtering, so if it returns 1 detection, accept it
    // If it returns 2+ detections, they were already validated as different people in detectFaces
    // So we should only reject if detectFaces threw an error about multiple faces
    // (The error handling above already catches that case)
    // This check is just a safety net - if we somehow get here with multiple detections,
    // it means detectFaces didn't properly filter them, so we should still accept the best one
    if (detections.length > 1) {
      console.warn(`‚ö†Ô∏è validatePreview: Received ${detections.length} detections (should be 1 after NMS)`);
      console.warn(`   Using best detection (score: ${(detections[0].score * 100).toFixed(1)}%)`);
      // Use best detection instead of rejecting
      // This handles edge cases where NMS didn't fully filter duplicates
    }
    
    // üêõ CRITICAL FIX: Convert normalized coordinates to pixels before comparison
    // bestDetection.box.width/height are normalized (0-1), not pixels!
    // Must convert using canonical dimensions, same as generateEmbedding does
    const faceWidth_pixels = bestDetection.box.width * canonicalData.canonicalWidth;
    const faceHeight_pixels = bestDetection.box.height * canonicalData.canonicalHeight;
    const faceSize_pixels = Math.min(faceWidth_pixels, faceHeight_pixels);
    
    if (faceSize_pixels < CONFIG.MIN_FACE_SIZE) {
      issues.push('face_too_small');
      feedback = 'Please move closer to the camera';
    } else if (faceSize_pixels > CONFIG.MAX_FACE_SIZE) {
      issues.push('face_too_large');
      feedback = 'Please move slightly farther away';
    }
    
    // üêõ FIX: Improved angle estimation from landmarks
    // Use same logic as generateEmbedding for consistency
    let totalAngleDeviation = 0;
    if (bestDetection.landmarks) {
      const { leftEye, rightEye, nose } = bestDetection.landmarks;
      if (leftEye && rightEye && nose) {
        // Convert normalized landmark coordinates to pixels for accurate calculation
        const leftEyeX = leftEye.x * canonicalData.canonicalWidth;
        const leftEyeY = leftEye.y * canonicalData.canonicalHeight;
        const rightEyeX = rightEye.x * canonicalData.canonicalWidth;
        const rightEyeY = rightEye.y * canonicalData.canonicalHeight;
        const noseX = nose.x * canonicalData.canonicalWidth;
        const noseY = nose.y * canonicalData.canonicalHeight;
        
        // Calculate face orientation from eye positions (in pixels)
        const eyeDistance = Math.sqrt(
          Math.pow(rightEyeX - leftEyeX, 2) + 
          Math.pow(rightEyeY - leftEyeY, 2)
        );
        const eyeCenterX = (leftEyeX + rightEyeX) / 2;
        const eyeCenterY = (leftEyeY + rightEyeY) / 2;
        const noseToEyeCenter = Math.sqrt(
          Math.pow(noseX - eyeCenterX, 2) +
          Math.pow(noseY - eyeCenterY, 2)
        );
        
        // More accurate angle estimation
        // For a front-facing face, nose should be roughly centered between eyes
        // If nose is far from center, face is tilted
        const angleRatio = noseToEyeCenter / eyeDistance;
        
        // Calculate roll (rotation around Z-axis) from eye alignment
        const eyeSlope = Math.abs((rightEyeY - leftEyeY) / (rightEyeX - leftEyeX));
        const rollAngle = Math.atan(eyeSlope) * (180 / Math.PI);
        
        // Estimate yaw (left-right turn) from nose position relative to eye center
        const noseOffsetX = Math.abs(noseX - eyeCenterX);
        const yawAngle = (noseOffsetX / eyeDistance) * 45; // Rough estimate
        
        // Use the larger of roll or yaw as total deviation
        totalAngleDeviation = Math.max(rollAngle, yawAngle);
        
        // üéØ ENHANCED: More lenient angle detection for preview validation
        // Only flag if angle is VERY significant (2.5x threshold) AND impacts quality
        // This prevents false positives from landmark detection inaccuracies
        // Angle is only a blocker if it's severe AND face quality is already marginal
        const quality = bestDetection.score || 0;
        const isSevereAngle = totalAngleDeviation > CONFIG.MAX_FACE_ANGLE * 2.5; // 37.5¬∞ for preview (was 22.5¬∞)
        const isModerateAngle = totalAngleDeviation > CONFIG.MAX_FACE_ANGLE * 2.0; // 30¬∞ for advisory
        
        // Only block if severe angle AND quality is already low
        // For good quality faces, angle is less critical
        if (isSevereAngle && quality < 0.70) {
          // Severe angle + low quality = block
        } else if (isModerateAngle && quality < 0.60) {
          // Moderate angle + very low quality = block
        } else {
          // Angle is acceptable OR quality is good enough to compensate
          totalAngleDeviation = 0; // Reset if within acceptable range
        }
      }
    }
    
    // üéØ ENHANCED: Only flag angle if it's severe AND impacts quality
    const quality = bestDetection.score || 0;
    const isSevereAngle = totalAngleDeviation > CONFIG.MAX_FACE_ANGLE * 2.5; // 37.5¬∞ (was 22.5¬∞)
    const isModerateAngle = totalAngleDeviation > CONFIG.MAX_FACE_ANGLE * 2.0; // 30¬∞ for advisory feedback
    
    if (isSevereAngle && quality < 0.70) {
      // Only block if severe angle AND quality is already low
      issues.push('angle_too_tilted');
      if (!feedback) feedback = 'Look straight into the camera for better recognition';
    } else if (isModerateAngle && quality < 0.85) {
      // Advisory feedback for moderate angles (don't block, just inform)
      // Don't add to issues array - just provide feedback
      if (!feedback) feedback = 'Face the camera more directly for best results';
    }
    
    // Check quality score
    if (quality < CONFIG.MIN_FACE_QUALITY) {
      issues.push('quality_too_low');
      if (quality < 0.5) {
        if (!feedback) feedback = 'Move to better lighting';
      } else if (quality < 0.65) {
        if (!feedback) feedback = 'Good position! Adjust lighting for better quality';
      }
    }
    
    // Check eyes (if landmarks available)
    if (bestDetection.landmarks) {
      // For preview, we can't get eye open probability from SCRFD
      // But we can check if landmarks are reasonable
      const { leftEye, rightEye } = bestDetection.landmarks;
      if (!leftEye || !rightEye) {
        issues.push('features_not_detected');
        if (!feedback) feedback = 'Please ensure your face is clearly visible';
      }
    }
    
    // üéØ ENHANCED: Determine if ready based on actual quality and issues
    // More lenient: Allow ready state if quality is good even with minor issues
    const isReady = issues.length === 0 && quality >= CONFIG.MIN_FACE_QUALITY;
    
    // üéØ ENHANCED: Generate dynamic, responsive feedback based on actual detection results
    if (!feedback) {
      if (isReady) {
        // Quality-based feedback for ready state
        if (quality >= 0.85) {
          feedback = 'Perfect! Ready to capture ‚úì';
        } else if (quality >= 0.75) {
          feedback = 'Excellent! Ready to capture ‚úì';
        } else if (quality >= CONFIG.MIN_FACE_QUALITY) {
          feedback = 'Good! Ready to capture ‚úì';
        }
      } else {
        // Provide specific feedback based on what's missing
        if (quality < CONFIG.MIN_FACE_QUALITY) {
          if (quality < 0.50) {
            feedback = 'Move to better lighting';
          } else if (quality < 0.60) {
            feedback = 'Adjust lighting for better quality';
          } else {
            feedback = 'Almost there! Improve lighting slightly';
          }
        } else if (issues.length > 0) {
          // Issues present but quality is OK - provide specific guidance
          feedback = 'Adjust position for best results';
        } else {
          feedback = 'Almost ready! Hold still and ensure good lighting';
        }
      }
    }
    
    const validationTime = Date.now() - startTime;
    console.log(`‚ö° Preview validation: ${validationTime}ms, ready: ${isReady}, quality: ${(quality * 100).toFixed(1)}%, issues: ${issues.length}`);
    
    // üéØ FACIAL LANDMARKS: Include landmark coordinates for frontend visualization
    let landmarks = null;
    if (bestDetection.landmarks) {
      // Convert normalized landmarks to relative coordinates (0-1) for frontend
      landmarks = {
        leftEye: bestDetection.landmarks.leftEye ? {
          x: bestDetection.landmarks.leftEye.x,
          y: bestDetection.landmarks.leftEye.y
        } : null,
        rightEye: bestDetection.landmarks.rightEye ? {
          x: bestDetection.landmarks.rightEye.x,
          y: bestDetection.landmarks.rightEye.y
        } : null,
        nose: bestDetection.landmarks.nose ? {
          x: bestDetection.landmarks.nose.x,
          y: bestDetection.landmarks.nose.y
        } : null,
        leftMouth: bestDetection.landmarks.leftMouth ? {
          x: bestDetection.landmarks.leftMouth.x,
          y: bestDetection.landmarks.leftMouth.y
        } : null,
        rightMouth: bestDetection.landmarks.rightMouth ? {
          x: bestDetection.landmarks.rightMouth.x,
          y: bestDetection.landmarks.rightMouth.y
        } : null,
      };
    }
    
    // Face box coordinates (normalized 0-1)
    const faceBox = bestDetection.box ? {
      x: bestDetection.box.x,
      y: bestDetection.box.y,
      width: bestDetection.box.width,
      height: bestDetection.box.height
    } : null;
    
    return {
      ready: isReady,
      quality: Math.round(quality * 100),
      issues,
      feedback,
      metadata: {
        faceCount: 1,
        angle: totalAngleDeviation,
        size: faceSize_pixels < CONFIG.MIN_FACE_SIZE ? 'too_small' : (faceSize_pixels > CONFIG.MAX_FACE_SIZE ? 'too_large' : 'good'),
        distance: faceSize_pixels < CONFIG.MIN_FACE_SIZE ? 'far' : (faceSize_pixels > CONFIG.MAX_FACE_SIZE ? 'too_close' : 'good'),
        faceSize: Math.round(faceSize_pixels),
        quality: Math.round(quality * 100),
        // üë§ GENDER: Add gender detection result
        gender: genderResult.gender,
        genderConfidence: genderResult.confidence,
        // üéØ FACIAL LANDMARKS: Add landmarks and face box for visualization
        landmarks: landmarks,
        faceBox: faceBox
      }
    };
    
  } catch (error) {
    console.error('‚ùå Preview validation error:', error.message);
      return {
        ready: false,
        quality: 0,
        issues: ['validation_error'],
        feedback: 'Unable to analyze image. Please try again.',
        metadata: {
          error: error.message,
          gender: 'UNKNOWN',
          genderConfidence: 0.5
        }
      };
  }
}

module.exports = {
  loadModels,
  generateEmbedding,
  generateIDEmbedding, // üè¶ BANK-GRADE Phase 5: ID document embedding extraction
  findMatchingStaff,
  cosineSimilarity,
  validateTimePattern,
  computeCentroidTemplate, // üè¶ BANK-GRADE: Export centroid computation
  validatePreview, // NEW: Lightweight preview validation
  // üè¶ BANK-GRADE Phase 3: Multi-signal fusion
  generateDeviceFingerprint,
  createTrustedDeviceEntry,
  calculateTemporalSignal,
  calculateDeviceSignal,
  calculateLocationSignal,
  // üè¶ BANK-GRADE Phase 4: Enhanced features
  checkEnhancedLiveness,
  calculateRiskScore,
  // üè¶ BANK-GRADE Phase 4: Device quality tracking
  getDeviceQuality,
  trackDeviceQuality,
};

/**
 * üè¶ BANK-GRADE: Warmup inference pipeline
 * Runs a dummy inference to eliminate cold-start latency
 * This preloads JIT compilation, memory allocation, and model caches
 * 
 * @param {Buffer} dummyImageBuffer - Dummy image buffer for warmup
 * @returns {Promise<boolean>} - Success status
 */
/**
 * üè¶ BANK-GRADE: Warmup inference pipeline
 * Simple warmup that just loads models without running inference
 * More complex warmup with actual inference caused issues
 */
async function warmupInference() {
  try {
    console.log('üî• Warming up ONNX inference pipeline...');
    const warmupStart = Date.now();
    
    // Just load models - don't run actual inference
    await loadModels();
    
    const warmupTime = Date.now() - warmupStart;
    console.log(`üî• Models loaded in ${warmupTime}ms`);
    return true;
  } catch (error) {
    console.warn('‚ö†Ô∏è Warmup failed (non-critical):', error.message);
    return false;
  }
}

// Export warmup function
module.exports.warmupInference = warmupInference;


